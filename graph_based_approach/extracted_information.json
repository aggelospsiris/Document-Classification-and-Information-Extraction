{
    "tables": [
        {
            "ID": "4",
            "Authors": [
                "Robert Sablatnig",
                "Marco Peer",
                "Florian Kleber"
            ],
            "Title": "SAGHOG: Self-Supervised Autoencoder for Generating HOG Features for Writer Retrieval",
            "Abstract": "This paper introduces Saghog, a self-supervised pretraining\nstrategy for writer retrieval using HOG features of the binarized input\nimage. Our preprocessing involves the application of the Segment Any-\nthing technique to extract handwriting from various datasets, ending up\nwith about 24k documents, followed by training a vision transformer\non reconstructing masked patches of the handwriting. Saghog is then\nfinetuned by appending NetRVLAD as an encoding layer to the pre-\ntrained encoder. Evaluation of our approach on three historical datasets,\nHistorical-WI, HisFrag20, and GRK-Papyri, demonstrates the effective-\nness of Saghog for writer retrieval. Additionally, we provide ablation\nstudies on our architecture and evaluate un- and supervised finetuning.\nNotably, on HisFrag20, Saghog outperforms related work with a mAP\nof 57.2 % - a margin of 11.6 % to the current state of the art, showcas-\ning its robustness on challenging data, and is competitive on even small\ndatasets, e.g. GRK-Papyri, where we achieve a Top-1 accuracy of 58.0 %.",
            "Keywords": "Writer Retrieval \u00b7 Self-Supervised Learning \u00b7 Masked Au-\ntoencoder \u00b7 Document Analysis."
        },
        {
            "ID": "7",
            "Authors": [
                "Silvia Cascianelli",
                "Vittorio Pippi",
                "Rita Cucchiara",
                "Fabio Quattrini"
            ],
            "Title": "Binarizing Documents by Leveraging both Space and Frequency",
            "Abstract": "Document Image Binarization is a well-known problem in\nDocument Analysis and Computer Vision, although it is far from being\nsolved. One of the main challenges of this task is that documents gener-\nally exhibit degradations and acquisition artifacts that can greatly vary\nthroughout the page. Nonetheless, even when dealing with a local patch\nof the document, taking into account the overall appearance of a wide\nportion of the page can ease the prediction by enriching it with seman-\ntic information on the ink and background conditions. In this respect,\napproaches able to model both local and global information have been\nproven suitable for this task. In particular, recent applications of Vision\nTransformer (ViT)-based models, able to model short and long-range de-\npendencies via the attention mechanism, have demonstrated their supe-\nriority over standard Convolution-based models, which instead struggle\nto model global dependencies. In this work, we propose an alternative so-\nlution based on the recently introduced Fast Fourier Convolutions, which\novercomes the limitation of standard convolutions in modeling global in-\nformation while requiring fewer parameters than ViTs. We validate the\neffectiveness of our approach via extensive experimental analysis consid-\nering different types of degradations.",
            "Keywords": "Document Enhancement \u00b7 Document Image Binarization \u00b7\nFast Fourier Convolution."
        },
        {
            "ID": "13",
            "Authors": [
                "Christopher Kermorvant",
                "Sol\u00e8ne Tarride"
            ],
            "Title": "Revisiting N-Gram Models: Their Impact in Modern Neural Networks for Handwritten Text Recognition",
            "Abstract": "In recent advances in automatic text recognition (ATR),\ndeep neural networks have demonstrated the ability to implicitly cap-\nture language statistics, potentially reducing the need for traditional lan-\nguage models. This study directly addresses whether explicit language\nmodels, specifically n-gram models, still contribute to the performance\nof state-of-the-art deep learning architectures in the field of handwriting\nrecognition. We evaluate two prominent neural network architectures,\nPyLaia [23] and DAN [8], with and without the integration of explicit\nn-gram language models. Our experiments on three datasets - IAM [19],\nRIMES [11], and NorHand v2 [2] - at both line and page level, investi-\ngate optimal parameters for n-gram models, including their order, weight,\nsmoothing methods and tokenization level. The results show that incor-\nporating character or subword n-gram models significantly improves the\nperformance of ATR models on all datasets, challenging the notion that\ndeep learning models alone are sufficient for optimal performance. In par-\nticular, the combination of DAN with a character language model out-\nperforms current benchmarks, confirming the value of hybrid approaches\nin modern document analysis systems.",
            "Keywords": "Handwritten Text Recognition, Neural Networks, Statisti-\ncal Language Modeling, Tokenization"
        },
        {
            "ID": "16",
            "Authors": [
                "Askar Hamdulla",
                "Mayire Ibrayim",
                "Jianjun Kang",
                "Chunhu Zhang",
                "Mengmeng Chen"
            ],
            "Title": "A Real-Time Scene Uyghur Text Detection Network Based on Feature Complementation",
            "Abstract": "Text detection in complex background images is a challeng-\ning task. With the national emphasis on the culture of various ethnic\ngroups, carrying out the task of detecting Uyghur in natural scene images\nis of great significance to the intelligent information technology industry\nand economic construction. In fact, current text detection applications in\ncomplex scenes have large models, and sluggish detection rates, and are\nchallenging to implement on mobile devices. Uyghur text has unique writ-\ning characteristics that lead to low detection results. To address these\nproblems, we propose a feature-complementation-based text detection\nframework for real-time scenes (FC-Net). The network uses a deeply\nseparable residual convolutional network (DResNet) for feature extrac-\ntion, which lowers the model\u2019s parameter count and boosts the detector\u2019s\ndetection speed. Secondly, a feature enhancement network based on spa-\ntial feature attention (SFA-FEN) is included in to obtain multi-target\ninformation by expanding the range of perceptual fields and enhanc-\ning the robustness of small-scale Uyghur. The experimental results show\nthat FC-Net can maintain the advantages of a lightweight network while\nmaintaining high accuracy, effectively reducing model parameters, and\nspeeding up model detection. The robustness and real-time performance\nare achieved not only on the Uyghur dataset but also on an arbitrary\ntext dataset.",
            "Keywords": "Lightweight \u00b7 Scene text detection \u00b7 Feature complementa-\ntion \u00b7 Sensory field \u00b7 FC-Net."
        },
        {
            "ID": "18",
            "Authors": [
                "Carlos Francisco Moreno - Garc\u00eda",
                "Laura Jamieson",
                "Eyad Elyan"
            ],
            "Title": "A Multiclass Imbalanced Dataset Classification of Symbols from Piping and Instrumentation Diagrams",
            "Abstract": "Engineering diagrams provide rich source of information and\nare widely used across different industries. Recent years have seen grow-\ning research interest in developing solutions for processing and analysing\nthese diagrams using wide range of image-processing and computer vi-\nsion techniques. In this paper, we first, present a new multiclass im-\nbalanced dataset of symbols extracted from Piping and Instrumentation\nDiagrams (P&IDs). The dataset contains 7,728 instances representing 48\ndifferent types of engineering symbols and it is considered the first of its\nkind in the research community. Second, we present a new method for\nhandling multiclass imbalance classification based on class decomposi-\ntion by means of unsupervised machine learning methods. Experiments\nusing Convolutional Neural Networks showed that using class decompo-\nsition significantly improves the classification performance that can be\nachieved, without causing information loss, as it is the case with other\nclass imbalance data sampling approaches.",
            "Keywords": "Piping and Instrumentation Diagrams \u00b7 Class Imbalance \u00b7\nConvolutional Neural Networks"
        },
        {
            "ID": "19",
            "Authors": [
                "Hai Luu Tuan",
                "Jeff Yang",
                "Huynh Vu The"
            ],
            "Title": "Light-Weight Multi-Modality Feature Fusion Network for Visually-Rich Document Understanding",
            "Abstract": "Entity extraction (EE) is an important task in visually-rich\ndocument understanding (VrDU) which leverages multi-modal features\nof text, layout, and image. Recent transformer-based architectures en-\nable an effective fusion of these features, showing great performance on\nthe EE task. However, these models are heavy, leading to substantially\nhigh training cost and low inference speed. Thus, we propose a light-\nweight transformer-based model (named LMFFN) with a novel layout-\nself-attention layout-aware multi-modal fusion mechanism that allows an\nefficient entity extraction. Specifically, the proposed framework uses just\na simple pre-training objective coupled with an effective batch imple-\nmentation. In addition, no constraints are required with regard to the\ninput sequence length or the reading order. This relaxation gives our\nmodel an advantage when it comes to camera and skewed documents, as\nwe observed a 7% F1-score improvement when we compared our model\nto previous SOTA models on camera data. Evaluation results of three\npublic datasets (CORD, SROIE, and XFUND) show that our proposed\narchitecture achieves competitive performance compared to recent SOTA\nmodels while having 5 to 10 times fewer parameters.\n1\nIntroduction\nVisually-rich documents (VrDs) are tightly connected to our daily life from in-\nsurance, logistic, finance to travel and leisure. As they contain lots of useful\ninformation, there are demands for archiving, indexing or structuring document\ninformation in form of digital format. In visually-rich documents understand-\ning (VrDU), documents of various formats (PDF, images, etc.) are digitized,\nstructured and organized automatically, which plays a crucial role in developing\ndigital transformation processes in daily business operation.\nThe VrDU tasks such as semantic entity extraction and entity linking [18] are\nbuilt upon the inherent complexities of co-existed multi-modal features. Hence,\nan effective feature learning and fusion of multi-modal data play an important\nrole to achieve good performance. Another challenge of VrDU tasks is the lack\nof data. While there is a variety of document templates, some of them are",
            "Keywords": null
        },
        {
            "ID": "20",
            "Authors": [
                "Yoann Schneider",
                "Bastien Abadie",
                "M\u00e9lodie Boillet",
                "Christopher Kermorvant",
                "Marie Generali - Lince",
                "Sol\u00e8ne Tarride"
            ],
            "Title": "Improving Automatic Text Recognition with Language Models in the PyLaia Open-Source Library",
            "Abstract": "PyLaia is one of the most popular open-source software for\nAutomatic Text Recognition (ATR), delivering strong performance in\nterms of speed and accuracy. In this paper, we outline our recent contri-\nbutions to the PyLaia library, focusing on the incorporation of reliable\nconfidence scores and the integration of statistical language modeling\nduring decoding. Our implementation provides an easy way to combine\nPyLaia with n-grams language models at different levels. One of the high-\nlights of this work is that language models are completely auto-tuned:\nthey can be built and used easily without any expert knowledge, and\nwithout requiring any additional data. To demonstrate the significance of\nour contribution, we evaluate PyLaia\u2019s performance on twelve datasets,\nboth with and without language modelling. The results show that decod-\ning with small language models improves the Word Error Rate by 13%\nand the Character Error Rate by 12% in average. Additionally, we con-\nduct an analysis of confidence scores and highlight the importance of cal-\nibration techniques. Our implementation is publicly available in the offi-\ncial PyLaia repository (https://gitlab.teklia.com/atr/pylaia), and\ntwelve open-source models are released on Hugging Face1.",
            "Keywords": "Automatic Text Recognition, Neural Networks, Language\nModels, Open-source Software"
        },
        {
            "ID": "22",
            "Authors": [
                "Ryota Nakao",
                "DongHyun Kim",
                "Youngmin Baek",
                "Moon Bin Yim",
                "Yamato Okamoto",
                "Bado Lee",
                "Geewook Kim",
                "Seunghyun Park"
            ],
            "Title": "CREPE: Coordinate-Aware End-to-End Document Parser",
            "Abstract": "In this study, we formulate an OCR-free sequence genera-\ntion model for visual document understanding (VDU). Our model not\nonly parses text from document images but also extracts the spatial\ncoordinates of the text based on the multi-head architecture. Named as\nCoordinate-aware End-to-end Document Parser (CREPE), our method\nuniquely integrates these capabilities by introducing a special token for\nOCR text, and token-triggered coordinate decoding. We also proposed\na weakly-supervised framework for cost-efficient training, requiring only\nparsing annotations without high-cost coordinate annotations. Our ex-\nperimental evaluations demonstrate CREPE\u2019s state-of-the-art performances\non document parsing tasks. Beyond that, CREPE\u2019s adaptability is fur-\nther highlighted by its successful usage in other document understanding\ntasks such as layout analysis, document visual question answering, and\nso one. CREPE\u2019s abilities including OCR and semantic parsing not only\nmitigate error propagation issues in existing OCR-dependent methods, it\nalso significantly enhance the functionality of sequence generation mod-\nels, ushering in a new era for document understanding studies.",
            "Keywords": "Visual Document Understanding\u00b7 Document Parsing\u00b7 Doc-\nument Information Extraction\u00b7 Optical Character Recognition\u00b7 End-to-\nEnd Transformer\u00b7 Weakly Supervised Learning.\n\u2217 Equal contribution.\n\u2020 Current Affiliation: CyberAgent, Inc.\n\u2021 Corresponding author."
        },
        {
            "ID": "25",
            "Authors": [
                "Tahira Shehzadi",
                "Didier Stricker",
                "Muhammad Zeshan Afzal"
            ],
            "Title": "A Hybrid Approach for Document Layout Analysis in Document images",
            "Abstract": "Document layout analysis involves understanding the ar-\nrangement of elements within a document. This paper navigates the\ncomplexities of understanding various elements within document images,\nsuch as text, images, tables, and headings. The approach employs an\nadvanced Transformer-based object detection network as an innovative\ngraphical page object detector for identifying tables, figures, and dis-\nplayed elements. We introduce a query encoding mechanism to provide\nhigh-quality object queries for contrastive learning, enhancing efficiency\nin the decoder phase. We also present a hybrid matching scheme that\nintegrates the decoder\u2019s original one-to-one matching strategy with the\none-to-many matching strategy during the training phase. This approach\naims to improve the model\u2019s accuracy and versatility in detecting vari-\nous graphical elements on a page. Our experiments on PubLayNet, Do-\ncLayNet, and PubTables benchmarks show that our approach outper-\nforms current state-of-the-art methods. It achieves an average precision\nof 97.3% on PubLayNet, 81.6% on DocLayNet, and 98.6% on PubTa-\nbles, demonstrating its superior performance in layout analysis. These\nadvancements not only enhance the conversion of document images into\neditable and accessible formats but also streamline information retrieval\nand data extraction processes.",
            "Keywords": "Detection Transformer \u00b7 Document Layout Analysis \u00b7 Graph-\nical object detection"
        },
        {
            "ID": "27",
            "Authors": [
                "Tahira Shehzadi",
                "Shalini Sarode",
                "Muhammad Zeshan Afzal",
                "Didier Stricker"
            ],
            "Title": "Towards End-to-End Semi-Supervised Table Detection with Semantic Aligned Matching Transformer",
            "Abstract": "Table detection within document images is a crucial task in document\nprocessing, involving the identification and localization of tables. Recent strides in\ndeep learning have substantially improved the accuracy of this task, but it still heav-\nily relies on large labeled datasets for effective training. Several semi-supervised ap-\nproaches have emerged to overcome this challenge, often employing CNN-based de-\ntectors with anchor proposals and post-processing techniques like non-maximal sup-\npression (NMS). However, recent advancements in the field have shifted the focus\ntowards transformer-based techniques, eliminating the need for NMS and empha-\nsizing object queries and attention mechanisms. Previous research has focused on\ntwo key areas to improve transformer-based detectors: refining the quality of object\nqueries and optimizing attention mechanisms. However, increasing object queries\ncan introduce redundancy, while adjustments to the attention mechanism can in-\ncrease complexity. To address these challenges, we introduce a semi-supervised ap-\nproach employing SAM-DETR, a novel approach for precise alignment between ob-\nject queries and target features. Our approach demonstrates remarkable reductions\nin false positives and substantial enhancements in table detection performance, par-\nticularly in complex documents characterized by diverse table structures. This work\nprovides more efficient and accurate table detection in semi-supervised settings.",
            "Keywords": "Semi-Supervised Learning \u00b7 Detection Transformer \u00b7 SAM-DETR \u00b7 Ta-\nble Analysis \u00b7 Table Detection."
        },
        {
            "ID": "30",
            "Authors": [
                "Gang Yao",
                "Liangrui Peng",
                "Kemeng Zhao",
                "Ning Ding",
                "Tianqi Zhao",
                "Yao Tao",
                "Pei Tang"
            ],
            "Title": "Visual Prompt Learning for Chinese Handwriting Recognition",
            "Abstract": "Recognizing Chinese handwriting in unconstrained scenar-\nios remains a challenging task due to wide variations in writing styles\nand imaging conditions. Recently, prompt learning in natural language\nprocessing has shown success in leveraging context awareness in various\ndomains. This paper explores the idea of incorporating visual prompt\nlearning into an encoder-decoder model for handwriting recognition. For\nthe encoder, multi-scale meta prompts are incorporated to utilize con-\ntextual information in internal feature representations. For the decoder,\nadditional character-level visual prompts are used along with the em-\nbeddings of previously predicted text to guide the decoding process. Ex-\nperiments conducted on the SCUT-HCCDoc, SCUT-EPT and CASIA-\nHWDB Chinese handwriting datasets validate the effectiveness of the\nproposed methods.",
            "Keywords": "handwriting recognition \u00b7 visual prompt learning \u00b7 encoder-\ndecoder model"
        },
        {
            "ID": "31",
            "Authors": [
                "Gang Yao",
                "Liangrui Peng",
                "Kemeng Zhao",
                "Ning Ding",
                "Chengyu Deng",
                "Tianqi Zhao",
                "Yao Tao"
            ],
            "Title": "Geometric-aware Control in Diffusion Model for Handwritten Chinese Font Generation",
            "Abstract": "With the progress of artificial intelligence generated content\n(AIGC) technologies, style-controlled image generation methods based\non diffusion models have shown promising performance on different tasks\nincluding Chinese font generation. However, most of the current diffu-\nsion model-based Chinese font generation methods focus on generating\nprinted Chinese character images and pay insufficient attention to the ge-\nometric characteristics of character images with variations in handwriting\nstyle. This paper investigates incorporating geometric-aware control into\ndiffusion models to generate target character images with new content\ntemplates and given style references. First, a deformable attention mech-\nanism is utilized in the content aggregation process to adapt to variations\nin handwritten character structure. Second, the edge contour of the new\ncontent template is incorporated into the style control process. Third,\nthe geometric information such as the corner points of a target charac-\nter is utilized to weight the image reconstruction loss function for better\ncontrol of character shape. The effectiveness of the proposed method for\nhandwritten Chinese font generation is validated on the CASIA-HWDB\n1.0/1.1 Chinese handwriting dataset.",
            "Keywords": "handwritten font generation \u00b7 diffusion model \u00b7 geometric-\naware control."
        },
        {
            "ID": "32",
            "Authors": [
                "M. Saquib Sarfraz",
                "Rainer Stiefelhagen",
                "Omar Moured",
                "Jiaming Zhang"
            ],
            "Title": "AltChart: Enhancing VLM-based Chart Summarization Through Multi-Pretext Tasks",
            "Abstract": "Chart summarization is a crucial task for blind and visu-\nally impaired individuals as it is their primary means of accessing and\ninterpreting graphical data. Crafting high-quality descriptions is chal-\nlenging because it requires precise communication of essential details\nwithin the chart without vision perception. Many chart analysis meth-\nods, however, produce brief, unstructured responses that may contain\nsignificant hallucinations, affecting their reliability for blind people. To\naddress these challenges, this work presents three key contributions: (1)\nWe introduce the AltChart dataset, comprising 10,000 real chart images,\neach paired with a comprehensive summary that features long-context,\nand semantically rich annotations. (2) We propose a new method for\npretraining Vision-Language Models (VLMs) to learn fine-grained chart\nrepresentations through training with multiple pretext tasks, yielding a\nperformance gain with \u223c2.5%. (3) We conduct extensive evaluations of\nfour leading chart summarization models, analyzing how accessible their\ndescriptions are. Our dataset and codes are publicly available on our\nproject page: https://github.com/moured/AltChart.",
            "Keywords": "Alternative Text \u00b7 Text Semantics \u00b7 Pretext Tasks"
        },
        {
            "ID": "34",
            "Authors": [
                "Jos\u00e9 Andr\u00e9s",
                "Enrique Vidal",
                "Alejandro H. Toselli"
            ],
            "Title": "Mining and Analyzing Statistical Information from Untranscribed Form Images",
            "Abstract": "Large amounts of historical structured documents are avail-\nable in archives around the world. Here, we are interested in automat-\nically extracting statistical information of selected attributes contained\nin these documents. To this end, we propose a pipeline relying on prob-\nabilistic indexing and machine learning to mine and analyze relevant in-\nformation contained in historical form images. These ideas are assessed\non a large collection of images containing almost 295 000 forms with re-\nsults showing the adequateness of the proposed methods to perform the\nbig-data analytic task considered.",
            "Keywords": "Probabilistic Indexing, Big Data, Structured Documents,\nHandwritten Text Recognition"
        },
        {
            "ID": "36",
            "Authors": [
                "Isabelle Guyon",
                "Romain Egele",
                "Marcus Liwicki",
                "Birhanu Hailu Belay",
                "Bezawork Tilahun",
                "Tadele Mengiste",
                "Tesfa Tegegne"
            ],
            "Title": "A Historical Handwritten Dataset for Ethiopic OCR with Baseline Models and Human-level Performance",
            "Abstract": "This paper introduces a new OCR dataset for historical\nhandwritten Ethiopic script, characterized by a unique syllabic writ-\ning system, low-resource availability, and complex orthographic diacrit-\nics. The dataset consists of roughly 80,000 annotated text-line images\nfrom 1700 pages of 18th to 20th century documents, including a train-\ning set with text-line images from the 19th to 20th century and two\ntest sets. One is distributed similarly to the training set with nearly\n6,000 text-line images, and the other contains only images from the\n18th century manuscripts, with around 16,000 images. The former test\nset allows us to check baseline performance in the classical IID setting\n(Independently and Identically Distributed), while the latter addresses\na more realistic setting in which the test set is drawn from a differ-\nent distribution than the training set (Out-Of-Distribution or OOD).\nMultiple annotators labeled all text-line images for the HHD-Ethiopic\ndataset, and an expert supervisor double-checked them. We assessed\nhuman-level recognition performance and compared it with state-of-the-\nart (SOTA) OCR models using the Character Error Rate (CER) and\nNormalized Edit Distance (NED) metrics. Our results show that the\nmodel performed comparably to human-level recognition on the 18th\ncentury test set and outperformed humans on the IID test set. However,\nthe unique challenges posed by the Ethiopic script, such as detecting\ncomplex diacritics, still present difficulties for the models. Our baseline\nevaluation and dataset will encourage further research on Ethiopic script\nrecognition. The dataset and source code can be accessed at https:\n//github.com/bdu-birhanu/HHD-Ethiopic.",
            "Keywords": "Historical Ethiopic script \u00b7 Human-level recognition perfor-\nmance \u00b7 HHD-Ethiopic \u00b7 Normalized edit distance \u00b7 Text recognition"
        },
        {
            "ID": "37",
            "Authors": [
                "Robert Sablatnig",
                "Rafael Sterzinger",
                "Simon Brenner"
            ],
            "Title": "Drawing the Line: Deep Segmentation for Extracting Art from Ancient Etruscan Mirrors",
            "Abstract": "Etruscan mirrors constitute a significant category within Etr-\nuscan art and, therefore, undergo systematic examinations to obtain in-\nsights into ancient times. A crucial aspect of their analysis involves the\nlabor-intensive task of manually tracing engravings from the backside.\nAdditionally, this task is inherently challenging due to the damage these\nmirrors have sustained, introducing subjectivity into the process. We ad-\ndress these challenges by automating the process through photometric-\nstereo scanning in conjunction with deep segmentation networks which,\nhowever, requires effective usage of the limited data at hand. We ac-\ncomplish this by incorporating predictions on a per-patch level, and\nvarious data augmentations, as well as exploring self-supervised learn-\ning. Compared to our baseline, we improve predictive performance w.r.t.\nthe pseudo-F-Measure by around 16%. When assessing performance on\ncomplete mirrors against a human baseline, our approach yields quantita-\ntive similar performance to a human annotator and significantly outper-\nforms existing binarization methods. With our proposed methodology,\nwe streamline the annotation process, enhance its objectivity, and reduce\noverall workload, offering a valuable contribution to the examination of\nthese historical artifacts and other non-traditional documents.",
            "Keywords": "Binarization \u00b7 Photometric Stereo \u00b7 Image Segmentation \u00b7\nLimited Data \u00b7 Etruscan Art \u00b7 Cultural Heritage"
        },
        {
            "ID": "38",
            "Authors": [
                "Michal Hradi\u0161",
                "Martin Ki\u0161\u0161"
            ],
            "Title": "Self-supervised Pre-training of Text Recognizers",
            "Abstract": "In this paper, we investigate self-supervised pre-training meth-\nods for document text recognition. Nowadays, large unlabeled datasets\ncan be collected for many research tasks, including text recognition, but\nit is costly to annotate them. Therefore, methods utilizing unlabeled data\nare researched. We study self-supervised pre-training methods based on\nmasked label prediction using three different approaches \u2013 Feature Quan-\ntization, VQ-VAE, and Post-Quantized AE. We also investigate joint-\nembedding approaches with VICReg and NT-Xent objectives, for which\nwe propose an image shifting technique to prevent model collapse where\nit relies solely on positional encoding while completely ignoring the input\nimage. We perform our experiments on historical handwritten (Bentham)\nand historical printed datasets mainly to investigate the benefits of the\nself-supervised pre-training techniques with different amounts of anno-\ntated target domain data. We use transfer learning as strong baselines.\nThe evaluation shows that the self-supervised pre-training on data from\nthe target domain is very effective, but it struggles to outperform transfer\nlearning from closely related domains. This paper is one of the first re-\nsearches exploring self-supervised pre-training in document text recogni-\ntion, and we believe that it will become a cornerstone for future research\nin this area. We made our implementation of the investigated methods\npublicly available at https://github.com/DCGM/pero-pretraining.",
            "Keywords": "Self-supervised learning \u00b7 Text Recognition \u00b7 Pre-training \u00b7\nOCR \u00b7 HTR."
        },
        {
            "ID": "42",
            "Authors": [
                "Xin Xia",
                "Siyuan Li",
                "Guodong Ding"
            ],
            "Title": "LMTextSpotter: Towards Better Scene Text Spotting with Language Modeling in Transformer",
            "Abstract": "End-to-end text spotting aims to build a unified framework\nfor scene text detection and recognition. Recently, the DETR-like frame-\nwork has attracted great attention. However, most of such studies merely\nfocused on promoting the collaboration between two sub-tasks, without\nconsidering the importance of linguistic information in scene text. In\nthis paper, we propose a novel end-to-end text spotting model, termed\nas LMTextSpotter, which introduces language modeling capability into\nthe DETR-like text spotting framework. Specifically, we add an extra\nLM recognition branch besides the detection and commonly used CTC\nrecognition branch in most existing frameworks, which is trained with\nthe cross-entropy loss in autoregression manner. We backtrack the viterbi\npath given by CTC recognition branch to get an alignment between the\nqueries and characters, so as to train LM recognition branch without\nrelying on the character level annotations. After training phase, the LM\nrecognition branch can predict the character classes independently of the\nCTC recognition branch. Besides, we exploit the position priors provided\nby rotated bounding boxes and adopt a task-specific query strategy for\nfurther improving the performance. Comparison results demonstrate the\nsuperiority of our LMTextSpotter over previous state-of-the-art methods.\nAblation studies also prove the effectiveness of each component.",
            "Keywords": "Scene Text Spotting \u00b7 Deformable Attention \u00b7 CTC-based\nCharacter Recognition \u00b7 Language Modeling."
        },
        {
            "ID": "43",
            "Authors": [
                "Chixiang Ma",
                "Jiawei Wang",
                "Shunchi Zhang",
                "Kai Hu",
                "Zhuoyao Zhong",
                "Lei Sun",
                "Qiang Huo"
            ],
            "Title": "Dynamic Relation Transformer for Contextual Text Block Detection",
            "Abstract": "Contextual Text Block Detection (CTBD) is the task of\nidentifying coherent text blocks in complex natural scenes. Previous\nmethodologies have treated CTBD as either a visual relation extrac-\ntion problem from the perspective of computer vision or as a sequence\nmodeling problem from the perspective of natural language processing.\nWe introduce a new framework that frames CTBD as a graph gener-\nation problem. This methodology consists of two essential procedures:\nidentifying individual text units as graph nodes and discerning the se-\nquential reading order relationships among these units as graph edges.\nLeveraging the cutting-edge capabilities of DQ-DETR for node detec-\ntion, our framework innovates further by integrating a novel mechanism,\na Dynamic Relation Transformer (DRFormer), dedicated to edge gen-\neration. DRFormer incorporates a dual interactive transformer decoder\nthat manages a dynamic graph structure refinement process. Through\nthis iterative process, the model gradually enhances the accuracy of the\ngraph, ultimately resulting in improved precision in detecting contex-\ntual text blocks. Comprehensive experimental evaluations conducted on\nboth SCUT-CTW-Context and ReCTS-Context datasets substantiate\nthat our method achieves state-of-the-art results, underscoring the effec-\ntiveness and potential of our graph generation framework in advancing\nthe field of CTBD.",
            "Keywords": "Graph Generation \u00b7 Scene Text Detection \u00b7 Text Region\nDetection"
        },
        {
            "ID": "44",
            "Authors": [
                "Jiawei Wang",
                "Qiang Huo",
                "Kai Hu"
            ],
            "Title": "DLAFormer: An End-to-End Transformer For Document Layout Analysis",
            "Abstract": "Document layout analysis (DLA) is crucial for understand-\ning the physical layout and logical structure of documents, serving in-\nformation retrieval, document summarization, knowledge extraction, etc.\nHowever, previous studies have typically used separate models to address\nindividual sub-tasks within DLA, including table/figure detection, text\nregion detection, logical role classification, and reading order prediction.\nIn this work, we propose an end-to-end transformer-based approach for\ndocument layout analysis, called DLAFormer, which integrates all these\nsub-tasks into a single model. To achieve this, we treat various DLA\nsub-tasks (such as text region detection, logical role classification, and\nreading order prediction) as relation prediction problems and consoli-\ndate these relation prediction labels into a unified label space, allowing a\nunified relation prediction module to handle multiple tasks concurrently.\nAdditionally, we introduce a novel set of type-wise queries to enhance\nthe physical meaning of content queries in DETR. Moreover, we adopt\na coarse-to-fine strategy to accurately identify graphical page objects.\nExperimental results demonstrate that our proposed DLAFormer out-\nperforms previous approaches that employ multi-branch or multi-stage\narchitectures for multiple tasks on two document layout analysis bench-\nmarks, DocLayNet and Comp-HRDoc.",
            "Keywords": "Document Layout Analysis \u00b7 Relation Prediction \u00b7 Unified\nLabel Space"
        },
        {
            "ID": "45",
            "Authors": [
                "Jiawei Wang",
                "Weihong Lin",
                "Kai Hu",
                "Zhuoyao Zhong",
                "Lei Sun",
                "Qiang Huo"
            ],
            "Title": "UniVIE: A Unified Label Space Approach to Visual Information Extraction from Form-like Documents",
            "Abstract": "Existing methods for Visual Information Extraction (VIE)\nfrom form-like documents typically fragment the process into separate\nsubtasks, such as key information extraction, key-value pair extraction,\nand choice group extraction. However, these approaches often overlook\nthe hierarchical structure of form documents, including hierarchical key-\nvalue pairs and hierarchical choice groups. To address these limitations,\nwe present a new perspective, reframing VIE as a relation prediction\nproblem and unifying labels of different tasks into a single label space.\nThis unified approach allows for the definition of various relation types\nand effectively tackles hierarchical relationships in form-like documents.\nIn line with this perspective, we present UniVIE, a unified model that\naddresses the VIE problem comprehensively. UniVIE functions using a\ncoarse-to-fine strategy. It initially generates tree proposals through a\nTree Proposal Network, which are subsequently refined into hierarchical\ntrees by a Relation Decoder module. To enhance the relation prediction\ncapabilities of UniVIE, we incorporate two novel tree constraints into\nthe Relation Decoder: a Tree Attention Mask and a Tree Level Embed-\nding. Extensive experimental evaluations on both our in-house dataset\nHierForms and a publicly available dataset SIBR, substantiate that our\nmethod achieves state-of-the-art results, underscoring the effectiveness\nand potential of our unified approach in advancing the field of VIE.",
            "Keywords": "Visual Information Extraction \u00b7 Relation Prediction \u00b7 Uni-\nfied Label Space.\n\u2217Work done when Kai Hu and Jiawei Wang were interns, Weihong Lin, Zhuoyao\nZhong and Lei Sun were full-time employees in Multi-Modal Interaction Group, Mi-\ncrosoft Research Asia, Beijing, China.\n\u2020Correspondence to: Kai Hu at hk970213@mail.ustc.edu.cn."
        },
        {
            "ID": "46",
            "Authors": [
                "Lionel Kesztenbaum",
                "Yoann Schneider",
                "Bastien Abadie",
                "M\u00e9lodie Boillet",
                "Christopher Kermorvant",
                "Sol\u00e8ne Tarride"
            ],
            "Title": "The Socface Project: Large-Scale Collection Processing and Analysis of a Century of French Censuses",
            "Abstract": "This paper presents a complete processing workflow for ex-\ntracting information from French census lists from 1836 to 1936. These\nlists contain information about individuals living in France and their\nhouseholds. We aim at extracting all the information contained in these\ntables using automatic handwritten table recognition. At the end of the\nSocface project, in which our work is taking place, the extracted in-\nformation will be redistributed to the departmental archives, and the\nnominative lists will be freely available to the public, allowing anyone to\nbrowse hundreds of millions of records. The extracted data will be used\nby demographers to analyze social change over time, significantly im-\nproving our understanding of French economic and social structures. For\nthis project, we developed a complete processing workflow: large-scale\ndata collection from French departmental archives, collaborative anno-\ntation of documents, training of handwritten table text and structure\nrecognition models, and mass processing of millions of images.\nWe present the tools we have developed to easily collect and process\nmillions of pages. We also show that it is possible to process such a wide\nvariety of tables with a single table recognition model that uses the image\nof the entire page to recognize information about individuals, categorize\nthem and automatically group them into households. The entire process\nhas been successfully used to process the documents of a departmental\narchive, representing more than 450,000 images.",
            "Keywords": "Handwritten table recognition \u00b7 Large-scale data collection\n\u00b7 Collaborative annotation."
        },
        {
            "ID": "47",
            "Authors": [
                "Glen Pouliquen",
                "Joseph Chazalon",
                "Thierry G\u00e9raud",
                "Ahmad Montaser Awal",
                "Guillaume Chiron"
            ],
            "Title": "Weakly Supervised Training for Hologram Verification in Identity Documents",
            "Abstract": "We propose a method to remotely verify the authenticity of\nOptically Variable Devices (OVDs), often referred to as \u201cholograms\u201d,\nin identity documents. Our method processes video clips captured with\nsmartphones under common lighting conditions, and is evaluated on two\npublic datasets: MIDV-HOLO and MIDV-2020. Thanks to a weakly-\nsupervised training, we optimize a feature extraction and decision pipeline\nwhich achieves a new leading performance on MIDV-HOLO, while main-\ntaining a high recall on documents from MIDV-2020 used as attack sam-\nples. It is also the first method, to date, to effectively address the photo\nreplacement attack task, and can be trained on either genuine samples,\nattack samples, or both for increased performance. By enabling to verify\nOVD shapes and dynamics with very little supervision, this work opens\nthe way towards the use of massive amounts of unlabeled data to build ro-\nbust remote identity document verification systems on commodity smart-\nphones. Code is available at https://github.com/EPITAResearchLab/\npouliquen.24.icdar.",
            "Keywords": "Know Your Consumer (KYC) \u00b7 Identity Documents \u00b7 Holo-\ngram Verification \u00b7 Weakly Supervised Learning \u00b7 Contrastive Loss"
        },
        {
            "ID": "49",
            "Authors": [
                "Meixuan Qiao",
                "Lujun Tian",
                "Qiyu Hou",
                "Jun Wang"
            ],
            "Title": "Synthesizing Realistic Data for Table Recognition",
            "Abstract": "To overcome the limitations and challenges of current au-\ntomatic table data annotation methods and random table data synthe-\nsis approaches, we propose a novel method for synthesizing annotation\ndata specifically designed for table recognition. This method utilizes the\nstructure and content of existing complex tables, facilitating the efficient\ncreation of tables that closely replicate the authentic styles found in the\ntarget domain. By leveraging the actual structure and content of tables\nfrom Chinese financial announcements, we have developed the first ex-\ntensive table annotation dataset in this domain. We used this dataset\nto train several recent deep learning-based end-to-end table recognition\nmodels. Additionally, we have established the inaugural benchmark for\nreal-world complex tables in the Chinese financial announcement do-\nmain, using it to assess the performance of models trained on our syn-\nthetic data, thereby effectively validating our method\u2019s practicality and\neffectiveness. Furthermore, we applied our synthesis method to augment\nthe FinTabNet dataset, extracted from English financial announcements,\nby increasing the proportion of tables with multiple spanning cells to in-\ntroduce greater complexity. Our experiments show that models trained\non this augmented dataset achieve comprehensive improvements in per-\nformance, especially in the recognition of tables with multiple spanning\ncells.",
            "Keywords": "Table Data Synthesis \u00b7 Data Augmentation \u00b7 Table Recog-\nnition"
        },
        {
            "ID": "50",
            "Authors": [
                "Akarsh Upadhyay",
                "Rudrabha Mukhopadhyay",
                "Akkshita Trivedi",
                "Santanu Chaudhury"
            ],
            "Title": "GDP: Generic Document Pretraining to Improve Document Understanding",
            "Abstract": "In this paper, we propose a novel pretraining approach for\ndocument analysis that advances beyond conventional methods. The ap-\nproach, called the GDPerformer, trains a suite of unique architectures\nto predict both masked OCR tokens and masked OCR bounding boxes,\nfostering the network to learn document semantics such as structure and\nlanguage. Our experiments with GDPerformerv1 and GDPerformerv2\nshow enhanced performance on various downstream tasks, including Se-\nmantic Entity Recognition and Extraction and Multi-Modal Document\nClassification with minimal task-specific data and generalization to a\nwide range of documents. Furthermore, our pretrained features exhibit\nrobustness in handling noisy documents and can be easily extended to\nmultiple languages. Our experiments indicate that the proposed pretrain-\ning strategy requires only 50K document images, making it particularly\nbeneficial for low-resource languages.",
            "Keywords": "Pretraining \u00b7 Document Analysis \u00b7 Masked Language Mod-\neling \u00b7 Document Transformers \u00b7 Document Imaging \u00b7 Deep Learning \u00b7\nMasked Layout Modeling"
        },
        {
            "ID": "51",
            "Authors": [
                "David Rizo",
                "Jorge Calvo - Zaragoza",
                "Adri\u00e1n Rosell\u00f3",
                "Eliseo Fuentes - Mart\u00ednez",
                "Mar\u00eda Alfaro - Contreras"
            ],
            "Title": "Source-Free Domain Adaptation for Optical Music Recognition",
            "Abstract": "This work addresses the problem of Domain Adaptation\n(DA) in the context of staff-level end-to-end Optical Music Recognition.\nSpecifically, we consider a source-free DA approach to adapt a given\ntrained model to a new collection\u2014an extremely useful scenario for pre-\nserving musical heritage. The method involves re-training the pre-trained\nmodel to align the statistics stored from the original data in normal-\nization layers with those of the new collection, while also including a\nregularization mechanism to prevent the model from converging to un-\ndesirable solutions. Unlike conventional DA techniques, this approach is\nvery efficient and practical, as it only requires the pre-trained model and\nunlabeled data from the new collection, without relying on data from the\noriginal training collections (i.e., source-free). Evaluation of diverse music\ncollections in Mensural notation and a synthetic-to-real scenario of com-\nmon Western modern notation demonstrates consistent improvements\nover the baseline (no DA), often with remarkable relative improvements.",
            "Keywords": "Optical Music Recognition \u00b7 Domain Adaptation \u00b7\nSource-Free Domain Adaptation"
        },
        {
            "ID": "54",
            "Authors": [
                "Antonio Javier Gallego",
                "Francisco J. Castellanos",
                "Alejandro Gal\u00e1n - Cuenca",
                "Juan P. Martinez - Esteso"
            ],
            "Title": "A region-based approach for layout analysis of music score images in scarce data scenarios",
            "Abstract": "This work presents a novel region-based layout analysis (LA)\nmethod for Optical Music Recognition (OMR) systems, aimed at over-\ncoming the data scarcity challenge. Contemporary OMR techniques,\ngrounded in machine learning principles, have a critical requirement: a\nlabeled dataset for training. This presents a practical challenge due to\nthe extensive manual effort required, coupled with the fact that the avail-\nability of suitable data for creating training sets is not always guaran-\nteed. Unlike other approaches, our method focuses on adapting the train-\ning and sample extraction processes within an existing neural network\nframework. Our approach incorporates a labeled data-driven oversam-\npling technique, a masking layer to enable training with partial labeling,\nand an adaptive scaling process to improve results for varying score sizes.\nThrough comprehensive experimentation, we established the minimal la-\nbeled data necessary for an effective model and demonstrated that our\nmethod could achieve a performance comparable with the state-of-the-\nart with just 8 to 32 labeled samples. The implications of our research\nextend beyond improving LA, providing a scalable and practical solution\nfor digitizing and preserving musical documents.",
            "Keywords": "\u00b7 Few-shot Learning \u00b7 Deep Learning \u00b7 Layout Analysis \u00b7\nObject Detection \u00b7 Optical Music Recognition."
        },
        {
            "ID": "55",
            "Authors": [
                "Namwook Kim",
                "Taehee Lee",
                "Yun Young Choi",
                "Taehoon Kim",
                "Seongho Joe"
            ],
            "Title": "End to End Table Transformer",
            "Abstract": "Table extraction (TE) task in document images is important\nin deep learning for conveying structured information. TE was decom-\nposed into three subtasks: table detection (TD), table structure recog-\nnition (TSR), and functional analysis (FA). Most of previous research\nfocused on developing models specifically tailored to each of these tasks,\nleading to challenges in computational cost, model size, and performance\nlimitations. Transformer-based object detection models are being suc-\ncessfully applied to TE subtasks, yet they face inherent challenges due\nto the one-to-one set matching approach for detecting objects. This ap-\nproach assigns only a few queries as positive samples, diminishing the\ntraining efficacy of these samples and leading to a performance bot-\ntleneck. Therefore, prior research in the object detection field has in-\ntroduced modifications to the Detection Transformer (DETR), adding\nadditional queries and training schemes that improve performance. In\nthis work, we introduce the End-to-end Table Transformer (ETT), a\nspecialized transformer-based object detection model designed for high-\nperforming TE from document images with single model. Our model\ncomprises three key components: a backbone, the Deformable DETR\n(DDETR) model, and the novel layout analysis module with table layout\nloss. This layout analysis module leverages explicit relationships between\ntable objects to enhance the table extraction task performance in multi\ntables in images. We conduct rigorous experiments to assess the efficacy\nof our proposed model against table extraction benchmark datasets, com-\nparing it with other DETR variants, including vanilla DETR, DDETR,\nand H-DETR. Empirical evaluations highlight that our model efficiently\nsecures state-of-the-art results in TE task.",
            "Keywords": "Document analysis systems \u00b7 Document image processing \u00b7\nPhysical and logical layout analysis \u00b7 Tables and charts."
        },
        {
            "ID": "58",
            "Authors": [
                "Eric Ayllon",
                "Francisco J. Castellanos",
                "Jorge Calvo - Zaragoza"
            ],
            "Title": "Analysis of the Calibration of Handwriting Text Recognition Models",
            "Abstract": "Current neural approaches for Handwritten Text Recogni-\ntion (HTR) have proven to be successful in many settings, but their per-\nformance can be unpredictable when facing new data. In this context,\nit is essential to correctly estimate an approximate error of the target\npredictions. To achieve this, the model must be well calibrated, meaning\nthat the confidence values are sufficiently representative of the expected\naccuracy. Calibration is a crucial aspect in practical applications of HTR,\nbut the topic remains overly underexplored. In this paper, we fill this gap\nby studying calibration in state-of-the-art HTR models, along with spe-\ncific techniques for this purpose. We perform thorough experiments on\nseveral datasets, both in a classic setting and in cross-collection scenar-\nios. Our results report interesting conclusions about the calibration of\nHTR, highlighting their strengths, weaknesses, and the extent to which\nthe considered strategies improve results.",
            "Keywords": "Handwritten Text Recognition \u00b7 Calibration \u00b7 Convolutional\nRecurrent Neural Network"
        },
        {
            "ID": "59",
            "Authors": [
                "Xudong Yan",
                "Cheng",
                "Sen Dai",
                "He",
                "Lin Liu",
                "Hui Li",
                "Xiao",
                "Shuqi Mei",
                "Fei Yin"
            ],
            "Title": "GraphMLLM: A Graph-based Multi-level Layout Language-independent Model for Document Understanding",
            "Abstract": "Self-supervised multi-modal document pre-training for docu-\nment knowledge learning shows superiority in various downstream tasks.\nHowever, due to the diversity of document languages and structures,\nthere is still room to better model various document layouts while effi-\nciently utilizing the pre-trained language models. To this goal, this pa-\nper proposes a Graph-based Multi-level Layout Language-independent\nModel (GraphMLLM) which uses dual-stream structure to explore tex-\ntual and layout information separately and cooperatively. Specifically,\nGraphMLLM consists of a text stream which uses off-the-shelf pre-trained\nlanguage model to explore textual semantics and a layout stream which\nuses multi-level graph neural network (GNN) to model hierarchical page\nlayouts. Through the cooperation of the text stream and layout stream,\nGraphMLLM can model multi-level page layouts more comprehensively\nand improve the performance of language-independent document pre-\ntrained model. Experimental results show that compared with previ-\nous state-of-the-art methods, GraphMLLM yields higher performance on\ndownstream visual information extraction (VIE) tasks after pre-training\non less documents. Code and model will be available at https://github.\ncom/HSDai/GraphMLLM.",
            "Keywords": "Visual information extraction \u00b7 Self-supervised pre-training\n\u00b7 Multi-level page layouts."
        },
        {
            "ID": "60",
            "Authors": [
                "Takaya Kawakatsu"
            ],
            "Title": "Multi-Cell Decoder and Mutual Learning for Table Structure and Character Recognition",
            "Abstract": "Extracting table contents from documents such as scientific\npapers and financial reports and converting them into a format that can\nbe processed by large language models is an important task in knowledge\ninformation processing. End-to-end approaches, which recognize not only\ntable structure but also cell contents, achieved performance comparable\nto state-of-the-art models using external character recognition systems,\nand have potential for further improvements. In addition, these models\ncan now recognize long tables with hundreds of cells by introducing local\nattention. However, the models recognize table structure in one direction\nfrom the header to the footer, and cell content recognition is performed\nindependently for each cell, so there is no opportunity to retrieve useful\ninformation from the neighbor cells. In this paper, we propose a multi-cell\ncontent decoder and bidirectional mutual learning mechanism to improve\nthe end-to-end approach. The effectiveness is demonstrated on two large\ndatasets, and the experimental results show comparable performance to\nstate-of-the-art models, even for long tables with large numbers of cells.",
            "Keywords": "Deep Learning, Table Recognition, Transformer, Mutual Learning"
        },
        {
            "ID": "64",
            "Authors": [
                "Kei Nakatsuru",
                "Seiichi Uchida"
            ],
            "Title": "Learning to Kern: Set-wise Estimation of Optimal Letter Space",
            "Abstract": "Kerning is the task of setting appropriate horizontal spaces\nfor all possible letter pairs of a certain font. One of the di\ufb00iculties of kern-\ning is that the appropriate space differs for each letter pair. Therefore, for\na total of 52 capital and small letters, we need to adjust 52 \u00d7 52 = 2704\ndifferent spaces. Another di\ufb00iculty is that there is neither a general pro-\ncedure nor criterion for automatic kerning; therefore, kerning is still done\nmanually or with heuristics. In this paper, we tackle kerning by proposing\ntwo machine-learning models, called pairwise and set-wise models. The\nformer is a simple deep neural network that estimates the letter space\nfor two given letter images. In contrast, the latter is a transformer-based\nmodel that estimates the letter spaces for three or more given letter\nimages. For example, the set-wise model simultaneously estimates 2704\nspaces for 52 letter images for a certain font. Among the two models, the\nset-wise model is not only more e\ufb00icient but also more accurate because\nits internal self-attention mechanism allows for more consistent kerning\nfor all letters. Experimental results on about 2500 Google fonts and their\nquantitative and qualitative analyses show that the set-wise model has\nan average estimation error of only about 5.3 pixels when the average\nletter space of all fonts and letter pairs is about 115 pixels.",
            "Keywords": "kerning \u00b7 letter spacing \u00b7 machine learning."
        },
        {
            "ID": "69",
            "Authors": [
                "Elisa H Barney Smith",
                "Chang Liu",
                "Simon Corbill\u00e9"
            ],
            "Title": "MOoSE: Multi-Orientation Sharing Experts for Open-set Scene Text Recognition",
            "Abstract": "Open-set text recognition, which aims to address both novel\ncharacters and previously seen ones, is one of the rising subtopics in\nthe text recognition field. However, the current open-set text recognition\nsolutions only focuses on horizontal text, which fail to model the real-\nlife challenges posed by the variety of writing directions in real-world\nscene text. Multi-orientation text recognition, in general, faces challenges\nfrom diverse image aspect ratios, significant imbalance in data amount,\nand domain gaps between orientations. In this work, we first propose a\nMulti-Oriented Open-Set Text Recognition task (MOOSTR) to model\nthe challenges of both novel characters and writing direction variety. We\nthen propose a Multi-Orientation Sharing Experts (MOoSE) framework\nas a strong baseline solution. MOoSE uses a mixture-of-experts scheme\nto alleviate the domain gaps between orientations, while exploiting com-\nmon structural knowledge among experts to alleviate the data scarcity\nthat some experts face. The proposed MOoSE framework is validated\nby ablative experiments, and also tested for feasibility on an existing\nopen-set text recogntion benchmark. Code, models, and documents are\navailable at: https://github.com/lancercat/Moose/",
            "Keywords": "Open-set text recognition, multi-orientation text recognition, in-\ncremental learning"
        },
        {
            "ID": "71",
            "Authors": [
                "Zhang Yilun",
                "Meng Xianghui",
                "Xie Youbai",
                "Pan Qiangang",
                "Hu Yahong"
            ],
            "Title": "Deep Learning-Driven Innovative Model for Generating Functional Knowledge Units",
            "Abstract": "Design science research shows that existing knowledge is the\nbasis for product design. The functional knowledge unit is the most basic\nknowledge to describe the functional design knowledge. Nowadays, the\nacquisition of functional units is mainly manual, which is time-consuming\nand labor-intensive. Functional knowledge integration is an effective way\nto achieve innovation design, yet the insufficient functional units can-\nnot effectively support the integration. To address the above issue, this\npaper proposes a named-entity recognition (NER) model called Bound-\nary Perception NER (BP-NER). From the product manual, BP-NER\ncan automatically extract information necessary to describe the func-\ntional unit. The model leverages entity boundary information to predict\nentity classification labels and incorporates semantically-rich character-\nlevel feature information. BP-NER also introduces FocalLoss function\nto solve the problem of label imbalance. Experiments on the functional\nunit dataset demonstrate the effectiveness of the proposed model. Com-\npared with the baseline model BERT-BiLSTM-CRF, BP-NER increases\nthe overall label prediction accuracy by 5.05%, and the average F1-score\nimprovement is 32.8% for entities CIN,COT,DIN,DOT and ENY.",
            "Keywords": "Natural Language Processing \u00b7 Named Entity Recognition\n\u00b7 Product innovative Design \u00b7 Perception Awareness."
        },
        {
            "ID": "74",
            "Authors": [
                "Jeff Yang",
                "Huynh Vu The",
                "Van Pham Hoai"
            ],
            "Title": "One-shot Transformer-based Framework for Visually-Rich Document Understanding",
            "Abstract": "There is a growing need for efficient entity extraction (EE)\nfrom business documents. While recent EE models have shown good ac-\ncuracy for a variety of document templates, fine-tuning these models and\nacquiring additional training data can be expensive. To address this prob-\nlem, we propose a novel template-based system for the EE task which\ndoes not require model fine-tuning for new entities and templates. The\nsystem includes two one-shot transformer-based models: one for template\nrecognition and the other for entity recognition. The document recogni-\ntion model (OTDC) achieves high accuracy (over 93%) on more than 200\ntemplates of public and private datasets. The entity recognition model\n(OTER) outperforms recent zero-shot models with regards to the full set\nof labeled entities in the public SROIE datasets. We have also gathered\nand annotated the public RVL-CDIP and invoice datasets to showcase\nthe generalization of our OTER models for the EE task across a wide\nrange of document templates, containing both single and multiple-region\nfields.",
            "Keywords": "Entity extraction \u00b7 Visually-Rich document understanding\n\u00b7 One-shot approaches."
        },
        {
            "ID": "75",
            "Authors": [
                "Wanqing Li",
                "Jie Yang",
                "Wangli Yang",
                "Yi Guo"
            ],
            "Title": "ConClue: Conditional Clue Extraction for Multiple Choice Question Answering",
            "Abstract": "The task of Multiple Choice Question Answering (MCQA)\naims to identify the correct answer from a set of candidates, given a back-\nground passage and an associated question. Considerable research efforts\nhave been dedicated to addressing this task, leveraging a diversity of se-\nmantic matching techniques to estimate the alignment among the answer,\npassage, and question. However, key challenges arise as not all sentences\nfrom the passage contribute to the question answering, while only a few\nsupporting sentences (clues) are useful. Existing clue extraction methods\nsuffer from inefficiencies in identifying supporting sentences, relying on\nresource-intensive algorithms, pseudo labels, or overlooking the semantic\ncoherence of the original passage. Addressing this gap, this paper intro-\nduces a novel extraction approach, termed Conditional Clue extractor\n(ConClue), for MCQA. ConClue leverages the principles of Conditional\nOptimal Transport to effectively identify clues by transporting the se-\nmantic meaning of one or several words (from the original passage) to\nselected words (within identified clues), under the prior condition of the\nquestion and answer. Empirical studies on several competitive bench-\nmarks consistently demonstrate the superiority of our proposed method\nover different traditional approaches, with a substantial average improve-\nment of 1.1-2.5 absolute percentage points in answering accuracy.",
            "Keywords": "Multiple Choice Question Answering \u00b7 Optimal Transport\n\u00b7 Clue Extraction \u00b7 Machine Reading Comprehension"
        },
        {
            "ID": "77",
            "Authors": [
                "Mingxin Huang",
                "Xudong Xie",
                "Xiang Bai",
                "Linger Deng",
                "Lianwen Jin",
                "Yuliang Liu"
            ],
            "Title": "Progressive Evolution from Single-Point to Polygon for Scene Text",
            "Abstract": "The advancement of text shape representations towards com-\npactness has enhanced text detection and spotting performance, but at\na high annotation cost. Current models use single-point annotations to\nreduce costs, yet they lack sufficient localization information for down-\nstream applications. To overcome this limitation, we introduce Point2Pol-\nygon, which can efficiently transform single-points into compact poly-\ngons. Our method uses a coarse-to-fine process, starting with creating\nand selecting anchor points based on recognition confidence, then ver-\ntically and horizontally refining the polygon using recognition informa-\ntion to optimize its shape. We demonstrate the accuracy of the gener-\nated polygons through extensive experiments: 1) By creating polygons\nfrom ground truth points, we achieved an accuracy of 82.0% on ICDAR\n2015; 2) In training detectors with polygons generated by our method,\nwe attained 86% of the accuracy relative to training with ground truth\n(GT); 3) Additionally, the proposed Point2Polygon can be seamlessly\nintegrated to empower single-point spotters to generate polygons. This\nintegration led to an impressive 82.5% accuracy for the generated poly-\ngons. It is worth mentioning that our method relies solely on synthetic\nrecognition information, eliminating the need for any manual annotation\nbeyond single points.",
            "Keywords": "Text Representations \u00b7 Text Detection \u00b7 Single Point \u00b7 Recog-\nnition Information."
        },
        {
            "ID": "78",
            "Authors": [
                "Brian Kenji Iwana",
                "Yoh Yamashita"
            ],
            "Title": "Test Time Augmentation as a Defense Against Adversarial Attacks on Online Handwriting",
            "Abstract": "Neural networks have been shown to be weak against adver-\nsarial attacks. This study examines the effects of adversarial attacks on\nonline handwritten characters and proposes a method to defend against\nsuch attacks. In order to make temporal neural networks more robust to\nadversarial attacks, we propose using Test Time Augmentation (TTA).\nTTA combines the predictions of transformed inputs with a trained clas-\nsifier. We adapt TTA and propose its usage to make temporal neural\nnetworks more robust to adversarial attacks. The proposed method is\nevaluated using online handwritten characters and against four state-of-\nthe-art adversarial attacks. We demonstrate that the nontraditional use\nof TTA can be used to protect against these attacks for almost no cost.",
            "Keywords": "Online Handwriting \u00b7 Adversarial Attacks \u00b7 Defense"
        },
        {
            "ID": "79",
            "Authors": [
                "Daichi Haraguchi",
                "Seiichi Uchida",
                "Kazuki Kitajima"
            ],
            "Title": "Font Impression Estimation in the Wild",
            "Abstract": "This paper addresses the challenging task of estimating font\nimpressions from real font images. We use a font dataset with annota-\ntion about font impressions and a convolutional neural network (CNN)\nframework for this task. However, impressions attached to individual\nfonts are often missing and noisy because of the subjective characteristic\nof font impression annotation. To realize stable impression estimation\neven with such a dataset, we propose an exemplar-based impression esti-\nmation approach, which relies on a strategy of ensembling impressions of\nexemplar fonts that are similar to the input image. In addition, we train\nCNN with synthetic font images that mimic scanned word images so\nthat CNN estimates impressions of font images in the wild. We evaluate\nthe basic performance of the proposed estimation method quantitatively\nand qualitatively. Then, we conduct a correlation analysis between book\ngenres and font impressions on real book cover images; it is important\nto note that this analysis is only possible with our impression estimation\nmethod. The analysis reveals various trends in the correlation between\nthem \u2014 this fact supports a hypothesis that book cover designers care-\nfully choose a font for a book cover considering the impression given by\nthe font.",
            "Keywords": "Font impression \u00b7 Impression estimation \u00b7 Book covers."
        },
        {
            "ID": "80",
            "Authors": [
                "KhayTze Peong",
                "Seiichi Uchida",
                "Daichi Haraguchi"
            ],
            "Title": "Typographic Text Generation with Off-the-Shelf Diffusion Model",
            "Abstract": "Recent diffusion-based generative models show promise in\ntheir ability to generate text images, but limitations in specifying the\nstyles of the generated texts render them insu\ufb00icient in the realm of\ntypographic design. This paper proposes a typographic text generation\nsystem to add and modify text on typographic designs while specifying\nfont styles, colors, and text effects. The proposed system is a novel combi-\nnation of two off-the-shelf methods for diffusion models, ControlNet and\nBlended Latent Diffusion. The former functions to generate text images\nunder the guidance of edge conditions specifying stroke contours. The\nlatter blends latent noise in Latent Diffusion Models (LDM) to add ty-\npographic text naturally onto an existing background. We first show that\ngiven appropriate text edges, ControlNet can generate texts in specified\nfonts while incorporating effects described by prompts. We further in-\ntroduce text edge manipulation as an intuitive and customizable way to\nproduce texts with complex effects such as \u201cshadows\u201d and \u201creflections\u201d.\nFinally, with the proposed system, we successfully add and modify texts\non a predefined background while preserving its overall coherence.",
            "Keywords": "Text generation \u00b7 diffusion models \u00b7 image blending."
        },
        {
            "ID": "81",
            "Authors": [
                "Zhen",
                "Feng Chen",
                "Xu",
                "Lun Mo",
                "Qi Liu",
                "Cheng Yin",
                "Song - Lu Chen"
            ],
            "Title": "Multi-task Learning for License Plate Recognition in Unconstrained Scenarios",
            "Abstract": "The recognition of license plates in natural scenes often face\nchallenges such as multi-directional and multi-line variations. Addition-\nally, previous studies have treated license plate detection and recognition\nas separate tasks, resulting in inefficiencies and error accumulation. To\naddress these challenges, we propose an end-to-end method for license\nplate detection and recognition using multi-task learning. Firstly, we in-\ntroduce two parallel branches to detect the horizontal bounding box and\nthe four corners of the license plate, enabling multi-directional license\nplate detection in a multi-task manner. The outputs from these branches\nare combined to enhance recognition accuracy. Secondly, we propose to\nextract global features to perceive character layout and utilize reading\norder to spatially attend to characters for recognizing multi-line license\nplates. Finally, we combine detection and recognition using the same\nbackbone, with the detection branch based on multiple deep layers and\nthe recognition branch based on multiple shallow layers, thereby con-\nstructing an end-to-end detection and recognition network. Comparative\nexperiments on CCPD and RodoSol datasets validate that our method\nsignificantly outperforms state-of-the-art methods, particularly in sce-\nnarios involving multi-directional and multi-line license plates.",
            "Keywords": "License plate recognition \u00b7 Multi-task \u00b7 Multi-directional \u00b7\nMulti-line \u00b7 End-to-end."
        },
        {
            "ID": "82",
            "Authors": [
                "Yusheng Xie",
                "Jishen Zhao",
                "Luis Goncalves",
                "Hui Shi",
                "Sicun Gao"
            ],
            "Title": "WikiDT: Visual-based Table Recognition and Question Answering Dataset",
            "Abstract": "Companies and organizations grapple with the daily burden of docu-\nment processing. As manual handling is tedious and error-prune, automating this\nprocess is a significant goal. In response to this demand, research on table extrac-\ntion and information extraction from scanned documents in gaining increasing\ntraction. These extractions are fulfilled by machine learning models that require\nlarge-scale and realistic datasets for development. However, despite the clear need,\nacquiring high-quality and comprehensive dataset can be costly. In this work, we\nintroduce the WikiDT, a TableVQA dataset with hierarchical labels for model\ndiagnosis and potentially benefit the research on sub-tasks, e.g. table recognition.\nThis dataset boasts a massive collection of 70,919 images paired with a diverse set\nof 159,905 tables, providing an extensive corpus for tacking question-answering\ntasks. The creation of WikiDT is by extending the existing non-synthetic QA\ndatasets, with a fully automated process with verified heuristics and manual qual-\nity inspections, and therefore minimizes labeling effort and human errors. A novel\nfocus of WikiDT and its design goal is to answer questions that require locating the\ntarget information fragment and in-depth reasoning, given web-style document im-\nages. We established the baseline performance on the TableVQA, table extraction,\nand table retrieval task with recent state-of-the-art models. The results illustrate\nthat WikiDT is yet solved by the existing models that work moderately well on\nother VQA tasks, and also introduce advanced challenges on table extraction.\n1\nIntroduction\nQuestion answering is widely accepted as an AI-completeness task, while visual question\nanswering (VQA) is an alternative to the visual Turing test [23]. VQA tasks, which\nrequire the integration of natural language and image understanding, attract tremendous\ninterest from both computer vision and natural language processing communities. In\ngeneral, VQA tasks can test a wide range of knowledge and inference skills, provided\nthey can be related to information within an image. While knowledge in the wild world\nis extensive, VQA tasks typically bound the domain of the images and questions to make\nthe task practical. General VQA tasks restrict the image domain to daily-life images\nwith commonly seen objects, such as GQA[15] and VQA-v2[12]; Scene-Text VQAs\nconfine their questions to text information on the images; and document VQAs ask\nquestions on the image-formed documents, for example, OCR-VQA[26], DocVQA[25],\nand VisualMRC[41].",
            "Keywords": null
        },
        {
            "ID": "83",
            "Authors": [
                "Daichi Haraguchi",
                "Seiichi Uchida",
                "Yugo Kubota"
            ],
            "Title": "Impression-CLIP: Contrastive Shape-Impression Embedding for Fonts",
            "Abstract": "Fonts convey different impressions to readers. These impres-\nsions often come from the font shapes. However, the correlation between\nfonts and their impression is weak and unstable because impressions\nare subjective. To capture such weak and unstable cross-modal correla-\ntion between font shapes and their impressions, we propose Impression-\nCLIP, which is a novel machine-learning model based on CLIP (Con-\ntrastive Language-Image Pre-training). By using the CLIP-based model,\nfont image features and their impression features are pulled closer, and\nfont image features and unrelated impression features are pushed apart.\nThis procedure realizes co-embedding between font image and their im-\npressions. In our experiment, we perform cross-modal retrieval between\nfonts and impressions through co-embedding. The results indicate that\nImpression-CLIP achieves better retrieval accuracy than the state-of-the-\nart method. Additionally, our model shows the robustness to noise and\nmissing tags.",
            "Keywords": "Contrastive Embedding \u00b7 Font style \u00b7 Impression."
        },
        {
            "ID": "84",
            "Authors": [
                "Kangsoo Jung",
                "Dimosthenis Karatzas",
                "Mohamed Ali Souibgui",
                "Ernest Valveny",
                "Antti Honkela",
                "Rub\u00e8n Tito",
                "Khanh Nguyen",
                "Lei Kang",
                "Joonas J\u00e4lk\u00f6",
                "Aurelie Joseph",
                "Vincent Poulain D \u2019 Andecy",
                "Marlon Tobaben",
                "Mario Fritz",
                "Raouf Kerkouche"
            ],
            "Title": "Privacy-Aware Document Visual Question Answering",
            "Abstract": "Document Visual Question Answering (DocVQA) has quickly\ngrown into a central task of document understanding. But despite the\nfact that documents contain sensitive or copyrighted information, none\nof the current DocVQA methods offers strong privacy guarantees.\nIn this work, we explore privacy in the domain of DocVQA for the first\ntime, highlighting privacy issues in state of the art multi-modal LLM\nmodels used for DocVQA, and explore possible solutions.\nSpecifically, we focus on invoice processing as a realistic document under-\nstanding scenario, and propose a large scale DocVQA dataset comprising\ninvoice documents and associated questions and answers. We employ a\nfederated learning scheme, that reflects the real-life distribution of doc-\numents in different businesses, and we explore the use case where the\ndata of the invoice provider is the sensitive information to be protected.\nWe demonstrate that non-private models tend to memorise, a behaviour\nthat can lead to exposing private information. We then evaluate baseline\ntraining schemes employing federated learning and differential privacy\nin this multi-modal scenario, where the sensitive information might be\nexposed through either or both of the two input modalities: vision (doc-\nument image) or language (OCR tokens).\nFinally, we design attacks exploiting the memorisation effect of the model,\nand demonstrate their effectiveness in probing a representative DocVQA\nmodels.",
            "Keywords": "DocVQA \u00b7 Federated Learning \u00b7 Differential Privacy"
        },
        {
            "ID": "86",
            "Authors": [
                "Roberto Paredes",
                "Dan Anitei",
                "Daniel Parres"
            ],
            "Title": "Handwritten Document Recognition Using Pre-trained Vision Transformers",
            "Abstract": "Handwritten document recognition (HDR) is a rapidly grow-\ning field that aims to automate the process of document transcription,\nstreamlining the traditional stages of segmentation and block recognition\ninherent in handwritten text recognition systems. However, the efficacy of\nHDR systems often encounters performance hurdles due to limited data\navailability, constraining their practical applicability. In addressing this\nchallenge, our study delves into leveraging pre-trained vision transformer\nmodels, proposing two distinct architectures. One architecture integrates\na transformer decoder, while the other relies on a decoder-free connec-\ntionist temporal classification approach. Consequently, we present a com-\nprehensive analysis focused on adapting and optimizing weights to en-\nhance performance efficiently. Our methodology encompasses exploring\nan optimal blend of hyperparameters through grid search, implementing\nfreezing strategies, utilizing gradient accumulation, and employing hard\nnegative mining techniques. This analysis demonstrates that our pro-\nposed efficient fine-tuning workflow significantly mitigates error rates and\nenhances inference speed, surpassing state-of-the-art results across most\nextended HDR benchmark datasets. Specifically, we outperform previous\nmethods on the READ 2016 single-page and double-page with a word er-\nror rate (WER) of 11.49% and 11.86%, the RIMES 2009 with a WER of\n9.58%, and MAURDOR C3 and C4 with a WER of 16.21% and 13.32%.\nThe results of this study have important implications for the develop-\nment of HDR systems, where our approach can lead to improved system\nperformance and enable wider adoption of this technology. Source code is\npublicly available at https://github.com/dparres/Pretrained-Document-\nRecognition-Transformers",
            "Keywords": "Vision Transformers \u00b7 Handwritten Document Recognition\n\u00b7 Pre-trained Models \u00b7 Connectionist Temporal Classification."
        },
        {
            "ID": "89",
            "Authors": [
                "Mick",
                "Antoine Doucet",
                "##\u00ebl Coustaty",
                "Carlos - Emiliano Gonz\u00e1lez - Gallardo",
                "Wenjun Sun",
                "Hanh Thi Hong Tran"
            ],
            "Title": "Global-SEG: Text Semantic Segmentation Based on Global Semantic Pair Relations",
            "Abstract": "Text semantic segmentation is a crucial task in language un-\nderstanding, as subsequent natural language processing tasks often re-\nquire cohesive semantic blocks. This paper introduces a new perspective\non this task by utilizing global semantic pair relations from both token-\nand sentence-level language models. This approach addresses the limi-\ntations of prior work, which concentrated solely on individual semantic\nunits like sentences. Our model processes both local and global levels of\nsentence semantics via encoders and then combines the semantics ob-\ntained at each stage into a semantic embedding matrix. This matrix is\nthen fed through a convolutional neural network and finally used as input\nthrough another encoder. This process enables the identification of se-\nmantic segmentation boundaries by describing the relationships of global\nsemantic pairs. Furthermore, we utilize semantic embeddings from large\nlanguage models and consider the positional information of text within\nthe document to assess their efficacy in augmenting semantics. We test\nour model with both contemporary and historical corpora, and the re-\nsults demonstrate that our approach outperforms benchmarks on each\ndataset.",
            "Keywords": "Text semantic segmentation \u00b7 Semantic pair relation \u00b7 His-\ntorical documents"
        },
        {
            "ID": "92",
            "Authors": [
                "Wei Yang",
                "Askar Hamdulla",
                "Mayire Ibrayim",
                "Yihong Luo"
            ],
            "Title": "More and Less: Enhancing Abundance and Refining Redundancy for Text-Prior-Guided Scene Text Image Super-Resolution",
            "Abstract": "Scene text image super-resolution (STISR) aims to enhance\nlow-resolution text images, boosting downstream text recognition tasks.\nRecent STISR models leverage text recognizer for prior information,\nachieving superior performance via a novel strategy. However, we observe\nabundant erroneous prior information from the low-resolution (LR) text\nimages processed by the text recognizer, which can mislead text recon-\nstruction when fused with image features. Therefore, we propose a novel\nsequential residual blocks, termed sequence refinement blocks, to refine\nthe merged features of text images and text priors during the recon-\nstruction of LR images. Additionally, regarding the widespread problem\nof ignoring the contextual semantic information in the shallow features\nof text images in the STISR, We introduce a multi-scale feature module\nto supplement the fine-grained and coarse-grained information required\nin the reconstruction of LR text images, which can well resolve infor-\nmation loss and generate more accurate super-resolution text images.\nOur proposed method consistently outperforms baselines employing text\nrecognizers ASTER, MORAN, and CRNN by 1-2% on TextZoom, and\nachieves impressive gains of 4-5% on the challenging hard subset when\nleveraging multi-modal recognizers like ABINet and MATRN. The gen-\neralization experiments on scene text recognition datasets demonstrate\noptimal 5-8% performance improvements over the baselines.",
            "Keywords": "super-resolution \u00b7 text prior \u00b7 text recognition"
        },
        {
            "ID": "94",
            "Authors": [
                "Jordy Van Landeghem",
                "Matthew Blaschko",
                "Sien Moens",
                "Souhail Bakkali",
                "Omar Hamed"
            ],
            "Title": "Multimodal Adaptive Inference for Document Image Classification with Anytime Early Exiting",
            "Abstract": "This work addresses the need for a balanced approach be-\ntween performance and efficiency in scalable production environments\nfor visually-rich document understanding (VDU) tasks. Currently, there\nis a reliance on large document foundation models that offer advanced\ncapabilities but come with a heavy computational burden. In this paper,\nwe propose a multimodal early exit (EE) model design that incorporates\nvarious training strategies, exit layer types and placements5. Our goal\nis to achieve a Pareto-optimal balance between predictive performance\nand efficiency for multimodal document image classification. Through a\n5 Code is available at https://github.com/Jordy-VL/multi-modal-early-exit",
            "Keywords": null
        },
        {
            "ID": "95",
            "Authors": [
                "Marco Spinaci",
                "Marek Polewczyk"
            ],
            "Title": "ClusterTabNet: Supervised clustering method for table detection and table structure recognition",
            "Abstract": "Table detection and recognition consists of locating tables\nwithin a given document and identifying the exact location of its pieces,\nsuch as rows, columns, and headers. We present a novel deep-learning-\nbased method1 to cluster words in documents which we apply to detect\nand recognize tables given the OCR output. We interpret table structure\nbottom-up as a graph of relations between pairs of words (belonging to\nthe same row, column, header, as well as to the same table) and use a\ntransformer encoder model to predict its adjacency matrix. We demon-\nstrate the performance of our method on the PubTables-1M dataset in-\ntroduced in [18] as well as PubTabNet and FinTabNet datasets. Com-\npared to the current state-of-the-art detection methods such as DETR [2]\nand Faster R-CNN [16], our method achieves similar or better accuracy,\nwhile requiring a significantly smaller model.",
            "Keywords": "Table detection \u00b7 Table recognition \u00b7 Supervised clustering\n\u00b7 Transformers"
        },
        {
            "ID": "97",
            "Authors": [
                "Carlos - D. Mart\u00ednez - Hinarejos",
                "Mois\u00e9s Pastor - Gadea",
                "Ver\u00f3nica Romero",
                "Aparisi",
                "David Villanova",
                "Christopher Kermorvant",
                "Sol\u00e8ne Tarride"
            ],
            "Title": "Reading Order Independent Metrics for Information Extraction in Handwritten Documents",
            "Abstract": "Information Extraction processes in handwritten documents\ntend to rely on obtaining an automatic transcription and performing\nNamed Entity Recognition (NER) over such transcription. For this rea-\nson, in publicly available datasets, the performance of the systems is usu-\nally evaluated with metrics particular to each dataset. Moreover, most\nof the metrics employed are sensitive to reading order errors. Therefore,\nthey do not reflect the expected final application of the system and in-\ntroduce biases in more complex documents. In this paper, we propose\nand publicly release a set of reading order independent metrics tailored\nto Information Extraction evaluation in handwritten documents. In our\nexperimentation, we perform an in-depth analysis of the behavior of the\nmetrics to recommend what we consider to be the minimal set of metrics\nto evaluate a task correctly.",
            "Keywords": "Information Extraction \u00b7 Evaluation Metrics \u00b7 Reading Or-\nder \u00b7 Full Page Recognition \u00b7 End-to-End Model"
        },
        {
            "ID": "98",
            "Authors": [
                "Anna Zhu",
                "Huen Chen",
                "Jiangyang He"
            ],
            "Title": "Controllable Text Layout Generation For Synthesizing Scene Text Image",
            "Abstract": "In this paper, we propose an algorithm for generating a con-\ntrollable synthetic dataset of Chinese text layouts. This algorithm allows\nusers to specify the arrangement direction, segmentation method, and\ncurvature of the text, enabling the generation of more complex text lay-\nouts. Our algorithm provides flexible parameter control, allowing users to\ngenerate Chinese text datasets with diverse layouts. Additionally, we in-\ntroduce the ControlNet model for style transfer, enriching the generated\ndata with detailed information and backgrounds. Through comparative\nexperiments and ablation experiments, our algorithm has been validated,\ndemonstrating its effectiveness and superiority in generating controllable\nlayouts for Chinese text datasets. The incorporation of the ControlNet\nmodel for style transfer enhances the data generation process by enrich-\ning the details and backgrounds. Consequently, the synthesized dataset\ngenerated by our algorithm provides a valuable resource for training and\nevaluating Chinese text analysis algorithms, further advancing research\nand development in the field. Our algorithm lays the groundwork for\nexploring new technologies and promoting innovation in the realm of\nChinese text analysis.",
            "Keywords": "Controllable text layout \u00b7 Synthetic dataset \u00b7 Transformer."
        },
        {
            "ID": "100",
            "Authors": [
                "Carlos Boned",
                "Sanket Biswas",
                "Maxime Talarmain",
                "Oriol Ramos Terrades"
            ],
            "Title": "Recurrent Few-Shot model for Document Verification",
            "Abstract": "General-purpose ID, or travel, document image- and video-based ver-\nification systems have yet to achieve good enough performance to be considered\na solved problem. There are several factors that negatively impact their perfor-\nmance, including low-resolution images and videos and a lack of sufficient data to\ntrain the models. This task is particularly challenging when dealing with unseen\nclass of ID, or travel, documents. In this paper we address this task by proposing\na recurrent-based model able to detect forged documents in a few-shot scenario.\nThe recurrent architecture makes the model robust to document resolution vari-\nability. Moreover, the few-shot approach allow the model to perform well even for\nunseen class of documents. Preliminary results on the SIDTD and Findit datasets\nshow good performance of this model for this task.\n1\nIntroduction\nThe increased of remote identity authentication systems, incorporating biometrics and\nthe verification of ID and travel documents, has surged and become widespread in the\nwake of the COVID-19 pandemic. These authentication systems have empowered cit-\nizens to engage in work and business activities outside traditional office settings. Pub-\nlic administration, banks, productive industries, and numerous services have integrated\nthese systems seamlessly into their routine workflows. These services provide an on-\nline enrollment option, eliminating the need for users to physically attend by requesting\na selfie and an image of their ID document for authentication. Nevertheless, cyber-\ncrime has exploited societal vulnerabilities, evolving towards increasingly sophisticated\nthreats. Therefore, it is necessary to develop techniques that allow us to detect this type\nof fraudulent actions that expose citizens\u2019 data and their privacy to the general public.\nA current trend is to detect ID documents, passports or driving licenses that have\nphysically or digitally been modified from images acquired from mobile devices. In [4],\nthe authors applied texture descriptors to image patches and then applied BoW followed\nby an SVM classifier to classify each patch as genuine or fake. In that work, the most\nperformant CNN architectures of that time (AlexNet, VGG and Inception) were also\nused as general descriptor extractors. The results obtained in terms of F1-Score were\n\u22c6 This content reflects only the authors\u2019 view. The European Agency is not responsible for any\nuse that may be made of the information it contains.\n\u22c6\u22c6 Corresponding Author",
            "Keywords": null
        },
        {
            "ID": "102",
            "Authors": [
                "Jos\u00e9 Miguel Bened\u00ed",
                "Joan Andreu S\u00e1nchez",
                "Dan Anitei",
                "Daniel Parres"
            ],
            "Title": "Improving Efficiency and Performance through CTC-based Transformers for Mathematical Expression Recognition",
            "Abstract": "Mathematical expression recognition (MER) is an active field\nwith important implications for educational technology, document anal-\nysis, and scientific research automation. The bi-dimensionality and com-\nplexity of mathematical expressions presents challenges in accurately\ninterpreting expressions, such as distinguishing subscripts from super-\nscripts and comprehending structures like fractions and matrices, which\nrequire robust recognition systems that integrate spatial understanding\nand structural awareness. In this paper, we address the problem of type-\nset mathematical expression recognition using a convolutional neural net-\nwork backbone model and a transformer encoder trained with connec-\ntionist temporal classification loss. Compared to state-of-the-art systems,\nthis encoder-only model proves highly efficient, achieving a speed-up\nfactor of 3.5 and 4.2 for the IBEM and Im2Latex-100k datasets. More-\nover, this approach outperforms most state-of-the-art systems on mark-\nup-level and image-level metrics across both datasets. A comprehensive\nstudy demonstrates the model\u2019s capability to interpret complex reading\norders of mathematical expressions, showing that the monotonicity of the\nCTC alignments is not a limitation of CTC-based models for the prob-\nlem of MER. These findings underscore the effectiveness of this approach\nwhen compared to auto-regressive methods.",
            "Keywords": "Mathematical Expression Recognition \u00b7 Connectionist Tem-\nporal Classification \u00b7 Transformer Model \u00b7 Attention Visualization"
        },
        {
            "ID": "103",
            "Authors": [
                "Joan Andreu S\u00e1nchez",
                "Manuel Villarreal"
            ],
            "Title": "Enhancing Recognition of Historical Musical Pieces with Synthetic and Composed images",
            "Abstract": "Handwritten Music Recognition (HMR) poses the problem of\ntranscribing historical musical pieces from digital image to text. The vast\nnumber of untranscribed pieces, together with the scarcity of manually\nannotated training data renders the manual transcription impractical.\nHistorical musical pieces of particular interest are those dating back to\nthe XVth century and earlier, available only in their original manuscripts.\nCurrent state-of-the-art approaches leverage Convolutional and Recur-\nrent Neural Networks (CRNN) due to their effectiveness in processing\ninformation without relying on extensive datasets. This paper addresses\nthe data scarcity challenge in HMR by proposing two approaches. Firstly,\nthe utilization of synthetic images to augment the training data, leverag-\ning its successful applications in Handwritten Text Recognition (HTR).\nSecondly, the paper advocates for image composition, combining the im-\nages from a manuscript page to mitigate the contextual limitations asso-\nciated with single-line processing. Despite challenges observed in tradi-\ntional HTR models, the regularity found in historical musical document\nlayouts enhances the suitability of image composition for HMR. These\napproaches allow us to develop a system that can take advantage of\nadditional samples and contextual information to improve the recogni-\ntion capabilities of the HMR models. Results obtained show a relative\nimprovement when working with synthetic images and a substantial im-\nprovement when image composition is considered.",
            "Keywords": "Handwritten music recognition \u00b7 Handwritten text recogni-\ntion \u00b7 Synthetic images \u00b7 Image composition \u00b7 Convolutional Recurrent\nNeural Networks"
        },
        {
            "ID": "104",
            "Authors": [
                "Askar Hamdulla",
                "Mayire Ibrayim",
                "Chunhu Zhang",
                "Qilin Deng",
                "Hailong Luo"
            ],
            "Title": "Doc-DINO: A Transformer Model for Complex Logical Document Layout Analysis",
            "Abstract": "Document layout analysis is an indispensable part of doc-\nument information processing. It can be applied to various tasks such\nas document retrieval, machine translation, document information re-\ntrieval, and structured data extraction from documents. However, most\npublicly available datasets in the field of layout analysis primarily consist\nof documents with a single layout type, are in the English language, and\nare limited to PDF documents. In this paper, we propose the Doc-DINO\nmodel for analyzing complex logical document layouts using a dataset\nthat includes multiple formats, types, and a wider range of categories.\nFirstly, Aiming to learn more abstract and advanced representations by\nfusing multi-scale features, the Cross-Scale Convolution Fusion (CSCF)\nis proposed as the neck of the model. Secondly, the Fully Convolutional\nMulti-Core Self-Attention (FCMS) Encoder is presented, which includes\nconvolutional attention and convolutional feedforward networks to bet-\nter capture relationships between inputs and enhance the model\u2019s ex-\npressive power. The model achieves a mean Average Precision (mAP)\nof 65.7 on the complex document layout analysis dataset M6Doc and\n64.2 on SCUT-CAB, setting a new state-of-the-art performance for these\ndatasets.",
            "Keywords": "Document layout analysis \u00b7 Document logical layout \u00b7 Cross-\nscale feature fusion \u00b7 Fully convolutional fusion \u00b7 Convolutional self-\nattention."
        },
        {
            "ID": "105",
            "Authors": [
                "Dimosthenis Karatzas",
                "Lluis Gomez",
                "Mohamed Ali Souibgui",
                "Ernest Valveny",
                "Fei Yang",
                "Lei Kang"
            ],
            "Title": "Machine Unlearning for Document Classification",
            "Abstract": "Document understanding models have recently demonstrated remark-\nable performance by leveraging extensive collections of user documents. How-\never, since documents often contain large amounts of personal data, their usage\ncan pose a threat to user privacy and weaken the bonds of trust between hu-\nmans and AI services. In response to these concerns, legislation advocating \u201cthe\nright to be forgotten\u201d has recently been proposed, allowing users to request the\nremoval of private information from computer systems and neural network mod-\nels. A novel approach, known as machine unlearning, has emerged to make AI\nmodels forget about a particular class of data. In our research, we explore ma-\nchine unlearning for document classification problems, representing, to the best\nof our knowledge, the first investigation into this area. Specifically, we consider\na realistic scenario where a remote server houses a well-trained model and pos-\nsesses only a small portion of training data. This setup is designed for efficient\nforgetting manipulation. This work represents a pioneering step towards the de-\nvelopment of machine unlearning methods aimed at addressing privacy concerns\nin document analysis applications. Our code is publicly available at https://\ngithub.com/leitro/MachineUnlearning-DocClassification.",
            "Keywords": "Machine Unlearning \u00b7 Document Classification \u00b7 Data Privacy \u00b7 For-\ngetting."
        },
        {
            "ID": "107",
            "Authors": [
                "Lei Kang",
                "Dimosthenis Karatzas",
                "Ernest Valveny",
                "Rub\u00e8n Tito"
            ],
            "Title": "Multi-Page Document Visual Question Answering using Self-Attention Scoring Mechanism",
            "Abstract": "Documents are 2-dimensional carriers of written communication, and\nas such their interpretation requires a multi-modal approach where textual and\nvisual information are efficiently combined. Document Visual Question Answer-\ning (Document VQA), due to this multi-modal nature, has garnered significant\ninterest from both the document understanding and natural language processing\ncommunities. The state-of-the-art single-page Document VQA methods show im-\npressive performance, yet in multi-page scenarios, these methods struggle. They\nhave to concatenate all pages into one large page for processing, demanding sub-\nstantial GPU resources, even for evaluation. In this work, we propose a novel\nmethod and efficient training strategy for multi-page Document VQA tasks. In\nparticular, we employ a visual-only document representation, leveraging the en-\ncoder from a document understanding model, Pix2Struct. Our approach utilizes\na self-attention scoring mechanism to generate relevance scores for each docu-\nment page, enabling the retrieval of pertinent pages. This adaptation allows us\nto extend single-page Document VQA models to multi-page scenarios without\nconstraints on the number of pages during evaluation, all with minimal demand\nfor GPU resources. Our extensive experiments demonstrate not only achieving\nstate-of-the-art performance without the need for Optical Character Recognition\n(OCR), but also sustained performance in scenarios extending to documents of\nnearly 800 pages compared to a maximum of 20 pages in the MP-DocVQA\ndataset. Our code is publicly available at https://github.com/leitro/\nSelfAttnScoring-MPDocVQA.",
            "Keywords": "Document Visual Question Answering \u00b7 Multi-Page Scenario \u00b7 Multi-\nModal Scenario"
        },
        {
            "ID": "108",
            "Authors": [
                "Di Wu",
                "Shuqi Dai",
                "Yiran Zhao",
                "Tong Li"
            ],
            "Title": "Integrating Dependency Type and Directionality into Adapted Graph Attention Networks to Enhance Relation Extraction",
            "Abstract": "Relation extraction is fundamental in knowledge graph con-\nstruction. Recent studies have indicated the efficacy of Graph Convolu-\ntional Networks in enhancing performance in relation extraction tasks\nby leveraging dependency trees. However, noise in automatically gener-\nated dependency trees poses a challenge to using syntactic dependency\ninformation effectively. In this paper, we propose an Adaptive Graph\nAttention Network model based on Dependency Type and Direction in-\nformation, which effectively reduces noise through direction-aware de-\npendency relations, thereby enhancing extraction performance. Specifi-\ncally, we propose an adaptive graph attention mechanism to construct\ndirection-aware adjacency matrices for the precise aggregation of depen-\ndency pairs centred around entities as head words. This mechanism ef-\nfectively filters out noise interference from entities acting as dependent\nwords. Moreover, our model dynamically allocates weights to different\ndependency types based on their directions. This adaptive allocation en-\nhances the learning capability of entity representations, optimizing the\nencoding and extraction of entity information. Experimental results on\ntwo English benchmark datasets demonstrate that introducing direction\ninformation significantly enhances the model\u2019s performance. These find-\nings validate the efficacy of incorporating directionality in encoding to\nreduce dependency noise and improve relation extraction task perfor-\nmance.",
            "Keywords": "Relation extraction \u00b7 Adapted graph attention networks \u00b7\nDirectional information."
        },
        {
            "ID": "109",
            "Authors": [
                "Matthias Beckmann",
                "Dominik Hauser",
                "G\u00fcnther Koliander",
                "H. Siegfried Stiehl"
            ],
            "Title": "On Image Processing and Pattern Recognition for Thermograms of Watermarks in Manuscripts  A First Proof-of-Concept",
            "Abstract": "Watermarks in historical manuscripts are figural shapes serv-\ning as tokens for provenance research (e.g. scribe identification, dating,\npapermill attribution, scribe-papermaker relation, trading, etc.) in Hu-\nmanities such as Musicology. As of today, they come in a variety of\nformats: digitized handtracings and rubbings, X-ray based imagery and,\nmore recently, thermograms acquired with infrared (IR) cameras \u2013 all\nof which have been made accessible via image data bases in libraries\nor archives like the watermark information system (WZIS). A key use\ncase from a scholar\u2019s perspective is the search for similar or even equal\nwatermarks in whatever digitized data collections. Non-surprisingly, the\nprerequisite is the availability of a versatile, reliable, and user-friendly\ntool comprising methods from digital image processing (IP) and pattern\nrecognition (PR). In our paper, we focus on bridging the gap between\ndigitized thermograms of music manuscripts and watermark classification\nfor similarity-based search through (i) a state-of-the-art (SOTA) analy-\nsis, (ii) a resulting conceptual design based on well-understood SOTA as\nwell as novel methods, (iii) an easy-to-use implementation, and (iv) an\nexperimental validation as Proof-of-Concept (PoC). The current system\nperformance is characterized using thermograms recently made openly\navailable within the DRACMarkS project as well as WZIS. The exper-\nimental results clearly demonstrate success in bridging the existing gap\nhence also setting a baseline for an as yet lacking benchmark.",
            "Keywords": "Historical Document Analysis \u00b7 Image Processing \u00b7 Docu-\nment Analysis Systems"
        },
        {
            "ID": "111",
            "Authors": [
                "Mathieu Aubry",
                "Scott Trigg",
                "Matthieu Husson",
                "S\u00e9gol\u00e8ne Albouy",
                "Syrine Kalleli"
            ],
            "Title": "Historical Astronomical Diagrams Decomposition in Geometric Primitives",
            "Abstract": "Automatically extracting the geometric content from the\nhundreds of thousands of diagrams drawn in historical manuscripts would\nenable historians to study the diffusion of astronomical knowledge on a\nglobal scale. However, state-of-the-art vectorization methods, often de-\nsigned to tackle modern data, are not adapted to the complexity and\ndiversity of historical astronomical diagrams. Our contribution is thus\ntwofold. First, we introduce a unique dataset of 303 astronomical di-\nagrams from diverse traditions, ranging from the XIIth to the XVII-\nIth century, annotated with more than 3000 line segments, circles and\narcs. Second, we develop a model that builds on DINO-DETR to en-\nable the prediction of multiple geometric primitives. We show that it\ncan be trained solely on synthetic data and accurately predict primi-\ntives on our challenging dataset. Our approach widely improves over the\nLETR baseline, which is restricted to lines, by introducing a meaning-\nful parametrization for multiple primitives, jointly training for detection\nand parameter refinement, using deformable attention and training on\nrich synthetic data. Our dataset and code are available on our webpage:\nhttp://imagine.enpc.fr/\ufffdkallelis/icdar2024/.",
            "Keywords": "Vectorization \u00b7 Historical diagrams \u00b7 Transformers"
        },
        {
            "ID": "112",
            "Authors": [
                "Kurban Ubul",
                "Alimjan Aysa",
                "Tianqing Zhang",
                "Li Zhao"
            ],
            "Title": "Improving Retrieval-Based Dialogue Systems: Fine-grained Post-training Prompt Adaptation and Pairwise Optimization Fine-tuning Strategy",
            "Abstract": "Pre-trained models have demonstrated robust performance\nin natural language processing tasks. In retrieval-based dialogue sys-\ntems, the majority of existing studies have reduced the multi-round\ndialogue response selection problem to a classification problem. While\nsuch approaches have proven effective in retrieval-based dialogue sys-\ntems, they have not fully exploited the rich contextual understanding of\npre-trained models and have been unable to effectively deal with com-\nplex contexts and semantic relations in multi-turn dialogues, which may\nresult in potential information loss and performance bottlenecks. This\npaper proposes a fine-grained post-training prompt adaptation method\nand pairwise optimization fine-tuning strategy (FPPP). During training,\nthe model\u2019s contextual understanding and logical reasoning ability are\nenhanced through the use of a fine-grained post-training prompt adapta-\ntion method. In the prompt-tuning phase, a pairwise optimization fine-\ntuning strategy is employed to improve the model\u2019s ability to effectively\ndiscriminate between positive and negative samples. In all three datasets,\nFPPP outperforms the baseline model, resulting in an improvement of\nthe R10@1 metric by 0.1%, 1.4%, and 3.6%, respectively. The experi-\nmental results not only confirm the effectiveness of our method, but also\nprovide a new approach for retrieval-based dialogue systems.",
            "Keywords": "Prompt learning\u00b7 Retrieval-based Dialogue System\u00b7 Post-\ntraining\u00b7 Contextual Understanding\u00b7 Semantic Matching"
        },
        {
            "ID": "113",
            "Authors": [
                "Antonio R\u00edos - Vila",
                "Thierry Paquet",
                "Jorge Calvo - Zaragoza"
            ],
            "Title": "Sheet Music Transformer: End-To-End Optical Music Recognition Beyond Monophonic Transcription",
            "Abstract": "State-of-the-art end-to-end Optical Music Recognition (OMR)\nhas, to date, primarily been carried out using monophonic transcription\ntechniques to handle complex score layouts, such as polyphony, often\nby resorting to simplifications or specific adaptations. Despite their effi-\ncacy, these approaches imply challenges related to scalability and limita-\ntions. This paper presents the Sheet Music Transformer (SMT), the first\nend-to-end OMR model designed to transcribe complex musical scores\nwithout relying solely on monophonic strategies. Our model employs\na Transformer-based image-to-sequence framework that predicts score\ntranscriptions in a standard digital music encoding format from input\nimages. Our model has been tested on two polyphonic music datasets\nand has proven capable of handling these intricate music structures ef-\nfectively. The experimental outcomes not only indicate the competence\nof the model, but also show that it is better than the state-of-the-art\nmethods, thus contributing to advancements in end-to-end OMR tran-\nscription.",
            "Keywords": "Optical Music Recognition \u00b7 SMT \u00b7 Transformer \u00b7 Poly-\nphonic music transcription \u00b7 GrandStaff \u00b7 Quartets"
        },
        {
            "ID": "116",
            "Authors": [
                "Ferrer",
                "Miguel A",
                "Moises Diaz",
                "Alexios Giazitzis",
                "Elias N",
                "Zois"
            ],
            "Title": "Janus-faced handwritten signature attack: a clash between a handwritten Signature Duplicator and a writer independent Metric Meta-Learning Offline Signature Verifier",
            "Abstract": "Signature verification is a popular research area. SigmML,\na new system for offline, writer-independent verification, has been de-\nveloped, offering a unique approach outside typical Euclidean network\nlearning methods. This verifier operates in the space of symmetric posi-\ntive definite matrices and has demonstrated promising preliminary state-\nof-the-art results in intra and cross lingual dataset experiments. However,\nany offline automatic signature verifier faces a potential vulnerability:\nsusceptibility to massive attacks using synthetic signatures. This con-\ncern becomes more pronounced given the significant advancements in\nhandwritten image generation techniques. To evaluate the threat level of\nsynthetic attacks to the original version of SigmML, we assess its per-\nformance under several attack profiles involving the duplication of syn-\nthetically questioned signatures, which are used during the test stage.\nThese profiles advance the threat level to the SigmML verifier by refin-\ning the output of the duplicator with a quality control mechanism which\nintuitively adapts the a-priori knowledge of the intra-variability of each\nwriter. In our experiments, we considered signatures written in various\ncountries and styles, including specimens in Western, Devanagari, and\nBengali scripts. Quantitatively, we demonstrate this delicate security is-\nsue in the context of signature verification. The proposed attack profiles\nsignificantly degrade the performance of SigmML, surpassing the results\nobtained against skilled forgery experiments by more than double.",
            "Keywords": "Offline Signature Verification \u00b7 SigmML \u00b7 Riemannian Man-\nifold \u00b7 Synthetic signature attack \u00b7 Performance evaluation."
        },
        {
            "ID": "117",
            "Authors": [
                "O. Cruz",
                "F. Souzar",
                "I. Oliveirar",
                "Adriano L",
                "Robert Sabourin",
                "Rafael M",
                "Talles B. Vianar",
                "Victor L"
            ],
            "Title": "Robust Handwritten Signature Representation with Continual Learning of Synthetic Data over Predefined Real Feature Space",
            "Abstract": "Deep learning methods have emerged as state-of-the-art tech-\nniques for learning handwritten signature feature representations. How-\never, successful results in deep learning require a significant amount of\ntraining data. The GPDS-960 dataset used to be the largest publicly\navailable dataset of offline handwritten signatures for training deep mod-\nels. However, due to data protection regulatory issues, the GPDS-960\ndataset is no longer publicly available. This way, new investigators start-\ning research in this field have suffered from the absence of a large-scale\nreal signature dataset and have resorted to adopt the GPDSsynthetic\ndataset, a large-scale set of synthetically generated signatures for training\nmodels. Nevertheless, we have found a difference in verification perfor-\nmance between models trained using real and synthetic signature data.\nTo deal with this problem, we apply a method based on data-free knowl-\nedge transfer learning. Firstly, we generate inverted examples with the\nsame distribution as the real examples. Then, we complement a feature\nspace based on real data using synthetic data while minimizing the diver-\ngence in distribution between the representations provided by these two\ndifferent types of data sources. This is achieved through continual learn-\ning based on knowledge distillation. We evaluated models obtained with\nthe proposed method in terms of the equal error rate on GPDSsynthetic,\nGPDS-300, CEDAR, and MCYT-75 datasets in a writer-dependent veri-\nfication approach. Experiments demonstrated that the proposed method\nprovides a more robust model for writer-dependent verification when con-\nsidering real and synthetic signature datasets. Inverted data is available\nfor download at https://github.com/tallesbrito/continual_sigver.",
            "Keywords": "Signature verification \u00b7 Continual learning \u00b7 Knowledge dis-\ntillation."
        },
        {
            "ID": "119",
            "Authors": [
                "Wenqi Zhao",
                "Jianhua Zhu",
                "Liangcai Gao"
            ],
            "Title": "ICAL: Implicit Character-Aided Learning for Enhanced Handwritten Mathematical Expression Recognition",
            "Abstract": "Significant progress has been made in the field of handwrit-\nten mathematical expression recognition, while existing encoder-decoder\nmethods are usually difficult to model global information in LATEX. There-\nfore, this paper introduces a novel approach, Implicit Character-Aided\nLearning (ICAL), to mine the global expression information and en-\nhance handwritten mathematical expression recognition. Specifically, we\npropose the Implicit Character Construction Module (ICCM) to pre-\ndict implicit character sequences and use a Fusion Module to merge the\noutputs of the ICCM and the decoder, thereby producing corrected pre-\ndictions. By modeling and utilizing implicit character information, ICAL\nachieves a more accurate and context-aware interpretation of handwrit-\nten mathematical expressions. Experimental results demonstrate that\nICAL notably surpasses the state-of-the-art(SOTA) models, improving\nthe expression recognition rate (ExpRate) by 2.21%/1.75%/1.28% on\nthe CROHME 2014/2016/2019 datasets respectively, and achieves a re-\nmarkable 69.25% on the challenging HME100k test set. We make our\ncode available on the GitHub.1",
            "Keywords": "handwritten mathematical expression recognition \u00b7 trans-\nformer \u00b7 implicit character-aided learning \u00b7 encoder-decoder model"
        },
        {
            "ID": "120",
            "Authors": [
                "Sheraz Ahmed",
                "Saifullah Saifullah",
                "Andreas Dengel",
                "Stefan Agne"
            ],
            "Title": "DocXplain: A Novel Model-Agnostic Explainability Method for Document Image Classication",
            "Abstract": "Deep learning (DL) has revolutionized the \ufb01eld of document\nimage analysis, showcasing superhuman performance across a diverse set\nof tasks. However, the inherent black-box nature of deep learning models\nstill presents a signi\ufb01cant challenge to their safe and robust deployment\nin industry. Regrettably, while a plethora of research has been dedicated\nin recent years to the development of DL-powered document analysis\nsystems, research addressing their transparency aspects has been rela-\ntively scarce. In this paper, we aim to bridge this research gap by intro-\nducing DocXplain, a novel model-agnostic explainability method specif-\nically designed for generating high interpretability feature attribution\nmaps for the task of document image classi\ufb01cation. In particular, our\napproach involves independently segmenting the foreground and back-\nground features of the documents into di\ufb00erent document elements and\nthen ablating these elements to assign feature importance. We exten-\nsively evaluate our proposed approach in the context of document image\nclassi\ufb01cation, utilizing 4 di\ufb00erent evaluation metrics, 2 widely recognized\ndocument benchmark datasets, and 10 state-of-the-art document image\nclassi\ufb01cation models. By conducting a thorough quantitative and qual-\nitative analysis against 9 existing state-of-the-art attribution methods,\nwe demonstrate the superiority of our approach in terms of both faith-\nfulness and interpretability. To the best of the authors\u2019 knowledge, this\nwork presents the \ufb01rst model-agnostic attribution-based explainability\nmethod speci\ufb01cally tailored for document images. We anticipate that our\nwork will signi\ufb01cantly contribute to advancing research on transparency,\nfairness, and robustness of document image classi\ufb01cation models.",
            "Keywords": "Explainable Document Classi\ufb01cation \u00b7 Explainable AI\nDocument Image Classi\ufb01cation \u00b7 Model Interpretability \u00b7 Model-Agnostic\n\u22c6 This work was supported by the BMBF projects SensAI (BMBF Grant 01IW20007)"
        },
        {
            "ID": "121",
            "Authors": [
                "Andrei S",
                "Dinu Urse",
                "Dumitru",
                "Manuel",
                "Vlad",
                "Vasile P\u0103is",
                "R\u0103zvan",
                "Andrei",
                "erban",
                "Dragos",
                "Andreea Iuga",
                "Petru Sorlescu",
                "Cristian Matei",
                "Gabriel Micliu\u015f",
                "Adrian",
                "Marius Avram",
                "George",
                "Andrei Muntean",
                "Clementin Cercel",
                "Vlad Manolache"
            ],
            "Title": "HistNERo: Historical Named Entity Recognition for the Romanian Language",
            "Abstract": "This work introduces HistNERo, the first Romanian cor-\npus for Named Entity Recognition (NER) in historical newspapers. The\ndataset contains 323k tokens of text, covering more than half of the 19th\ncentury (i.e., 1817) until the late part of the 20th century (i.e., 1990).\nEight native Romanian speakers annotated the dataset with five named\nentities. The samples belong to one of the following four historical regions\nof Romania, namely Bessarabia, Moldavia, Transylvania, and Wallachia.\nWe employed this proposed dataset to perform several experiments for\nNER using Romanian pre-trained language models. Our results show\nthat the best model achieved a strict F1-score of 55.69%. Also, by reduc-\ning the discrepancies between regions through a novel domain adaption\ntechnique, we improved the performance on this corpus to a strict F1-\nscore of 66.80%, representing an absolute gain of more than 10%.",
            "Keywords": "Named Entity Recognition \u00b7 Historical Newspapers \u00b7 Ro-\nmanian Language \u00b7 Novel Dataset \u00b7 Transformer."
        },
        {
            "ID": "122",
            "Authors": [
                "Hongxi Wei",
                "Jingtao Ma",
                "Yiming Wang"
            ],
            "Title": "Recognition and Link Prediction of Onomatopoeia Texts with Arbitrary Shapes",
            "Abstract": "The onomatopoeia texts in the Japanese comic, with its arbi-\ntrary shapes diverse backgrounds and complex layouts, are a challenging\nand worthwhile subject of study. On the one hand, when recognizing\nonomatopoeia text images, using existing mainstream text recognition\nmethods may lead to the inability to achieve the expected recognition\nresults. This may be caused by these methods not taking into account\nthe unique characteristics of onomatopoeia words. On the other hand,\ntruncated text which is a part of a complete onomatopoeia word text\nbut not adjacent to other parts on a page of the comic has no meaning.\nIt is only when these truncated texts of a complete onomatopoeia word\nare linked together that their original meaning can be understood. So,\na new method named M4C-COO was proposed to predict the link by\nresearchers but the issue of class imbalance between truncated texts and\nnon-truncated texts was ignored. To solve these problems, in this paper,\na new recognition method exploiting the characteristics of onomatopoeia\ntexts was devised; focal loss (FL) was introduced to predict the link and,\nfurthermore, a completely novel loss function based on the focal loss\n(FB) was proposed. Finally, through experiments, the e\ufb00ectiveness of\nthe works was demonstrated, achieving the state-of-the-art performance.",
            "Keywords": "Comic onomatopoeia \u00b7 Arbitrary shapes \u00b7 Truncated texts."
        },
        {
            "ID": "123",
            "Authors": [
                "Cheng - Lin Liu",
                "Umapada Pal",
                "Arnab Halder",
                "Shivakumara Palaiahnakote",
                "Michael Blumenstein"
            ],
            "Title": "A New Unsupervised Approach for Text Localization in  Shaky and Non-Shaky Scene Video",
            "Abstract": "Text Detection in shaky and non-shaky videos is challenging due to poor video \nquality and the presence of static and dynamic obstacles. Video captured by a shaky camera \ndue to wind is considered shaky video, while video captured by a fixed camera is consid-\nered as non-shaky video. Most state-of-the-art methods achieve the best results when ex-\nploring the concept of deep learning. The present study proposes an unsupervised approach \nfor text spotting in shaky and non-shaky videos. In the first stage, our method selects \nkeyframes from the input video by estimating the similarity between the temporal frames, \nwhich we named activation frames. For each activation frame, the proposed method ex-\ntracts statistical features such as orientation, spectral, edge density and intensity features \nthat represent text information. The extracted features are fed to a K-means clustering \nmethod to obtain the text clusters, which results in text regions in the activation frames. \nFor each region, the proposed method uses optical flow to extract spatial consistency, mo-\ntion consistency and depth map consistency for localizing text using temporal voting-non-\nmaximum suppression. Experiments are conducted on our shaky and non-shaky dataset, and \nthe benchmark dataset of ICDAR 2015. For the experiments it can be seen that the proposed \nmethod is superior to existing methods.",
            "Keywords": "Scene text detection, Video text detection, Unsupervised technique, Shaky \nand non-shaky video."
        },
        {
            "ID": "133",
            "Authors": [
                "Cheng",
                "Lin Liu",
                "Yi Chen",
                "Yangyang Liu",
                "Fei Yin"
            ],
            "Title": "Context-Aware Confidence Estimation for Rejection in Handwritten Chinese Text Recognition",
            "Abstract": "Handwritten Chinese Text Recognition (HCTR) has been\nadvanced largely by deep learning in recent years. However, the remain-\ning recognition errors still hinder reliability-critical applications where\nzero-error is desired. Rejecting low-confidence patterns can help reduce\nthe error rate but the increased rejection rate is also harmful. In this pa-\nper, we propose a character confidence estimation method incorporating\ncontexts for character rejection in HCTR. Based on a text line recog-\nnizer outputting character segmentation and classification results, the\nconfidence of each segmented character is estimated by combining the\nscores of a re-trained character classifier, the linguistic and geometric\ncontexts. We introduce a probabilistic formula for estimating the confi-\ndence by combining the classifier and contextual scores, and an improved\napproach for scoring the geometric context using unary and binary ge-\nometric features. Experimental evaluations on the CASIA-HWDB and\nICDAR2013 datasets demonstrate that our method can significantly im-\nprove the rejection performance in respect of low error rate at moderate\nrejection rate. The re-trained classifier, the linguistic context and the\ngeometric context are all justified effective to improve the confidence.",
            "Keywords": "Handwritten Chinese Text Recognition \u00b7 Confidence Esti-\nmation \u00b7 Geometric Context \u00b7 Bayesian probability formula."
        },
        {
            "ID": "134",
            "Authors": [
                "Chao Liu",
                "Jie Yang",
                "Wanqing Li"
            ],
            "Title": "Extractive Question Answering with Contrastive Puzzles and Reweighted Clues",
            "Abstract": "The task of Extractive Question Answering (EQA) involves\nidentifying correct answer spans in response to provided questions and\npassages. The emergence of Pretrained Language Models (PLMs) has\nsparked increased interest in leveraging these models for EQA tasks,\nyielding promising results. Nonetheless, current approaches frequently\nneglect the issue of label noise, which arises from incomplete labeling and\ninconsistent annotations, thereby reducing the model performance. To\naddress this issue, we propose the Contrastive Puzzles and Reweighted\nClues (CPRC) method, designed to mitigate the adverse effects of label\nnoise. Our approach involves categorizing training data into Puzzle and\nClue samples based on their loss and text similarity to the golden answer\nduring model training. Subsequently, CPRC incorporates a hybrid intra-\nand inter-contrastive learning approach for Puzzle samples and dynam-\nically adjusts the weights of Clue samples, respectively. The experimen-\ntal results, conducted on three benchmark datasets, demonstrates the\nsuperior performance of the proposed CPRC compared to conventional\napproaches, highlighting its efficacy in mitigating the label noise and\nachieving enhanced EQA performance.",
            "Keywords": "Extractive Question Answering \u00b7 Label Noise \u00b7 Contrastive\nLearning \u00b7 Sample Reweighting"
        },
        {
            "ID": "136",
            "Authors": [
                "Kurban",
                "Jun Ding",
                "Jiaoyan Wang",
                "Alimjan Aysa",
                "Xuebin Xu"
            ],
            "Title": "Oracle Bone Inscriptions Image Retrieval Based on Metric Learning",
            "Abstract": "The goal of oracle bone inscriptions image retrieval is to find\nthe most similar image in the oracle database. This technology provides\nvaluable tools and methods for scholars to promote the digital research\nand development of oracle bone inscriptions. However, there are many\nchallenges in the practical application of oracle bone images, such as\nsevere noise interference, sample loss, and low similarity within the same\nclass, while the high similarity between the different classes. To address\nthese issues, this article first introduces a denoising process for oracle\nbone images to reduce the impact of noise on image retrieval. Secondly,\na multi-strategy data augmentation method is also adopted to expand\nthe fewer representative samples and enhance their diversity. Finally, this\narticle proposes a metric learning-based Oracle image retrieval method\nthat compares query images with images in image databases. To improve\nthe retrieval accuracy, we have designed a feature extraction network\nspecifically tailored for oracle bone image retrieval. This network includes\na module for multi-scale convolution calculations of inputs, as well as a\nspatial pooling component for generating oracle image retrieval vectors.\nIn the two retrieval datasets utilized in this article, our proposed method\nachieved performance improvements of 4.92% and 4.59% respectively\ncompared to before improvement. This advancement will contribute to\nthe widespread application of oracle bone inscriptions image retrieval in\nacademic research and other relevant fields.",
            "Keywords": "Oracle bone inscriptions image retrieval \u00b7 Metric learning \u00b7\nImage denoising \u00b7 Multi-strategy data augmentation \u00b7 Spatial pooling."
        },
        {
            "ID": "140",
            "Authors": [
                "Shumpei Takezaki",
                "Daichi Haraguchi",
                "Seiichi Uchida",
                "Tetta Kondo"
            ],
            "Title": "Font Style Interpolation with Diffusion Models",
            "Abstract": "Fonts have huge variations in their styles and give readers\ndifferent impressions. Therefore, generating new fonts is worthy of giving\nnew impressions to readers. In this paper, we employ diffusion models to\ngenerate new font styles by interpolating a pair of reference fonts with\ndifferent styles. More specifically, we propose three different interpola-\ntion approaches, image-blending, condition-blending, and noise-blending,\nwith the diffusion models. We perform qualitative and quantitative ex-\nperimental analyses to understand the style generation ability of the\nthree approaches. According to experimental results, three proposed ap-\nproaches can generate not only expected font styles but also somewhat\nserendipitous font styles. We also compare the approaches with a state-of-\nthe-art style-conditional Latin-font generative network model to confirm\nthe validity of using the diffusion models for the style interpolation task.",
            "Keywords": "Font generation \u00b7 Style interpolation \u00b7 Diffusion models."
        },
        {
            "ID": "143",
            "Authors": [
                "Zuo Li",
                "Heng Zhang",
                "Xiao - Hui Li",
                "Tian",
                "Fei Yin"
            ],
            "Title": "Adaptive Scaling and Refined Pyramid Feature Fusion Network for Scene Text Segmentation",
            "Abstract": "Although scene text recognition has achieved high perfor-\nmance, text segmentation still needs to be improved. The goal of text\nsegmentation is to obtain pixel-level foreground text masks from scene\nimages. In this paper, we adaptively resize the input images to their op-\ntimal scales and propose the Refined Pyramid Feature Fusion Network\n(RPFF-Net) for robust scene text segmentation. To address the issue of\ninconsistent text scaling, we propose an adaptive image scaling method\nthat takes into account the density of text regions in each scene image.\nIn the RPFF-Net, we first extract multi-scale features from the back-\nbone network, and then combine these features using effective pyramid\nfeature fusion methods. To enhance the interaction between text from\ncontextual characters and extract features at different levels, we apply\ntwo self-attention mechanisms to the fusion feature map in spatial and\nchannel dimensions. The experimental results demonstrate the effective-\nness of our approach on several text segmentation benchmarks including\nthe monolingual TextSeg and bilingual BTS datasets, and show that it\noutperforms the existing state-of-the-art scene text segmentation meth-\nods even without OCR (optical character recognition) enhancement.",
            "Keywords": "Text segmentation \u00b7 Adaptive scaling \u00b7 Pyramid feature fu-\nsion \u00b7 Self-attention mechanism"
        },
        {
            "ID": "144",
            "Authors": [
                "Benjamin Kiessling",
                "Matthias Gille Levenson",
                "Patricia O \u2019 Connor",
                "Malamatenia Vlachou",
                "Federico Boschetti",
                "Ariane Pinche",
                "Jean - Baptiste Camps",
                "Mike Kestemont",
                "Avery Manton",
                "Efstathiou",
                "Thibault Cl\u00e9rice",
                "Michael Gervers",
                "Agn\u00e8s Boutreux",
                "Wouter Haverals",
                "Olivier Brisville - Fertin",
                "Simon Gabay",
                "Caroline Vandyck",
                "Alix Chagu\u00e9",
                "Franz Fischer"
            ],
            "Title": "CATMuS Medieval: A multilingual large-scale cross-century dataset in Latin script for handwritten text recognition and beyond",
            "Abstract": "The surge in digitisation initiatives by Cultural Heritage\ninstitutions has facilitated online accessibility to numerous historical\nmanuscripts. However, a substantial portion of these documents exists\nsolely as images, lacking machine-readable text. Handwritten Text Recog-\nnition (HTR) has emerged as a crucial tool for converting these images\ninto machine-readable formats, enabling researchers and scholars to anal-\nyse vast collections efficiently. Despite significant technological progress,\nestablishing consistent ground truth across projects for HTR tasks, par-\nticularly for complex and heterogeneous historical sources like medieval\nmanuscripts in Latin scripts (8th-15th century CE), remains nonetheless\nchallenging.\nWe introduce the Consistent Approaches to Transcribing Manuscripts\n(CATMuS) dataset for medieval manuscripts, which offers (1) a uniform",
            "Keywords": null
        },
        {
            "ID": "145",
            "Authors": [
                "Xin Yang"
            ],
            "Title": "Document Specular Highlight Removal with Coarse-to-fine Strategy",
            "Abstract": "Specular highlight detection and removal are fundamental\nchallenges in computer vision and image processing, with the detection\nresults serving as a precursor to guide the model in achieving better re-\nmoval of specular highlights. This paper introduces a novel highlight re-\nmoval model, which presents an efficient end-to-end deep learning frame-\nwork designed to automatically remove specular highlights from a single\nimage. Our architecture comprises three key modules: the Coarse Predic-\ntor (CP), Refinement Predictor (RP), and Global Discriminator (GD).\nThe CP utilizes a novel Transformer-based Unet architecture to recover\nthe primary content, while the GD incorporates a discriminator to ensure\nthe coarse result is more feasible in a global context. Lastly, the RP is\nbased on conditional Denoising Diffusion Probabilistic Models (DDPM)\nand is responsible for predicting the residual information between the\nground-truth and the CP-predicted image. Experimental results on four\npublic benchmark images demonstrate that our method surpasses state-\nof-the-art methods in the task of highlight removal.",
            "Keywords": "Document Specular Highlight Removal \u00b7 Transformer \u00b7 Con-\nditional Diffusion Models."
        },
        {
            "ID": "146",
            "Authors": [
                "Cheng",
                "Lin Liu",
                "Heng Zhang",
                "Rong Ling",
                "Lu",
                "Fei Yin"
            ],
            "Title": "Deep Metric Learning with Cross-Writer Attention for Offline Signature Verification",
            "Abstract": "Signature verification is a biometric and document forensics\ntechnology useful for personal identification in various security applica-\ntions. Signature verification in the writer-independent scenario remains a\nchallenge, particularly in distinguishing between genuine signatures and\nskilled forgeries. In this paper, we propose a writer-independent signature\nverification method based on deep metric learning with cross-writer at-\ntention. Our cross-writer attention module includes two parts: SimAM (a\nSimple, Parameter-Free Attention Module), as well as the cross-attention\nmechanism. SimAM is combined with each DenseBlock to interact infor-\nmation of two inputs, which makes the learned weights better account\nfor the difference between two input signatures. Cross-attention aligns\nglobal and local information in learned feature representations of two\ninput signatures. Further, we introduce a focal contrast loss function for\ndeep metric learning to overcome the sample imbalance. Extensive ex-\nperiments demonstrate the effectiveness of the proposed method, which\nachieves superior performance on several public datasets and also indi-\ncates the effectiveness of each module.",
            "Keywords": "Signature Verification \u00b7 SimAM \u00b7 Cross-Attention \u00b7 Deep\nMetric Learning."
        },
        {
            "ID": "148",
            "Authors": [
                "Jiefeng Ma",
                "Zhongyuan Han",
                "Mobai Xue",
                "Zhenrong Zhang",
                "Pengfei Hu",
                "Jun Du"
            ],
            "Title": "Radical Similarity Based Model Optimization and Post-correction for Chinese Character Recognition",
            "Abstract": "Radical-based methods for Chinese character recognition (CCR)\nhave been proven effective and offer substantial advantages. Different\nfrom character-based methods, Chinese characters are described as com-\nbinations of structures and radicals, and character recognition is achieved\nby the proper identifications of these components. However, there are vi-\nsual similarities among radicals, leading to the ambiguity problem for\nCCR, which is not fully utilized in previous work. Accordingly, in this\nstudy, we first employ the stroke order information of Chinese radicals to\nestablish a radical similarity metric. Then we improve the radical-based\nCCR in two ways. During the training stage, we propose a new loss func-\ntion called minimum Bayesian risk (MBR) based on the radical similarity\nmetric to yield better performance. During the recognition stage, the rad-\nical similarity is adopted to post-correct the potential error recognition\nresults, offering a low-cost yet effective solution. Experimental results\non different radical-based CCR models and datasets demonstrate the\neffectiveness and robustness of our proposed method.",
            "Keywords": "Radical similarity \u00b7 Chinese character recognition \u00b7 Bayesian\nrisk."
        },
        {
            "ID": "153",
            "Authors": [
                "Cheng - Lin Liu",
                "Yi - Ming Chen",
                "Chun - Bo Xu"
            ],
            "Title": "EntityLayout: Entity-level Pre-training Language Model for Semantic Entity Recognition and Relation Extraction",
            "Abstract": "Semantic entity recognition (SER) and relation extraction\n(RE) are the core tasks of information extraction from visually-rich doc-\numents (VrDs). Although self-supervised pre-training models have ad-\nvanced the performance of these tasks, existing methods are insufficient\nin fusing the token features and modeling SER and RE jointly. In this\npaper, we propose an entity-level language model, named EntityLayout,\nand use a graph-based approach for efficient entity labeling and linking\njointly on a continual pre-training model. In EntityLayout, a Token Fu-\nsion Module (TFM) is proposed to fuse the token feature and learn an\nentity-level representation. Then, we use the entity-level representation\nto build a graph and propose a Graph Pruning Module (GPM) to ef-\nfectively prune the invalid links. Finally, SER and RE are accomplished\nsimultaneously by a joint information extraction Task Module. Experi-\nmental results on public datasets FUNSD and CORD demonstrate that\nthe proposed EntityLayout achieves competitive performance in SER and\nstate-of-the-art performance in RE, i.e., SER F1 scores of 0.9108 and\n0.9650, respectively, RE F1 scores of 0.8212 and 0.9898, respectively.",
            "Keywords": "Visual Information Extraction \u00b7 Semantic Entity Recogni-\ntion \u00b7 Relation Extraction \u00b7 Graph Pruning \u00b7 Graph Neural Network.\n\u2020 Equal contributions. Work was done when Chun-Bo Xu was a Research Intern at\nHundsun Technologies Inc.\n\u2021 Corresponding author."
        },
        {
            "ID": "155",
            "Authors": [
                "Th\u00e9o LARCHER",
                "Thomas",
                "Lucas PR",
                "##T",
                "Pierrick TRANOUEZ",
                "Thierry PAQ",
                "Sandra BREE"
            ],
            "Title": "End-to-end information extraction in handwritten documents: Understanding Paris marriage records from  to",
            "Abstract": "The EXO-POPP project aims to establish a comprehen-\nsive database comprising 300,000 marriage records from Paris and its\nsuburbs, spanning the years 1880 to 1940, which are preserved in over\n130,000 scans of double pages. Each marriage record may encompass up\nto 118 distinct types of information that require extraction from plain\ntext. In this paper, we introduce the M-POPP dataset, a subset of the\nM-POPP database with annotations for full-page text recognition and\ninformation extraction in both handwritten and printed documents, and\nwhich is now publicly available.3 We present a fully end-to-end archi-\ntecture adapted from the DAN, designed to perform both handwritten\ntext recognition and information extraction directly from page images\nwithout the need for explicit segmentation. We showcase the informa-\ntion extraction capabilities of this architecture by achieving a new state\nof the art for full-page Information Extraction on Esposalles and we use\nthis architecture as a baseline for the M-POPP dataset. We also assess\nand compare how different encoding strategies for named entities in the\ntext affect the performance of jointly recognizing handwritten text and\nextracting information, from full pages.",
            "Keywords": "handwriting recognition \u00b7 named entity recognition \u00b7 infor-\nmation extraction \u00b7 document understanding"
        },
        {
            "ID": "156",
            "Authors": [
                "Weihong Ma",
                "Yang Xue",
                "Yuxin Kong",
                "Lianwen Jin"
            ],
            "Title": "GARDEN: Generative Prior Guided Network for Scene Text Image Super-Resolution",
            "Abstract": "Scene text image super-resolution (STISR) is a popular re-\nsearch topic due to its great potential for improving downstream recog-\nnition performance. Many recent STISR approaches have utilized recog-\nnition feedback to guide the reconstruction process. However, their ef-\nfectiveness is often limited by inaccurate recognition feedback and in-\nsufficient use of visual priors. To address these challenges, we propose\na novel GenerAtive pRior guiDEd Network, namely GARDEN, which\nsurpasses existing practices by exploiting enriched generative priors for\nprecise and reliable guidance towards STISR. Innovatively, GARDEN\nleverages a pre-trained Vision Transformer (ViT) as the generative style\nbank, which provides diverse image priors and further assists in generat-\ning reliable text priors. This allows the network to leverage prior informa-\ntion from both visual and semantic domains for the final reconstruction,\nleading to more efficient learning of both texture generation and text re-\ncovery. In addition, GARDEN introduces multi-scale sequential residual\nblock (MS-SRB), a simple, efficient, and flexible structure for achiev-\ning the maximal utilization of generative priors. By leveraging enriched\ngenerative priors within a novel architecture design, GARDEN is better\nsuited to encode, transfer, and reconstruct super-resolution text images\nthan the best previous methods in terms of both fidelity and recognition\naccuracy, as shown in Fig.1. Code will be publicly available.",
            "Keywords": "Scene text image super-resolution \u00b7 Scene text recognition\n\u00b7 Prior information."
        },
        {
            "ID": "158",
            "Authors": [
                "Marie B",
                "Manh Tu"
            ],
            "Title": "ViT-ED: Transformer network for image similarity measurement",
            "Abstract": "Measuring image similarity correctly and reliability is a crit-\nical requirement with profound implications across various applications,\nincluding puzzle reconstruction and historial document retrieval. In this\nwork, we introduce a new deep neural network called ViT-ED, which\nstands for Vision Transformer with Encoder and Decoder network, to\nsolve this task of image similarity estimation. By utilizing the atten-\ntion and cross-attention mechanisms from the Transformer architecture,\nViT-ED is capable of incorporating both global and local dependencies\nbetween patches from the two input images to make better similarity\nmeasurement. Experimental results on benchmark datasets of the two\nrelated problems: puzzle reconstruction and image retrieval, show that\nour ViT-ED model signi\ufb01cantly outperforms state-of-the-art approaches\non these tasks, e.g. ViT-ED achieves 19% of improvement in terms of\nmean Average Precision over state-of-the-art approach on the HisFra-\ngIR20 benchmark dataset. These results suggest that ViT-ED could be\na strong candidate to solve image similarity related problems.",
            "Keywords": "Image similarity estimation \u00b7 Deep neural network \u00b7 Vision\nTransformer."
        },
        {
            "ID": "160",
            "Authors": [
                "Haoyu Dong",
                "Haochen Wang",
                "Liangcai Gao",
                "Kai Hu"
            ],
            "Title": "DocTabQA: Answering Questions from Long Documents Using Tables",
            "Abstract": "We study a new problem setting of question answering (QA),\nreferred to as DocTabQA. Within this setting, given a long document,\nthe goal is to respond to questions by organizing the answers into struc-\ntured tables derived directly from the document\u2019s content. Unlike tra-\nditional QA approaches which predominantly rely on unstructured text\nto formulate responses, DocTabQA aims to leverage structured tables\nas answers to convey information clearly and systematically, thereby\nenhancing user comprehension and highlighting relationships between\ndata points. To the best of our knowledge, this problem has not been\npreviously explored. In this paper, we introduce the QTabA dataset,\nencompassing 300 financial documents, accompanied by manually an-\nnotated 1.5k question-table pairs. Initially, we leverage Large Language\nModels (LLMs) such as GPT-4 to establish a baseline. However, it is\nwidely acknowledged that LLMs encounter difficulties when tasked with\ngenerating intricate, structured outputs from long input sequences. To\novercome these challenges, we present a two-stage framework, called\nDocTabTalk, which initially retrieves relevant sentences from extensive\ndocuments and subsequently generates hierarchical tables based on these\nidentified sentences. DocTabTalk incorporates two key technological in-\nnovations: AlignLLaMA and TabTalk, which are specifically tailored\nto assist GPT-4 in tackling DocTabQA, enabling it to generate well-\nstructured, hierarchical tables with improved organization and clarity.\nComprehensive experimental evaluations conducted on both QTabA and\nRotoWire datasets demonstrate that our DocTabTalk significantly en-\nhances the performances of the GPT-4 in our proposed DocTabQA task\nand the table generation task. The code and dataset are available at\nhttps://github.com/SmileWHC/DocTabQA for further research.",
            "Keywords": "Question Answering \u00b7 Table Generation \u00b7 Large Language\nModel \u00b7 Retrieval Augmented Generation \u00b7 Dataset"
        },
        {
            "ID": "162",
            "Authors": [
                "Nam Tuan Ly",
                "Toshihiko Horie",
                "Nghia Thanh Truong",
                "Masaki Nakagawa",
                "Hung Tuan Nguyen"
            ],
            "Title": "Content-based Similarity for Automatic Scoring of  Handwritten Descriptive Answers",
            "Abstract": "This paper introduces content-based similarity for automatic scoring \nof handwritten descriptive answers, focusing on Japanese, English, and mathe-\nmatical expressions. Our experiments were made on a collection of handwritten \ndescriptive answers from elementary school students, encompassing 37,500 Jap-\nanese, 15,896 English, and 86,264 math answers. We used neural network-based \nonline and offline handwriting recognizers for each answer and applied automatic \nscoring of recognized candidates with expected answers. In the initial experiment, \nwe applied a perfect match with expected answers, revealing issues and chal-\nlenges, especially with the rate of correct answers scored as wrong (false nega-\ntives) exceeding 30% in some subjects. Then, we propose a recognition confi-\ndence-based rejection scheme to reduce false positives. Moreover, we propose \ncontent-awareness similarity that calculates a similarity between the recognized \ncandidates of an answer and the expected answers. According to the computed \nsimilarity, it scores the answers as correct, wrong, or rejected. Human scorers \nshould score false negative answers that are likely claimed by students and re-\njected answers. The experiment suggests that human scorers need to score \n14.39% after applying the automatic scoring method with the rate of incorrect \nanswers scored correct of 3.03% for Japanese, the former as 10.75% and the latter \nas 1.79% for English, and the former as 27.34% and the latter as 0.45% for math. \nThese promising results underscore the system's effectiveness.",
            "Keywords": "handwriting recognition, automatic scoring, deep neural networks, \nensemble recognition."
        },
        {
            "ID": "165",
            "Authors": [
                "Badri Vishal Kasuba",
                "Venkatapathy Subramanian",
                "Parag Chaudhuri",
                "Ganesh Ramakrishnan",
                "Dhruv Kudale"
            ],
            "Title": "SPRINT: Script-agnostic Structure Recognition in Tables",
            "Abstract": "Table Structure Recognition (TSR) is vital for various down-\nstream tasks like information retrieval, table reconstruction, and docu-\nment understanding. While most state-of-the-art (SOTA) research pre-\ndominantly focuses on TSR in English documents, the need for similar\ncapabilities in other languages is evident, considering the global diversity\nof data. Moreover, creating substantial labeled data in non-English lan-\nguages and training these SOTA models from scratch is costly and time-\nconsuming. We propose TSR as a language-agnostic cell arrangement\nprediction and introduce SPRINT \u2014 Script-agnostic Structure Recogni-\ntion in Tables. SPRINT uses recently introduced Optimized Table Struc-\nture Language (OTSL) sequences to predict table structures. We show\nthat when coupled with a pre-trained table grid estimator, SPRINT can\nimprove the overall tree edit distance-based similarity structure scores\nof tables even for non-English documents. We experimentally evaluate\nour performance across benchmark TSR datasets including PubTabNet,\nFinTabNet, and PubTables-1M. Our findings reveal that SPRINT not\nonly matches SOTA models in performance on standard datasets but\nalso demonstrates lower latency. Additionally, SPRINT excels in accu-\nrately identifying table structures in non-English documents, surpass-\ning current leading models by showing an absolute average increase of\n11.12%. We also present an algorithm for converting valid OTSL predic-\ntions into a widely used HTML-based table representation. To encourage\nfurther research, we release our code and Multilingual Scanned and Scene\nTable Structure Recognition Dataset, MUSTARD labeled with OTSL\nsequences for 1428 tables in thirteen languages encompassing several\nscripts at https://github.com/IITB-LEAP-OCR/SPRINT",
            "Keywords": "Table Structure Recognition, Layout Detection, Document\nAnalysis"
        },
        {
            "ID": "166",
            "Authors": [
                "Unter",
                "Stephan M"
            ],
            "Title": "Text Line Segmentation on Ancient Egyptian Papyri: Layout Analysis with Object Detection Networks and Connected Components",
            "Abstract": "The automatic localization of text lines is an important step\nin the analysis of handwritten historical documents. It is a valuable tool\nfor further analysis, such as studying handwriting styles and typefaces\nor realigning fragments based on a continuation of text lines. This paper\ntests and compares various architectures, originally designed for object\ndetection tasks, for text line segmentation in ancient Egyptian hieratic\npapyri. The corpus used in this study presents a significant challenge due\nto noisy text carriers, complex scripts and layouts, and a highly frag-\nmentary condition. The experiments conducted include tests on transfer\nlearning, data augmentation, and various input resolutions. The results\nsuggest that most object detection networks, particularly the Faster R-\nCNN and Mask R-CNN architectures, are well-suited to identify lines\nof text even in very complex or fragmented textual layouts. In addi-\ntion, we present and evaluate an alternative \u2019bottom-up\u2019 approach that\nutilises binarised documents. This method derives text lines from con-\nnected components, allowing for greater control over parameters and\nimproved identification of small ink remnants. Although the numerical\nevaluation metrics may be lower, this technique offers increased control\nby the user.",
            "Keywords": "Text Line Segmentation \u00b7 Historical Document Analysis \u00b7\nAncient Egyptian Papyri \u00b7 Machine Learning \u00b7 Object Detection."
        },
        {
            "ID": "167",
            "Authors": [
                "Hongxi Wei",
                "Hao Yu",
                "Yiming Wang"
            ],
            "Title": "Deepfake In-air Signature Verification via Two-channel Model",
            "Abstract": "The in-air signature verification system has garnered at-\ntention due to its flexibility, friendliness, security, high efficiency, re-\nmote accessibility, and contactless usage patterns. However, with the\nadvancement of machine learning, computers can now learn and imitate\nmany things through artificial intelligence. This technology, referred to\nas \"deepfake\", has been used to create fake signatures, raising concerns\nabout the security of the in-air signature verification system. Conse-\nquently, research on in-air signature verification technology against deep-\nfakes has become an urgent need. The current challenges are: (1) Most\nresearch on in-air signature verification at this stage focuses on human\nforgery, with a lack of high-performance verification systems for deepfake\nin-air signatures. (2) Due to the long sequence and small amount of data,\nthere is a lack of good generation methods for in-air signature generation\ntasks. To address these challenges, this paper proposes an improved in-air\nfeature data model based on the GLNLSTM autoencoder. The signature\nsamples generated by this model are more authentic than the baseline\nmodel. Additionally, we introduce an in-air signature verification model\nbased on a two-channel model. The signature semantic feature extraction\nmodule of this model uses one-dimensional CNN and bidirectional LSTM\nto extract dynamic time features. This model achieves the best results\nin both deepfake verification and artificial forgery signature verification\ntasks on the SCUT-MMSIG-AIR database.",
            "Keywords": "Deepfake \u00b7 In-air signature generation \u00b7 In-air signature ver-\nification"
        },
        {
            "ID": "168",
            "Authors": [
                "Petr Kulagin",
                "Dmitry Polevoy",
                "Dmitry Nikolaev",
                "Marina Chukalina",
                "Vladimir V. Arlazarov"
            ],
            "Title": "Fully automatic virtual unwrapping method for documents imaged by X-ray tomography",
            "Abstract": "The study of historical documents faces challenges due to ag-\ning, particularly when rolled or folded, risking damage during unfolding.\nWhile computer tomography enables 3D digital replicas, direct exami-\nnation is inconvenient. To facilitate content analysis, various virtual un-\nfolding methods have been proposed. We present a groundbreaking, fully\nautomated system for virtual unfolding/unrolling, employing a neural\nnetwork to generate a binary document mask and perform skeletoniza-\ntion on 2D sections of the 3D volume. Additional algorithms address ar-\ntifacts, false loops, branching, and discontinuities. Introducing a unified\ncoordinate system for skeletal sections allows the generation of an un-\nfolded document image. Performance is assessed on the CT-OCR-2022\ndataset, utilizing a novel criterion for geometric distortion evaluation.\nEnriched with marker coordinates, the dataset facilitates future algo-\nrithm assessments. Accompanying source codes for the proposed algo-\nrithms and evaluation criterion are publicly available.",
            "Keywords": "Virtual unwrapping \u00b7 virtual unrolling \u00b7 virtual unfolding\n\u00b7 tomographic reconstruction \u00b7 cultural heritage \u00b7 automatic pipeline \u00b7\nunrolling accuracy."
        },
        {
            "ID": "169",
            "Authors": [
                "Yejing Xie",
                "Harold Mouch\u00e8re"
            ],
            "Title": "Stroke-Level Graph Labeling with Edge-weighted Graph Attention Network for Handwritten Mathematical Expression Recognition",
            "Abstract": "Handwritten Mathematic Expression Recognition (HMER)\nalgorithms with deep learning aproaches have developed rapidly in re-\ncent years, most algorithms are dependent on heavy pre-training and also\ncomplex network structures. Existing architectures are build on encoder-\ndecoder from on-line or off-line inputs to produce LATEX markup strings,\nor stroke-level graphs to generate symbol-level graphs. They all remain\non a latent space, which is not directly related to the input data: the\nstrokes. Using the Stroke Label Graph modelisation allows a direct con-\nnection between the input data and the output labels. In this research, we\nproposed a novel stroke-level graph labeling method with edge-weighted\ngraph attention network (EGAT). This lightweight model doesn\u2019t rely\non any pre-training, abandons the laborious process of encoder-decoder,\nis totaly end-to-end, directly accomplishes stroke-to-stroke feature ex-\ntraction, and produces strokes and relations classification. Experiments\nshow that our proposed EGAT algorithm can effectively fuse the node\nfeatures as well as the weighted edge features, and predict the node and\nedge attributes simultaneously.",
            "Keywords": "Handwritten Mathematical Expression Recognition \u00b7 Graph-\nbased approaches \u00b7 Graph Attention Network \u00b7 Stroke Level Labeling"
        },
        {
            "ID": "170",
            "Authors": [
                "Axel Carlier",
                "Thomas Forgione",
                "Travis Seng",
                "Vincent Charvillat",
                "Wei Tsang Ooi"
            ],
            "Title": "SlideCraft: Synthetic Slides Generation for Robust Slide Analysis",
            "Abstract": "The increasing amount of slide presentations in various sec-\ntors has amplified the need for effective slide layout and semantic analy-\nsis. However, we found that current slide datasets contain inconsistencies,\nmislabels, and incomplete annotations. Using them as a basis for develop-\ning deep learning-based slide analysis models could lead to models that\nare not robust and suboptimal. Addressing these challenges, we intro-\nduce SlideCraft, a tool for creating synthetic slide datasets that imitate\nreal-world presentations. This tool overcomes the drawbacks of existing\ndatasets by allowing users to create balanced, diverse, and accurately\nannotated slide data. We demonstrate SlideCraft\u2019s efficacy in enhancing\nslide layout analysis algorithms, focusing on its capability to improve\ndataset quality and object detection performance. Our code and a demo\ncan be found at this address.",
            "Keywords": "Slide datasets \u00b7 Slide analysis \u00b7 Open source tool \u00b7 Synthetic\ndataset."
        },
        {
            "ID": "172",
            "Authors": [
                "Tristan Repolusk",
                "Eduardo Veas"
            ],
            "Title": "The KuiSCIMA Dataset for Optical Music Recognition of Ancient Chinese Suzipu Notation",
            "Abstract": "In recent years, the development of Optical Music Recogni-\ntion (OMR) has progressed signi\ufb01cantly. However, music cultures with\nsmaller communities have only recently been considered in this process.\nThis results in a lack of adequate ground truth datasets needed for\nthe development and benchmarking of OMR systems. In this work, the\nKuiSCIMA (Jiang Kui Score Images for Musicological Analysis) dataset\nis introduced. KuiSCIMA is the \ufb01rst machine-readable dataset of the\nsuzipu notations in Jiang Kui\u2019s collection Baishidaoren Gequ from 1202.\nCollected from \ufb01ve di\ufb00erent woodblock print editions, the dataset con-\ntains 21797 manually annotated instances on 153 pages in total, from\nwhich 14500 are text character annotations, and 7297 are suzipu nota-\ntion symbols. The dataset comes with an open-source tool which allows\nediting, visualizing, and exporting the contents of the dataset \ufb01les. In\ntotal, this contribution promotes the preservation and understanding of\ncultural heritage through digitization.",
            "Keywords": "Ancient Chinese music \u00b7 Optical Music Recognition \u00b7\nSuzipu \u00b7 Banzipu \u00b7 Jiang Kui"
        },
        {
            "ID": "173",
            "Authors": [
                "Mathieu Aubry",
                "Elliot Vincent",
                "Fabienne Vial - Bonacci",
                "Zeynep Sonat Baltaci",
                "Remi Emonet",
                "Christelle Bahier - Porte",
                "Sayan Kumar Chaki",
                "Thierry Fournel"
            ],
            "Title": "Historical Printed Ornaments: Dataset and Tasks",
            "Abstract": "This paper aims to develop the study of historical printed or-\nnaments with modern unsupervised computer vision. We highlight three\ncomplex tasks that are of critical interest to book historians: cluster-\ning, element discovery, and unsupervised change localization. For each\nof these tasks, we introduce an evaluation benchmark, and we adapt\nand evaluate state-of-the-art models. Our Rey\u2019s Ornaments dataset is\ndesigned to be a representative example of a set of ornaments histori-\nans would be interested in. It focuses on an XVIIIth century bookseller,\nMarc-Michel Rey, providing a consistent set of ornaments with a wide\ndiversity and representative challenges. Our results highlight the limi-\ntations of state-of-the-art models when faced with real data and show\nsimple baselines such as k-means or congealing can outperform more so-\nphisticated approaches on such data. Our dataset and code can be found\nat https://printed-ornaments.github.io/.",
            "Keywords": "Book ornaments \u00b7 Clustering \u00b7 Element discovery \u00b7 Unsu-\npervised change localization"
        },
        {
            "ID": "176",
            "Authors": [
                "Yongge Liu",
                "Xinyu Wang",
                "Shengwei Han",
                "Xiang Bai",
                "Pengjie Wang",
                "Kaile Zhang",
                "Lianwen Jin",
                "Yuliang Liu"
            ],
            "Title": "Puzzle Pieces Picker: Deciphering Ancient Chinese Characters with Radical Reconstruction",
            "Abstract": "Oracle Bone Inscriptions is one of the oldest existing forms\nof writing in the world. However, due to the great antiquity of the era,\na large number of Oracle Bone Inscriptions (OBI) remain undeciphered,\nmaking it one of the global challenges in the field of paleography today.\nThis paper introduces a novel approach, namely Puzzle Pieces Picker\n(P3), to decipher these enigmatic characters through radical reconstruc-\ntion. We deconstruct OBI into foundational strokes and radicals, then\nemploy a Transformer model to reconstruct them into their modern coun-\nterparts, offering a groundbreaking solution to ancient script analysis. To\nfurther this endeavor, a new Ancient Chinese Character Puzzles (ACCP)\ndataset was developed, comprising an extensive collection of character\nimages from seven key historical stages, annotated with detailed radi-\ncal sequences. The experiments have showcased considerable promising\ninsights, underscoring the potential and effectiveness of our approach\nin deciphering the intricacies of ancient Chinese scripts. Through this\nnovel dataset and methodology, we aim to bridge the gap between tradi-\ntional philology and modern document analysis techniques, offering new\ninsights into the rich history of Chinese linguistic heritage.",
            "Keywords": "Historic Chinese Characters \u00b7 Oracle Bone Characters \u00b7 Op-\ntical Character Recognition \u00b7 Radical Recognition"
        },
        {
            "ID": "177",
            "Authors": [
                "Fadila Wendigoundi Douamba",
                "Ling Fu",
                "Jianjun Song",
                "Xiang Bai",
                "Yuliang Liu"
            ],
            "Title": "The First Swahili Language Scene Text Detection and Recognition Dataset",
            "Abstract": "Scene text recognition is essential in many applications, in-\ncluding automated translation, information retrieval, driving assistance,\nand enhancing accessibility for individuals with visual impairments. Much\nresearch has been done to improve the accuracy and performance of scene\ntext detection and recognition models. However, most of this research\nhas been conducted in the most common languages, English and Chi-\nnese. There is a significant gap in low-resource languages, especially the\nSwahili Language. Swahili is widely spoken in East African countries but\nis still an under-explored language in scene text recognition. No studies\nhave been focused explicitly on Swahili natural scene text detection and\nrecognition, and no dataset for Swahili language scene text detection and\nrecognition is publicly available. We propose a comprehensive dataset of\nSwahili scene text images and evaluate the dataset on different scene\ntext detection and recognition models. The dataset contains 976 images\ncollected in different places and under various circumstances. Each im-\nage has its annotation at the word level. The proposed dataset can also\nserve as a benchmark dataset specific to the Swahili language for evalu-\nating and comparing different approaches and fostering future research\nendeavours.",
            "Keywords": "Scene Text recognition \u00b7 Swahili Scene Text Dataset \u00b7 Swahili\nText Recognition"
        },
        {
            "ID": "178",
            "Authors": [
                "Zhi Pan",
                "Alimjan Aysa",
                "Yaowei Yang",
                "Kurban Ubul"
            ],
            "Title": "A New Bottom-up Path Augmentation Attention Network for Script Identification in Scene Images",
            "Abstract": "Script identification is an important component of a multi-\nlingual OCR system and plays a key role in the stability and accuracy of\nthe OCR system. The greatest challenge of the script identification task\nis that similar scripts share a large set that consists of the same or similar\ncharacters, which makes script identification a fine-grained classification\nproblem. Furthermore, when it comes to scene text script identification,\nadditional challenges emerge, like the complex background, various text\nstyles, arbitrary aspect ratios diverse noise, etc. In this paper, we design\nthe Feature Intensification module, which aims to reduce the interference\nof redundant features and intensify feature representation through im-\nplicit cross-channel interaction and information fusion. To better adapt\nto sequential texts, the improved Bottom-up Path Augmentation Struc-\nture is proposed to capture long-range dependencies and fuse multi-scale\nfeature maps more effectively. Moreover, by combining channel grouping\nand attention mechanism, the network can more accurately focus on the\ntext and each word in a picture. In the classification layer, we utilize\na fully convolutional classifier to generate channel-level classifications,\nwhich are then processed by a global pooling layer to improve classifica-\ntion efficiency. We evaluated the proposed method on the four benchmark\ndatasets, and the experimental results demonstrate the effectiveness of\neach carefully designed component. Finally, we achieved better perfor-\nmance compared to competitive models, with accuracy rates of 89.65%,\n96.11%, 98.88%, and 97.20% on RRC-MLT 2017, SIW-13, CVSI-2015,\nand MLe2e, respectively.",
            "Keywords": "Script identification \u00b7 Feature Intensification \u00b7 Implicit cross-\nchannel interaction \u00b7 Long-range dependencies \u00b7 Attention Mechanism .\n\u22c6 Corresponding author"
        },
        {
            "ID": "180",
            "Authors": [
                "Kl\u00e9berson F. Alves",
                "V. Rocha",
                "Alejandro H. Toselli",
                "Macileide F. Oliveira",
                "S\u00e1vio S. Ara\u00fajo",
                "Hugo J",
                "F. Hazin",
                "Byron L",
                "S. Neto",
                "Arthur F",
                "Pedro H",
                "S. Souza",
                "Wiliane M",
                "Samara V",
                "S. Lins",
                "D. Bezerra",
                "A"
            ],
            "Title": "BRESSAY: A Brazilian Portuguese Dataset for Offline Handwritten Text Recognition",
            "Abstract": "This work introduces the BRESSAY dataset, a novel con-\ntribution to the field of offline Handwritten Text Recognition (HTR),\nspecifically targeting Brazilian Portuguese. Despite significant advance-\nments in HTR, challenges remain due to the variability of human hand-\nwriting. This is particularly evident in academic contexts, where diverse\nhandwriting styles are common, often influenced by external factors such\nas time constraints and pressure. In this context, the BRESSAY dataset\nprovides a rich and complex environment that reflects these challenges,\nmaking it a valuable resource for developing robust optical models. It in-\ncludes a unique collection of student essays with special annotations such\nas superscript and subscript texts, various forms of illegible or crossed-\nout text, erasures, and a broad range of writing styles. In our research, we\nconducted an exploration using well-established optical models for line-\nlevel recognition. The initial study was designed to evaluate the dataset\u2019s\nutility in enhancing model performance, with the aim of establishing a\nbaseline for future research in this area. The dataset is expected to sig-\nnificantly contribute to the HTR community by aiding in the automated\ntranscription and analysis of handwritten educational materials. Conse-\nquently, our work not only adds a valuable resource to the HTR research\ndomain but also opens up possibilities for innovations in the field of au-\ntomated assessment based on handwritten documents. The BRESSAY\ndataset is available for download in the repository1",
            "Keywords": "dataset \u00b7 brazilian portuguese essays \u00b7 computer vision \u00b7\ndeep learning \u00b7 handwritten text recognition."
        },
        {
            "ID": "182",
            "Authors": [
                "Emanuele Nardone",
                "Cesare Davide Pace",
                "Tiziana D \u2019 Alessandro",
                "Claudio De Stefano",
                "Francesco Fontanella"
            ],
            "Title": "From Handwriting Analysis to Alzheimers Disease Prediction: An Experimental Comparison of Classifier Combination Methods",
            "Abstract": "In recent years, there has been a growing scientific interest\nin the study of effective methods for the early diagnosis of Alzheimer\u2019s\ndisease. In this area, it is generally agreed that handwriting analysis\ncan provide very promising contributions: the act of writing is the result\nof a complex process that involves various cognitive functions, including\nmemory, language, and other executive functions, all or partially affected\nin the early stages of Alzheimer\u2019s disease. Based on these considerations,\nvarious handwriting tasks have been proposed and designed to highlight\nthe different abilities that the onset of the disease could compromise,\nand various classification systems have been developed that use the in-\nformation derived from such data. It is useful to remark that the results\nof these tasks should be analyzed jointly since, as mentioned before, the\nfirst symptoms of the disease can concern different cognitive abilities of\nthe subjects involved: this explains the importance of applying classi-\nfier combination techniques, which allow the responses provided by the\nindividual classifiers on each single task, to be combined for improving\nthe overall classification accuracy. In this framework, our study aims\nto conduct a large set of experiments to compare the results of com-\nbining techniques for diagnosing Alzheimer\u2019s disease. The data used in\nthe experiments relates to a set of 174 subjects, including 89 patients\ndiagnosed with Alzheimer\u2019s disease and 85 healthy controls, to whom a\nwriting test consisting of 34 handwriting tasks was administered. The re-\nsults obtained allowed us, on the one hand, to evaluate the performance\nincrease obtained with the different combining techniques and, on the\nother, to characterize the contribution of the different handwriting tasks\nin the diagnosis of Alzheimer\u2019s disease.",
            "Keywords": "Classifier Combination \u00b7 Convolutional Neural Networks\n(CNN) \u00b7 Handwriting Analysis \u00b7 Alzheimer\u2019s Disease Prediction.\n\u22c6 Corresponding author (email: tiziana.dalessandro@unicas.it)."
        },
        {
            "ID": "185",
            "Authors": [
                "Aur\u00e9lie Joseph",
                "Mick",
                "Vincent Poulain d \u2019 Andecy",
                "##\u00ebl Coustaty",
                "Jean - Marc Ogier",
                "Ibrahim Souleiman Mahamoud"
            ],
            "Title": "CHIC: Corporate Document for Visual Question Answering",
            "Abstract": "The massive use of digital documents due to the substan-\ntial trend of paperless initiatives confronted some companies with find-\ning ways to process thousands of documents per day automatically. To\nachieve this, they use automatic information retrieval (IR) allowing them\nto extract useful information from large datasets quickly. In order to have\neffective IR methods, it is first necessary to have an adequate dataset.\nAlthough companies have enough data to take into account their needs,\nthere is also a need for a public database to compare contributions be-\ntween state-of-the-art methods. Some public document datasets already\nexist like DocVQA and XFUND, but they do not fully satisfy the needs\nof companies. First, XFUND contains only form documents while the\ncompany uses several types of documents (i.e. structured documents like\nforms but also semi-structured as invoices, and unstructured as emails).\nDocVQA, for its part, has several types of documents but only 4.5% of\nthem are business documents (i.e. invoice, purchase order, etc). All of\nthese 4.5% of documents do not meet the diversity of documents that\ncompanies may encounter in their daily document flow. In order to ex-\ntend these limitations, we propose in this paper the CHIC dataset, a\nvisual question-answering public dataset that contains different types of\nbusiness documents and the information extracted from these documents\nmeets the expectations of companies.",
            "Keywords": "Dataset, Visual Question Answering, Multimodality"
        },
        {
            "ID": "187",
            "Authors": [
                "Alejandro H. Toselli",
                "Enrique Vidal"
            ],
            "Title": "Zipf Curves and Basic Text Analytics from Untranscribed Manuscript Images",
            "Abstract": "The development of Probabilistic Indexing (PrIx) for large\nscale historical manuscript collections was originally driven by the need\nof searching for textual information in large collections of untranscribed\ntext images. The spots that result from the PrIx process are not image\ntranscripts, but they provide very rich probabilistic information about\nthe text rendered in speci\u001cc regions or locations of the images. This paper\npresents approaches that exploit this information to go beyond informa-\ntion search applications. In particular we consider text analytics tasks\nthat traditionally require proper textual data such as electronic text.\nFirst, for a PrIx-indexed text image collection, word frequency statisti-\ncal expectations are derived. These expected frequencies are then used to\nestimate the Zipf curve for the collection. Finally, based on the proper-\nties of Zipf curves, we estimate the amount of running words and the size\nof the lexicon used in the text images considered. Experimental results,\nreported on several large datasets, show that Zipf curves estimated from\ntext images, accurately match the real curves computed using the ground\ntruth image transcripts. In addition, the parameters derived from these\ncurves (running words and lexicon size) are also fairly accurate approxi-\nmations to the real values computed from the ground truth plaintext.",
            "Keywords": "Probabilistic Indexing \u00b7 HTR \u00b7 Zipf Curve \u00b7 Text Analytics\n\u00b7 Estimating Word Frequencies and Lexicon Size"
        },
        {
            "ID": "190",
            "Authors": [
                "Biao Yang",
                "Wenwen Yu",
                "Xiang Bai",
                "Chenyang Gao",
                "Yuliang Liu"
            ],
            "Title": "Knowledge Mining of Scene Text for Referring Expression Comprehension",
            "Abstract": "Text-based referring expression comprehension requires read-\ning and understanding scene text in an image to locate a specific object\ndescribed by a natural language expression. Existing methods predom-\ninantly focus on the literal interpretation of scene text. Such methods\noften fall short when the scene text has a tenuous connection to the\nobjects in the referring expressions. To address this limitation, we intro-\nduce a novel approach that leverages the implicit contextual knowledge\nunderlying scene text. More specifically, we construct a comprehensive\nknowledge base utilizing the Amazon Review Data Dataset. This knowl-\nedge base serves as the foundation for our proposed knowledge-enhanced\nscene text encoder. This encoder stands out for its proficiency in in-\ntegrating common knowledge to extract features more effectively. A key\nadvantage of our method is its compatibility and ease of integration with\nexisting text-based referring expression comprehension frameworks, lead-\ning to enhanced performance outcomes. Furthermore, we improved the\nquality of expressions in the TextREC dataset, by re-annotating the re-\nferring expressions with richer semantic information. Experiments on two\nbenchmark datasets, TextREC-v2 and RefText, show that our method\noutperforms the state-of-the-art by 3.3% and 1.0% in terms of preci-\nsion@1 measure.",
            "Keywords": "Referring expression comprehension \u00b7 Scene text\nrepresentation \u00b7 Multi-modal understanding"
        },
        {
            "ID": "191",
            "Authors": [
                "Zhi Pan"
            ],
            "Title": "Script Identification in the Wild with FFT-Multi-Grained Mix Attention Transformer",
            "Abstract": "Script identification plays an indispensable role in the sta-\nbility and accuracy of OCR systems, and its biggest challenge is the\nsimilarity between different scripts. Specifically, scene text-based script\nidentification is challenged by inter-language similarities, complex back-\ngrounds, and diverse text styles. To address the above problem, we use\nFFT Block to map the token to the frequency domain and decompose\nit into multiple frequency components, dynamically assign weights to\ndifferent frequency components within frequency domain, and then use\na well-designed Multi-Grained Mix Attention implementation to cal-\nculate the attention scores of tokens with different granularities. This\napproach improves upon the inability of Multi-Head Self Attention to\ncapture high-frequency information that primarily conveys local details.\nWe evaluated the proposed method on the benchmark datasets of RRC-\nMLT 2017, SIW-13, CVSI-2015, and MLe2e, achieving better perfor-\nmance than the competing models. Furthermore, we achieved the state-\nof-the-art (SOTA) on the RRC-MLT 2017 dataset, with correctness rates\nof 91.35%, 96.48%, 98.97%, and 97.35% for RRC-MLT 2017, SIW-13,\nCVSI-2015, and MLe2e, respectively.",
            "Keywords": "Script Identification \u00b7 Multi-Grained Mix Attention \u00b7 FFT\n\u00b7 Transformer."
        },
        {
            "ID": "199",
            "Authors": [
                "Hongxi Wei",
                "Yu Li",
                "Shiwen Sun"
            ],
            "Title": "LABT: A Sequence-to-Sequence Model for Mongolian Handwritten Text Recognition with Local Aggregation BiLSTM and Transformer",
            "Abstract": "Mongolian handwritten text recognition poses challenges\nwith the unique characteristics of Mongolian script, its large vocabulary,\nand the presence of out-of-vocabulary (OOV) words. This paper proposes\na model that uses local aggregation BiLSTM for sequence modeling\nof visual features and Transformer for word prediction.Specifically,\nwe introduce a local aggregation operation in BiLSTM (Bidirectional\nLong and Short Term Memory) to improve contextual understanding\nby aggregating adjacent information at each time step. The improved\nBiLSTM is able to capture context-dependent and letter shape changes\nthat occur in different contexts. It effectively addresses the difficulty\nof accurately identifying variable letters and generating OOV words\nwithout relying on predefined words during training. The contextual\nfeatures extracted by BiLSTM are passed through multiple layers of\nTransformer\u2019s encoder and decoder. At each layer, the representations\nof the previous layer are accessible, allowing layered representations to\nbe refined and improved. By using hierarchical representations, accurate\npredictions can be made even in large vocabulary text recognition\ntasks. Our proposed model achieves state-of-the-art performance on two\ncommonly used Mongolian handwritten text recognition datasets.",
            "Keywords": "Mongolian handwritten text recognition\u00b7 BiLSTM\u00b7 Local\naggregation\u00b7 Transformer."
        },
        {
            "ID": "201",
            "Authors": [
                "Roi Pony",
                "Ahmed Nassar",
                "Foad Abo Dahood",
                "Ophir Azulai",
                "Inbar Shapira",
                "Idan Friedman",
                "Christoph Auer",
                "Maksym Lysak",
                "Yevgeny Yaroker",
                "Nadav Rubinstein",
                "Peter Staar",
                "Udi Barzelay",
                "Oshri Naparstek",
                "Nikolaos Livathinos",
                "Orit Prince",
                "Elad Amrani",
                "Yevgeny Burshtein"
            ],
            "Title": "KVPk : A Comprehensive Dataset for Key-Value Pair Extraction in Business Documents",
            "Abstract": "In recent years, the challenge of extracting information from\nbusiness documents has emerged as a critical task, finding applications\nacross numerous domains. This effort has attracted substantial interest\nfrom both industry and academy, highlighting its significance in the cur-\nrent technological landscape. Most datasets in this area are primarily\nfocused on Key Information Extraction (KIE), where the extraction pro-\ncess revolves around extracting information using a specific, predefined\nset of keys. Unlike most existing datasets and benchmarks, our focus is on\ndiscovering key-value pairs (KVPs) without relying on predefined keys,\nnavigating through an array of diverse templates and complex layouts.\nThis task presents unique challenges, primarily due to the absence of\ncomprehensive datasets and benchmarks tailored for non-predetermined\nKVP extraction. To address this gap, we introduce KVP10k , a new\ndataset and benchmark specifically designed for KVP extraction. The\ndataset contains 10707 richly annotated images. In our benchmark, we\nalso introduce a new challenging task that combines elements of KIE as\nwell as KVP in a single task. KVP10k sets itself apart with its extensive\ndiversity in data and richly detailed annotations, paving the way for ad-\nvancements in the field of information extraction from complex business\ndocuments.\n1\nIntroduction\nExtracting KVPs from business documents is a critical task that holds significant\nimportance for businesses today. In an increasingly data-driven world, organiza-\ntions generate and receive vast amounts of unstructured textual data in the form\nof invoices, contracts, reports, and other documents. The ability to efficiently ex-\ntract relevant information in the form of key-value pairs from these documents\ncan greatly benefit businesses. It not only streamlines data entry processes but\nalso enables quick and accurate access to essential information, leading to im-\nproved decision-making, enhanced efficiency, and better overall business opera-\ntions. In this context, KVP extraction plays an important role in transforming",
            "Keywords": null
        },
        {
            "ID": "202",
            "Authors": [
                "Muhammad Shahzad Younis",
                "Maham Jahangir",
                "Azka Basit",
                "Faisal Shafait"
            ],
            "Title": "PCA-Based Adversarial Attacks on Signature Verification Systems",
            "Abstract": "Handwritten Signatures are popular biometrics that are used\nto authenticate individuals based on their unique physical or behavioral\nattributes. These systems rely on deep neural networks (DNN) for fea-\nture extraction. However, DNNs are vulnerable to small imperceptible\nperturbations. This research evaluates the robustness of signature veri-\nfication systems through adversarial attacks. The state-of-the-art adver-\nsarial attacks possess certain limitations: first attacks being white-box in\nnature are impractical, and second these attacks add noise to the whole\nimage including the background making them quite perceptible. To ad-\ndress these limitations, this study presents a lightweight approach based\non principal component analysis (PCA). Our novel algorithm generates\na universal noise vector using spatial transformation on principal compo-\nnents. It also strategically confines perturbation to specific regions while\nexploiting the principal components of the input image to launch at-\ntacks. We also computed evaluations conducted across three benchmark\ndatasets to demonstrate compelling outcomes, in terms of attack success\nrate, imperceptibility, and transferability.",
            "Keywords": "Adversarial Attack \u00b7 Principal Component Analysis \u00b7 Sig-\nnature Verification."
        },
        {
            "ID": "204",
            "Authors": [
                "Muhammad Zeshan Afzal",
                "Muhammad Saif Ullah Khan",
                "Sankalp Sinha",
                "Didier Stricker",
                "Talha Uddin Sheikh"
            ],
            "Title": "CICA: Content-Injected Contrastive Alignment for Zero-Shot Document Image Classification",
            "Abstract": "Zero-shot learning has been extensively investigated in the\nbroader field of visual recognition, attracting significant interest recently.\nHowever, the current work on zero-shot learning in document image clas-\nsification remains scarce. The existing studies either focus exclusively on\nzero-shot inference, or their evaluation does not align with the estab-\nlished criteria of zero-shot evaluation in the visual recognition domain.\nWe provide a comprehensive document image classification analysis in\nZero-Shot Learning (ZSL) and Generalized Zero-Shot Learning (GZSL)\nsettings to address this gap. Our methodology and evaluation align with\nthe established practices of this domain. Additionally, we propose zero-\nshot splits for the RVL-CDIP dataset. Furthermore, we introduce CICA3,\na framework that enhances the zero-shot learning capabilities of CLIP.\nCICA consists of a novel \u2019content module\u2019 designed to leverage any\ngeneric document-related textual information. The discriminative fea-\ntures extracted by this module are aligned with CLIP\u2019s text and image\nfeatures using a novel \u2019coupled-contrastive\u2019 loss. Our module improves\nCLIP\u2019s ZSL top-1 accuracy by 6.7% and GZSL harmonic mean by 24%\non the RVL-CDIP dataset. Our module is lightweight and adds only\n3.3% more parameters to CLIP. Our work sets the direction for future\nresearch in zero-shot document classification.",
            "Keywords": "Document Image Classification \u00b7 Zero-Shot Learning \u00b7 Gen-\neralised Zero-Shot Learning \u00b7 Multimodal Learning"
        },
        {
            "ID": "205",
            "Authors": [
                "Darko Obradovic",
                "Marcel Lamott",
                "Faisal Shafait",
                "Yves - Noel Weweler",
                "Dirk Krechel",
                "Adrian Ulges"
            ],
            "Title": "LAPDoc: Layout-Aware Prompting for Documents",
            "Abstract": "Recent advances in training large language models (LLMs)\nusing massive amounts of solely textual data lead to strong generalization\nacross many domains and tasks, including document-specific ones. On the\nother hand, there is a trend to train multi-modal transformer architec-\ntures tailored for document understanding that are designed specifically\nto fuse textual inputs with the corresponding document layout. This in-\nvolves a separate fine-tuning step for which additional training data is\nrequired. At present, no document transformers with comparable gen-\neralization to LLMs are available. This raises the question which type\nof model is to be preferred for document understanding tasks. In this\npaper we investigate the possibility to use purely text-based LLMs for\ndocument-specific tasks by using layout enrichment. We explore drop-\nin modifications and rule-based methods to enrich purely textual LLM\nprompts with layout information. In our experiments we investigate the\neffects on the commercial ChatGPT model and the open-source LLM So-\nlar. We demonstrate that using our approach both LLMs show improved\nperformance on various standard document benchmarks. In addition, we\nstudy the impact of noisy OCR and layout errors, as well as the limita-\ntions of LLMs when it comes to utilizing document layout. Our results\nindicate that layout enrichment can improve the performance of purely\ntext-based LLMs for document understanding by up to 15%, and by 6%\non average compared to just using plain document text. In conclusion,\nthis approach should be considered for the best model choice between\ntext-based LLM or multi-modal document transformers.",
            "Keywords": "Document Understanding \u00b7 Large Language Models \u00b7 Lay-\nout Understanding \u00b7 Prompt Enrichment"
        },
        {
            "ID": "210",
            "Authors": [
                "Seif Edinne Laatiri",
                "Wiam Adnan",
                "Laurent Lam",
                "Fabien Caspani",
                "Joel Tang",
                "Yassine Bel Khayat Zouggari"
            ],
            "Title": "A LayoutLMv-Based Model for Enhanced Relation Extraction in Visually-Rich Documents",
            "Abstract": "Document Understanding is an evolving field in Natural Lan-\nguage Processing (NLP). In particular, visual and spatial features are\nessential in addition to the raw text itself and hence, several multimodal\nmodels were developed in the field of Visual Document Understanding\n(VDU). However, while research is mainly focused on Key Information\nExtraction (KIE), Relation Extraction (RE) between identified entities\nis still under-studied. For instance, RE is crucial to regroup entities or\nobtain a comprehensive hierarchy of data in a document. In this paper,\nwe present a model that, initialized from LayoutLMv3, can match or\noutperform the current state-of-the-art results in RE applied to Visually-\nRich Documents (VRD) on FUNSD and CORD datasets, without any\nspecific pre-training and with fewer parameters. We also report an exten-\nsive ablation study performed on FUNSD, highlighting the great impact\nof certain features and modelization choices on the performances.",
            "Keywords": "Relation Extraction \u00b7 Visual Document Understanding \u00b7\nDocument Intelligence."
        },
        {
            "ID": "211",
            "Authors": [
                "Dimosthenis Karatzas",
                "Amelia G\u00f3mez Grabowska",
                "Jerod Weinman"
            ],
            "Title": "Counting the Corner Cases: Revisiting Robust Reading Challenge Data Sets Evaluation Protocols and Metrics",
            "Abstract": "For two decades, robust reading challenges (RRCs) have\ndriven and measured progress of text recognition systems in new and\ndi\ufb03cult domains. Such standardized benchmarks bene\ufb01t the \ufb01eld by al-\nlowing participants and observers to systematically track steady perfor-\nmance improvements as interest in the problem continues to grow. To\nbetter understand their impacts and create opportunities for further im-\nprovements, this work empirically analyzes three important aspects of\nseveral challenges from the last decade: data sets, evaluation protocols,\nand competition metrics. First, we explore implications of certain anno-\ntation protocols. Second, we identify limitations in existing evaluation\nprotocols that cause even the ground truth annotations to receive less\nthan perfect scores. To remedy this, we propose evaluation protocol up-\ndates that boost both recall and precision. Accounting for these corner\ncases causes almost no changes to current rankings; however, such cases\nmay become more prominent and important to consider as challenges\nfocus on increasingly complex reading tasks. Finally, inspired by the re-\ncent HierText challenge\u2019s use of Panoptic Quality (PQ), we explore the\nimpact of this simple, parameter-free tightness-aware metric on six prior\nchallenges, and we propose a new variant\u2014Panoptic Character Quality\n(PCQ)\u2014for simultaneously measuring character-level accuracy and word\ndetection tightness. We \ufb01nd PQ-based metrics have a greater re-ranking\nimpact on detection-only tasks, but predict end-to-end rankings slightly\nbetter than F-score. In sum, our empirical analysis and associated code\nshould allow future challenge designers to make better-informed choices.",
            "Keywords": "Scene text reading \u00b7 Evaluation protocols \u00b7 Optimization."
        },
        {
            "ID": "213",
            "Authors": [
                "Andreas Dengel",
                "Sheraz Ahmed",
                "Syed Jawwad Haider Hamdani",
                "Saifullah Saifullah",
                "Stefan Agne"
            ],
            "Title": "Latent Diffusion for Guided Document Table Generation",
            "Abstract": "Obtaining annotated table structure data for complex tables\nis a challenging task due to the inherent diversity and complexity of real-\nworld document layouts. The scarcity of publicly available datasets with\ncomprehensive annotations for intricate table structures hinders the de-\nvelopment and evaluation of models designed for such scenarios. This\nresearch paper introduces a novel approach for generating annotated im-\nages for table structure by leveraging conditioned mask images of rows\nand columns through the application of latent diffusion models. The\nproposed method aims to enhance the quality of synthetic data used for\ntraining object detection models. Specifically, the study employs a con-\nditioning mechanism to guide the generation of complex document table\nimages, ensuring a realistic representation of table layouts. To evaluate\nthe effectiveness of the generated data, we employ the popular YOLOv5\nobject detection model for training. The generated table images serve as\nvaluable training samples, enriching the dataset with diverse table struc-\ntures. The model is subsequently tested on the challenging pubtables-1m\ntestset, a benchmark for table structure recognition in complex document\nlayouts. Experimental results demonstrate that the introduced approach\nsignificantly improves the quality of synthetic data for training, lead-\ning to YOLOv5 models with enhanced performance. The mean Average\nPrecision (mAP) values obtained on the pubtables-1m testset showcase\nresults closely aligned with state-of-the-art methods. Furthermore, low\nFID results obtained on the synthetic data further validate the efficacy\nof the proposed methodology in generating annotated images for table\nstructure.",
            "Keywords": "synthetic table generation \u00b7 latent diffusion models \u00b7 diffu-\nsion transformers."
        },
        {
            "ID": "215",
            "Authors": [
                "Hamza Gbada",
                "Mohamed Ali Mahjoub",
                "Karim Kalti"
            ],
            "Title": "Information Extraction from Visually Rich Documents using Directed Weighted Graph Neural Network",
            "Abstract": "This paper presents a novel approach to information extrac-\ntion (IE) from visually rich documents (VRD) by employing a directed\nweighted graph representation to capture relationships among various\nVRD components. In contrast to conventional methods relying on spa-\ntial proximity through Euclidean distance, our approach aims to enhance\nperformance by introducing a novel representation of relationships using\ndirected weighted graphs. The information extraction task from VRD\nis treated as a node classification problem, leveraging graph convolu-\ntional networks that process the VRD graphs. We conducted evaluations\non five real-world datasets, showcasing notable results and performances\nthat align with established norms.",
            "Keywords": "Information Extraction \u00b7 Visually Rich Documents \u00b7 Graph\nconvolutional networks."
        },
        {
            "ID": "216",
            "Authors": [
                "Qiufeng Wang",
                "Weiguang Zhang",
                "Xiaomeng Gu",
                "Fengjun Guo",
                "Kaizhu Huang"
            ],
            "Title": "Coarse-to-Fine Document Image Registration for Dewarping",
            "Abstract": "Document dewarping has made great progress in recent years,\nhowever it usually requires huge document pairs with pixel-level anno-\ntation to learn a mapping function. Although photographed document\nimages are easy to obtain, the pixel-level annotation between warped\nand flat images is time-consuming and almost impossible for large-scale\ndatasets. To overcome this issue, we propose to register photographed\ndocuments with corresponding flat counterparts, obtaining the auto-\nannotation of pixel-level mapping labels. Due to the severe deformation\nin the real photographed documents, we introduce a coarse-to-fine reg-\nistration pipeline to learn global-scale transformation and local details\nalignment respectively. In addition, the lack of registration labels moti-\nvates us to tailor a teacher-student dual branch under semi-supervised\ntraining, where the model is initialized on synthetic documents with la-\nbels. Furthermore, we contribute a large-scale dataset containing 12,500\ntriplets of synthetic-real-flat documents. Extensive experiments demon-\nstrate the effectiveness of our proposed registration method. Specifically,\ntrained by our registered pixel-level documents, the dewarping model\ncan obtain comparable performance with SOTAs trained by almost 100\u00d7\nscale of samples, showing the high quality of our registration results. Our\ndataset and code are available at https://github.com/hanquansanren/DIRD.",
            "Keywords": "Document Dewarping \u00b7 Document Registration \u00b7 Coarse-\nto-Fine \u00b7 Semi-supervised Learning."
        },
        {
            "ID": "218",
            "Authors": [
                "Alexander Gayer",
                "Vladimir V. Arlazarov",
                "Daria Ershova",
                "Alexander Sheshkus"
            ],
            "Title": "An Ultra-Lightweight Approach for Machine Readable Zone Detection via Semantic Segmentation and Fast Hough Transform",
            "Abstract": "The Machine Readable Zone (MRZ) detection task is crucial\nfor automated document processing systems, particularly in identity ver-\nification and authentication tasks. Due to the limited memory capacity\nof embedded devices, modern deep learning models for MRZ localization\nshould not only show solid quality on challenging images but also have\na small size. In this paper, we present HED-MRZ (Hough Encoder for\nDetection) - an ultra-lightweight deep learning model for MRZ detection\nbased on semantic segmentation with direct and transposed Fast Hough\nTransform (FHT) layers. The usage of FHT layers allows us to deal with\nthe global receptive field on the first layers of the network that helps to re-\nduce not only the depth of the network but also the number of trainable\nparameters. Compared to the regression-based state-of-the-art YOLO-\nMRZ approach, HED-MRZ decreases number of undetected MRZs on\nthe MIDV-LAIT by 50%, as well as outperforms it on other challeng-\ning datasets such as MIDV-2019 and MIDV-2020. At the same time, it\nhas an order of magnitude fewer trainable parameters and weights only\n120KB, thereby making it an ideal solution for use on embedded devices.",
            "Keywords": "MRZ \u00b7 Semantic Segmentation \u00b7 Hough Transform \u00b7 Docu-\nment processing \u00b7 Deep Learning"
        },
        {
            "ID": "221",
            "Authors": [
                "Tim Raven",
                "Arthur Matei",
                "Gernot A",
                "Fink"
            ],
            "Title": "Self-Supervised Vision Transformers for Writer Retrieval",
            "Abstract": "While methods based on Vision Transformers (ViT) have\nachieved state-of-the-art performance in many domains, they have not\nyet been applied successfully in the domain of writer retrieval. The field is\ndominated by methods using handcrafted features or features extracted\nfrom Convolutional Neural Networks. In this work, we bridge this gap\nand present a novel method that extracts features from a ViT and ag-\ngregates them using VLAD encoding. The model is trained in a self-\nsupervised fashion without any need for labels. We show that extracting\nlocal foreground features is superior to using the ViT\u2019s class token in\nthe context of writer retrieval. We evaluate our method on two historical\ndocument collections. We set a new state-at-of-art performance on the\nHistorical-WI dataset (83.1% mAP), and the HisIR19 dataset (95.0%\nmAP). Additionally, we demonstrate that our ViT feature extractor can\nbe directly applied to modern datasets such as the CVL database (98.6%\nmAP) without any fine-tuning.",
            "Keywords": "Writer Retrieval \u00b7 Writer Identification \u00b7 Historical Docu-\nments \u00b7 Self-Supervised Learning \u00b7 Vision Transformer"
        },
        {
            "ID": "229",
            "Authors": [
                "Qiu - Feng Wang",
                "Yan - Ming Zhang",
                "Yijie Hu",
                "Kaizhu Huang"
            ],
            "Title": "Class Incremental Learning for Character String Recognition",
            "Abstract": "Character string recognition (CSR) has drawn much atten-\ntion for document intelligence, but its performance is limited by the pre-\ndefined character set without the ability to recognize new characters.\nTo overcome this issue, class incremental learning (CIL) can be adopted\nwhere the model learns from new data instances incrementally over time.\nHowever, it is challenging to directly apply existing CIL methods in CSR\nbecause CSR is a typical sequence recognition problem. Without accu-\nrate alignment, the recognition error of new characters will affect the\nrecognition of other characters in the same sequence. Moreover, the new\ncharacters are usually much fewer than the old ones, resulting in a data\nimbalance issue for learning new classes. To tackle the misalignment is-\nsue, we decouple the learning of feature alignment and classifiers during\nthe incremental process in CSR. To handle the data imbalance issue, we\npropose a Prototype Incremental Learning framework for CSR, namely\nPIL-CSR. In the PIL-CSR framework, we propose a prototype-centered\nloss (PCL) to aid the model in facilitating better class separation, and\nwe further propose a prototype separation and feature alignment (PSFA)\nstrategy, allowing the model to adapt and learn new classes seamlessly.\nFinally, we collect a CSR dataset to evaluate CIL performance 1. Exper-\nimental results demonstrate the effectiveness of our proposed sequence\nCIL method, obtaining a significant improvement in both line-level and\ncharacter-level accuracy.",
            "Keywords": "Character String Recognition \u00b7 Class Incremental Learning\n\u00b7 Sequence-to-Sequence \u00b7 OCR"
        },
        {
            "ID": "232",
            "Authors": [
                "Boraq Madi",
                "Reem Alaasam",
                "Jihad El - Sana"
            ],
            "Title": "Text Enhancement for Historical Handwritten Documents",
            "Abstract": "This paper presents a text enhancement method for histor-\nical handwritten documents. Text enhancement is a sub-field of super-\nresolution focused on document images and text. Our work is based on\ngenerative adversarial networks(GAN), and we have introduced a new\nevaluation metric for document image enhancement. Our approach de-\nnoises historical documents and holistically increases their resolution us-\ning GANs. We modified The generator structure of our GAN model by re-\nplacing Batch Norm layers with Residual-in-Residual Dense Blocks(RRDB)\nand adopting a discriminator based on the Relativistic GAN. Our eval-\nuation metric for text enhancement focuses on text quality based on the\nmagnitude of gradients at text edges to assess the improvement of the\ngenerated images. We tested our method on three degraded handwrit-\nten historical datasets of two languages and obtained excellent results.\nIn addition, We compare our approach with SRGAN, Nearest, and Bi-\nCubic interpolations and show that our method performs much better\nthan these methods on the three datasets. The proposed approach can\nhandle various types of noise while applying text enhancement up to 16\ntimes the input image.",
            "Keywords": "Super-Resolution, Historical Handwritten Documents, and\nGenerative Adversarial Networks"
        },
        {
            "ID": "237",
            "Authors": [
                "Xinhong Chen",
                "Dezhi Peng",
                "Chongyu Liu",
                "Chenfan Qu",
                "Bangdong Chen",
                "Lianwen Jin"
            ],
            "Title": "DTSM: Toward Dense Table Structure Recognition with Text Query Encoder and Adjacent Feature Aggregator",
            "Abstract": "In recent years, significant progress has been made in ta-\nble structure recognition, yet the recognition of structures within dense\ntables remains a challenge that has been largely overlooked. To ad-\ndress this gap, we introduce DenseTab, a new dataset consisting of\n16,575 dense tables with comprehensive annotation information that\nincludes physical position, logical position, and text content of each\ncell, along with the HTML sequence. To tackle the challenge of dense\ntable structure recognition, we propose a new method called Dense\nTable Splitting and Merging Model (DTSM). DTSM includes a novel\ntext query encoder to leverage layout information associated with the\ntext\u2019s location, and an adjacent feature aggregator to enhance the pre-\ndiction of cell merging information. Experimental results demonstrate\nthat our proposed method achieves state-of-the-art performance in rec-\nognizing dense table structures. The dataset and code is available at\nhttps://github.com/TenMilesLotus/DTSM.",
            "Keywords": "Table Structure Recognition \u00b7 Dense Table \u00b7 Dataset"
        },
        {
            "ID": "238",
            "Authors": [
                "Jianing Zhang",
                "Tong Zhang",
                "Rong Yan"
            ],
            "Title": "Synergistic Diverse Perspective for Topic Evolution Analysis on Weibo",
            "Abstract": "Nowadays, Weibo has been one of the most popular social media plat-\nforms for information dissemination, social interaction, and public opinion influ-\nence. It is an urgent study for grasping the dynamic evolution of public opinion\non Weibo. However, most of existing studies take little account of the evolution of\nlocal topics instead merely discussed the overall distribution of topics along the\ntimeline as well as the low quality of generated topics due to data sparsity. In this\npaper, we exploit an innovative method to extract the topic vectors from Weibo by\nfusing multi-semantic vectors denotation. The extracted vectors generate a new\ntopic representations from both the global and the local perspectives to enhance\nthe semantic depth of the topic descriptions. Extensive experimental results show\nthat the effectiveness of our proposed method that can comprehensively mine the\npotential evolutionary information on Weibo.",
            "Keywords": "Weibo \u00b7 Topic vector \u00b7 Topic evolution."
        },
        {
            "ID": "239",
            "Authors": [
                "David Doermann",
                "Bikhyat Adhikari",
                "Mahesh Bhosale",
                "Jay Lal",
                "Pengyu Yan"
            ],
            "Title": "ChartReformer: Natural Language-Driven Chart Image Editing",
            "Abstract": "Chart visualizations are essential for data interpretation and\ncommunication; however, most charts are only accessible in image for-\nmat and lack the corresponding data tables and supplementary informa-\ntion, making it difficult to alter their appearance for different scenarios\nof application. To eliminate the need for original underlying data and\ninformation to perform chart editing, we propose ChartReformer, a nat-\nural language-driven chart image editing solution that directly edits the\ncharts from the input images with the given instruction prompts. In-\nstead of predicting the plotting code, the key in this method is that we\nallow the model to comprehend the chart and reason over the prompt to\ngenerate the corresponding underlying data table and visual attributes\nfor new charts, enabling a precise and stable editing result. To general-\nize ChartReformer, we define and standardize the chart editing category\nand generate the ChartCraft dataset, covering style, layout, format, and\ndata-centric edits. The experiments show promising results for the nat-\nural language-driven chart image editing. Our datasets and model are\navailable at: https://github.com/pengyu965/ChartReformer.",
            "Keywords": "Chart Editing, Chart Appearance Editing, Chart Data Ex-\ntraction, Chart Understanding, Visual Language Model"
        },
        {
            "ID": "243",
            "Authors": [
                "Ajoy Mondal",
                "C V Jawahar",
                "Krishna Tulsyan"
            ],
            "Title": "Indic Scene Text on the Roadside",
            "Abstract": "Extensive research and the development of benchmark datasets have primarily\nfocused on Scene Text Recognition (STR) in Latin languages. However, the scenario differs for\nIndian languages, where the complexities in syntax and semantics have posed many challenges,\nresulting in limited datasets and comparatively less research in this domain. Overcoming\nthese challenges is crucial for advancing scene text recognition in Indian languages. Although\na few works have touched upon this issue, they are constrained in the size and scale of\nthe data as far as we know. To bridge this gap, this paper introduces a large scale, diverse\ndataset, named as IIIT-IndicSTR-Word for Indic scene text. Comprising a total of 250K word\nlevel images in ten different languages \u2014 Bengali, Gujarati, Hindi, Kannada, Malayalam,\nMarathi, Odia, Punjabi, Tamil, and Telugu, these images are extracted from roadside scenes\ncaptured by a GoPro camera. The dataset encompasses a wide array of realistic adversarial\nconditions, including blur, changes in illumination, occlusion, non-iconic texts, low resolution,\nand perspective text. We establish a baseline for the proposed dataset, facilitating evaluation\nand benchmarking with a specific focus on STR tasks. Our findings indicate that our dataset is\na practical training source to enhance performance on respective datasets. The code, dataset,\nand benchmark results are available at https://cvit.iiit.ac.in/usodi/istr.php.",
            "Keywords": "Scene Text Recognition (STR), word images, Indic languages, Indic scene text,\nroadside, benchmark."
        },
        {
            "ID": "244",
            "Authors": [
                "Tony Martinez",
                "Taylor Archibald"
            ],
            "Title": "DELINEK: A Synthetic Data Pipeline for the Semantic Segmentation of Historical Documents",
            "Abstract": "Document semantic segmentation is a promising avenue that\ncan facilitate document analysis tasks, including optical character recog-\nnition (OCR), form classification, and document editing. Although sev-\neral synthetic datasets have been developed to distinguish handwriting\nfrom printed text, they fall short in class variety and document diver-\nsity. We demonstrate the limitations of training on existing datasets\nwhen solving the National Archives Form Semantic Segmentation dataset\n(NAFSS), a dataset which we introduce. To address these limitations, we\npropose the most comprehensive document semantic segmentation syn-\nthesis pipeline to date, incorporating preprinted text, handwriting, and\ndocument backgrounds from over 10 sources to create the Document\nElement Layer INtegration Ensemble 8K, or DELINE8K dataset1. Our\ncustomized dataset exhibits superior performance on the NAFSS bench-\nmark, demonstrating it as a promising tool in further research.",
            "Keywords": "document binarization, semantic segmentation, data syn-\nthesis"
        },
        {
            "ID": "245",
            "Authors": [
                "Deepak Prakash",
                "Shubham Sharma",
                "Raghvendra Kumar",
                "Sriparna Saha"
            ],
            "Title": "IndicBART alongside Visual Element: Multimodal Summarization in Diverse Indian Languages",
            "Abstract": "In the age of information overflow, the demand for advanced\nsummarization techniques has surged, especially in linguistically diverse\nregions such as India. This paper introduces an innovative approach\nto multimodal multilingual summarization that seamlessly unites tex-\ntual and visual elements. Our research focuses on four prominent Indian\nlanguages: Hindi, Bangla, Gujarati, and Marathi, employing abstractive\nsummarization methods to craft coherent and concise summaries. For\ntext summarization, we leverage the capabilities of the pre-trained In-\ndicBART model, known for its exceptional proficiency in comprehending\nand generating text in Indian languages. We integrate an image summa-\nrization component based on the Image Pointer model to tackle multi-\nmodal challenges. This component identifies images from the input that\nenhance and complement the generated summaries, contributing to the\noverall comprehensiveness of our multimodal summaries. Our proposed\nmethodology attains excellent results, surpassing other text summariza-\ntion approaches tailored for the specified Indian languages. Furthermore,\nwe enhance the significance of our work by incorporating a user satis-\nfaction evaluation method, thereby providing a robust framework for\nassessing the quality of summaries. This holistic approach contributes\nto the advancement of summarization techniques, particularly in diverse\nIndian languages.",
            "Keywords": "Multimodal Summarization \u00b7 Indian Languages \u00b7 IndicBART"
        },
        {
            "ID": "249",
            "Authors": [
                "Shuo Zhang",
                "Zhang Li",
                "Biao Yang",
                "Xiang Bai",
                "Zhiyin Ma",
                "Yuliang Liu"
            ],
            "Title": "Exploring the Capabilities of Large Multimodal Models on Dense Text",
            "Abstract": "While large multi-modal models (LMM) have shown notable\nprogress in multi-modal tasks, their capabilities in tasks involving dense\ntextual content remains to be fully explored. Dense text, which carries\nimportant information, is often found in documents, tables, and prod-\nuct descriptions. Understanding dense text enables us to obtain more\naccurate information, assisting in making better decisions. To further\nexplore the capabilities of LMM in complex text tasks, we propose the\nDT-VQA dataset, with 170k question-answer pairs. In this paper, we\nconduct a comprehensive evaluation of GPT4V, Gemini, and various\nopen-source LMMs on our dataset, revealing their strengths and weak-\nnesses. Furthermore, we evaluate the effectiveness of two strategies for\nLMM: prompt engineering and downstream fine-tuning. We find that even\nwith automatically labeled training datasets, significant improvements\nin model performance can be achieved. We hope that this research will\npromote the study of LMM in dense text tasks.Code will be released at\nhttps://github.com/Yuliang-Liu/MultimodalOCR.",
            "Keywords": "Large multi-modal model \u00b7 Dense text visual question an-\nswering \u00b7 Evaluation."
        },
        {
            "ID": "255",
            "Authors": [
                "Shumpei Takezaki",
                "Daichi Haraguchi",
                "Seiichi Uchida",
                "Sho Shimotsumagari"
            ],
            "Title": "Cross-Domain Image Conversion by CycleDM",
            "Abstract": "The purpose of this paper is to enable the conversion between\nmachine-printed character images (i.e., font images) and handwritten\ncharacter images through machine learning. For this purpose, we propose\na novel unpaired image-to-image domain conversion method, CycleDM,\nwhich incorporates the concept of CycleGAN into the diffusion model.\nSpecifically, CycleDM has two internal conversion models that bridge\nthe denoising processes of two image domains. These conversion models\nare e\ufb00iciently trained without explicit correspondence between the do-\nmains. By applying machine-printed and handwritten character images\nto the two modalities, CycleDM realizes the conversion between them.\nOur experiments for evaluating the converted images quantitatively and\nqualitatively found that ours performs better than other comparable ap-\nproaches.",
            "Keywords": "diffusion model \u00b7 character image generation \u00b7 cross-domain."
        },
        {
            "ID": "257",
            "Authors": [
                "Mohammad Reza Soheili",
                "Mohammad Minouei",
                "Didier Stricker"
            ],
            "Title": "Embedding Layout in Text for Document Understanding Using Large Language Models",
            "Abstract": "In this paper, we address the challenge of effectively utiliz-\ning Large Language Models (LLMs) for Visually Rich Document Under-\nstanding (VRDU), a key part of intelligent document processing systems.\nWhile LLMs excel in various Natural Language Processing (NLP) tasks,\ntheir application for extracting information from complex structured doc-\numents like invoices and forms is limited. This limitation arises from the\ndifficulty in contextually understanding these documents, largely due to\nthe lack of layout information. Our research is dedicated to unlocking\nthe full potential of LLMs for VRDU by integrating OCR data into an\nHTML format, which preserves the essential spatial layout for accurate\ninformation extraction. The empirical results show a notable improve-\nment, with a more than 20 percent increase over baseline performances.\nThis research highlights the promising potential of LLMs in VRDU and\nsets the stage for further innovations in automated document processing.",
            "Keywords": "Document Understanding \u00b7 Large Language Model \u00b7 Infor-\nmation Extraction"
        },
        {
            "ID": "260",
            "Authors": [
                "Ahana Kundu",
                "Ujjwal Bhattacharya"
            ],
            "Title": "YOLO Assisted A Algorithm for Robust Line Segmentation of Degraded Document Images",
            "Abstract": "Although OCR from images of good quality documents can\nbe considered as a solved problem, the same is not true whenever its\nquality gets degraded due to certain reasons such as its very old age.\nOn the other hand, OCR of old documents has significant importance\ntowards preservation of cultural heritage, indexing, retrieval etc. The\ntask of degraded document OCR is often critical due to a number of\nreasons, including the high resemblance between noisy background and\nfaded foreground pixels, asymmetric skews of different lines etc. The\nstudy presented in this article has been conducted on a dataset of re-\ncently collected sample images of old severely degraded document pages\nin addition to a few others and the task is very difficult due to the high\ndegradation level of the samples and lack of training ground truths. Here,\nwe propose a hybrid approach combining both of a learning-based and\nanother rule-based methods for line segmentation of similar degraded\ndocuments. The proposed method utilizes well-known object detection\nsystem YOLO, trained on a publicly available dataset of handwritten\nsamples, to predict starting point (left extreme point) of each line di-\nvider, the remaining part of the segmenting line has been obtained using\na modified version of graph traversing approach \u2018A* path finding\u2019. Thus,\nthe path of the segmenting line suitably dividing two consecutive text\nlines starting from the predicted left end point and terminating at the\nright end point could be obtained. The proposed approach has overcome\nvarious existing challenges of line segmentation of old degraded quality\ndocuments and improved results on several publicly available datasets.\nPerformance comparisons of three existing strategies on five datasets of\ndifferent languages and varying degradation levels, both of printed and\nhandwritten texts have been presented in this article.",
            "Keywords": "Degraded document image \u00b7 Line segmentation \u00b7 Object\ndetection \u00b7 Path finding."
        },
        {
            "ID": "266",
            "Authors": [
                "Dimosthenis Karatzas",
                "Emanuele Vivoli",
                "Ernest Valveny Llobet",
                "Joan Lafuente Baeza"
            ],
            "Title": "Multimodal Transformer for Comics Text-Cloze",
            "Abstract": "This work explores a closure task in comics, a medium where\nvisual and textual elements are intricately intertwined. Specifically, text-\ncloze refers to the task of selecting the correct text to use in a comic\npanel, given its neighbouring panels. Traditional methods based on recur-\nrent neural networks, have struggled with this task due to limited OCR\naccuracy and inherent model limitations. We introduce a novel Multi-\nmodal Large Language Model (Multimodal-LLM) architecture, specifi-\ncally designed for text-cloze, achieving a 10% improvement over exist-\ning state-of-the-art models in both its easy and hard variants. Central\nto our approach is a Domain-Adapted ResNet-50 based visual encoder,\nfine-tuned to the comics domain in a self-supervised manner using Sim-\nCLR. This encoder delivers comparable results to more complex mod-\nels with just one-fifth of the parameters. Additionally, we release new\nOCR annotations for this dataset, enhancing model input quality and\nresulting in another 1% improvement. Finally, we extend the task to\na generative format, establishing new baselines and expanding the re-\nsearch possibilities in the field of comics analysis. Code is available at\nhttps://github.com/joanlafuente/ComicVT5.",
            "Keywords": "comics analysis \u00b7 text-cloze \u00b7 VL-T5 \u00b7 SimCLR \u00b7 ResNet"
        },
        {
            "ID": "270",
            "Authors": [
                "Jacob Murel",
                "David Smith"
            ],
            "Title": "Self-training and Active Learning with Pseudo-Relevance Feedback for Handwriting Detection in Historical Print",
            "Abstract": "Handwritten text recognition research largely focuses on en-\ntirely handwritten documents, yet many bibliographic researchers are\ninterested in handwriting left by readers in historical print. Due to the\nsparse and inconsistent appearance of handwritten annotations, com-\npiling sufficient datasets of handwriting in print can be difficult. We\npropose a method for utilizing visual similarities among text exemplars\nfor improving handwriting detection in historical print. We investigate\nthe effect of pseudo-labeled page images on improving object detection\nmodel performance for handwriting localization across multiple exem-\nplars of Shakespeare\u2019s First Folio. We compare differences in self-training\nand active learning with pseudo-labels for positive and negative-sample\nimages, using pseudo-relevance and relevance feedback as selection meth-\nods. We find that pseudo-labels from positive and negative-sample im-\nages improve detection task performance on individual exemplars with\nan average precision increase of 15%. Tests on collections of multiple\nexemplars are less conclusive. We discuss how variations in historical\nprint\u2019s materiality may explain these results and outline further research\nto investigate this matter.",
            "Keywords": "Object detection \u00b7 Handwriting \u00b7 Convolutional neural net-\nwork \u00b7 Historical print \u00b7 Digital humanities"
        },
        {
            "ID": "271",
            "Authors": "Ting-Wei LIAO and Hsiang-An WANG",
            "Title": "Recognition of Components in Taoist Charm Images",
            "Abstract": "Taoist charms are used in Chinese culture for communication\nwith spirits, offering healing, exorcism, and supernatural abilities. These\ncharms, often crafted by Taoist priests, feature a variety of characters\nand symbols, differing across sects, which complicates manual recogni-\ntion and study. This paper focuses on automating the recognition and\nretrieval of charm components to ease research efforts. By manually an-\nnotating 998 charm images into 107 component categories, we addressed\ndata scarcity and imbalance through data augmentation, expanding the\ndataset significantly. We refined data labeling accuracy and minimized\ndistortions in synthetic data, employing deep learning techniques for\nobject detection and segmentation. After testing different models, we\noptimized our approach by pre-training the model\u2019s backbone network\non a large Chinese text dataset, enhancing the model\u2019s ability to recog-\nnize charm components. Achieving a bounding box mAP of 0.764 and a\nmask mAP of 0.726.",
            "Keywords": "Chinese text \u00b7 Data augmentation\u00b7 Image segmentation\u00b7\nObject detection\u00b7 Taoist charms."
        },
        {
            "ID": "273",
            "Authors": [
                "Adarsh Tiwari",
                "Sanket Biswas",
                "Josep Llad\u00f3s"
            ],
            "Title": "SketchGPT: Autoregressive Modeling for Sketch Generation and Recognition",
            "Abstract": "We present SketchGPT, a flexible framework that employs\na sequence-to-sequence autoregressive model for sketch generation, and\ncompletion, and an interpretation case study for sketch recognition. By\nmapping complex sketches into simplified sequences of abstract prim-\nitives, our approach significantly streamlines the input for autoregres-\nsive modeling. SketchGPT leverages the next token prediction objec-\ntive strategy to understand sketch patterns, facilitating the creation and\ncompletion of drawings and also categorizing them accurately. This pro-\nposed sketch representation strategy aids in overcoming existing chal-\nlenges of autoregressive modeling for continuous stroke data, enabling\nsmoother model training and competitive performance. Our findings ex-\nhibit SketchGPT\u2019s capability to generate a diverse variety of drawings\nby adding both qualitative and quantitative comparisons with existing\nstate-of-the-art, along with a comprehensive human evaluation study.\nThe code and pretrained models will be released on this GitHub\u2020.",
            "Keywords": "Sketch Completion \u00b7 Sketch Recognition \u00b7 Next Stroke Pre-\ndiction \u00b7 Stroke-to-Primitive Mapping \u00b7 Autoregressive Models"
        },
        {
            "ID": "274",
            "Authors": [
                "Qidi Luo",
                "Hiba Maryam",
                "Shafayet",
                "Ling Fu",
                "Xiang Bai",
                "Jiajun Song",
                "Tajrian",
                "Yuliang Liu"
            ],
            "Title": "Dataset and Benchmark for Urdu Natural Scenes Text Detection Recognition and Visual Question Answering",
            "Abstract": "The development of Urdu scene text detection, recognition,\nand Visual Question Answering (VQA) technologies is crucial for ad-\nvancing accessibility, information retrieval, and linguistic diversity in\ndigital content, facilitating better understanding and interaction with\nUrdu-language visual data. This initiative seeks to bridge the gap between\ntextual and visual comprehension. We propose a new multi-task Urdu\nscene text dataset comprising over 1000 natural scene images, which\ncan be used for text detection, recognition, and VQA tasks. We provide\nfine-grained annotations for text instances, addressing the limitations\nof previous datasets for facing arbitrary-shaped texts. By incorporating\nadditional annotation points, this dataset facilitates the development\nand assessment of methods that can handle diverse text layouts, intri-\ncate shapes, and non-standard orientations commonly encountered in\nreal-world scenarios. Besides, the VQA annotations make it the first\nbenchmark for the Urdu Text VQA method, which can prompt the\ndevelopment of Urdu scene text understanding.",
            "Keywords": "Urdu Scene Text Dataset \u00b7 Urdu Text Recognition \u00b7 Urdu\nNatural Image \u00b7 Visual Question Answering"
        },
        {
            "ID": "275",
            "Authors": [
                "Jan Haji\u010d",
                "Pavel Pecina",
                "Ji\u0159\u00ed Mayer",
                "Milan Straka"
            ],
            "Title": "Practical End-to-End Optical Music Recognition for Pianoform Music",
            "Abstract": "The majority of recent progress in Optical Music Recogni-\ntion (OMR) has been achieved with Deep Learning methods, especially\nmodels following the end-to-end paradigm that read input images and\nproduce a linear sequence of tokens. Unfortunately, many music scores,\nespecially piano music, cannot be easily converted to a linear sequence.\nThis has led OMR researchers to use custom linearized encodings, in-\nstead of broadly accepted structured formats for music notation. Their\ndiversity makes it difficult to compare the performance of OMR systems\ndirectly. To bring recent OMR model progress closer to useful results:\n(a) We define a sequential format called Linearized MusicXML, allow-\ning us to train an end-to-end model directly and maintain close cohe-\nsion and compatibility with the industry-standard MusicXML format.\n(b) We create a dev and test set for benchmarking typeset OMR based\non OpenScore Lieder corpus, containing 1,438 and 1,493 pianoform sys-\ntems, each with an image from IMSLP and MusicXML ground truth.\n(c) We train and fine-tune an end-to-end model to serve as a baseline\non the dataset and employ the TEDn metric to evaluate the model. We\nalso test our model against the recently published synthetic pianoform\ndataset GrandStaff and surpass the state-of-the-art results.",
            "Keywords": "Optical Music Recognition \u00b7 Evaluation \u00b7 Datasets."
        },
        {
            "ID": "284",
            "Authors": [
                "Christopher Kermorvant",
                "Manon Blanco",
                "Eva Bardou",
                "Bastien Abadie"
            ],
            "Title": "Callico: a Versatile Open-Source Document Image Annotation Platform",
            "Abstract": "This paper presents Callico, a web-based open source plat-\nform designed to simplify the annotation process in document recogni-\ntion projects. The move towards data-centric AI in machine learning and\ndeep learning underscores the importance of high-quality data, and the\nneed for specialised tools that increase the efficiency and effectiveness\nof generating such data. For document image annotation, Callico offers\ndual-display annotation for digitised documents, enabling simultaneous\nvisualisation and annotation of scanned images and text. This capability\nis critical for OCR and HTR model training, document layout analysis,\nnamed entity recognition, form-based key value annotation or hierarchi-\ncal structure annotation with element grouping. The platform supports\ncollaborative annotation with versatile features backed by a commitment\nto open source development, high-quality code standards and easy de-\nployment via Docker. Illustrative use cases - including the transcription\nof the Belfort municipal registers, the indexing of French World War II\nprisoners for the ICRC, and the extraction of personal information from\nthe Socface project\u2019s census lists - demonstrate Callico\u2019s applicability\nand utility.",
            "Keywords": "Document Image Annotation \u00b7 Open-Source Software \u00b7 Col-\nlaborative Transcription"
        },
        {
            "ID": "285",
            "Authors": [
                "Nauman Riaz",
                "Andreas Dengel",
                "Sheraz Ahmed",
                "Saifullah Saifullah",
                "Stefan Agne"
            ],
            "Title": "StylusAI: Stylistic Adaptation for Robust German Handwritten Text Generation",
            "Abstract": "In this study, we introduce StylusAI, a novel architecture\nleveraging diffusion models in the domain of handwriting style genera-\ntion. StylusAI is specifically designed to adapt and integrate the stylistic\nnuances of one language\u2019s handwriting into another, particularly focusing\non blending English handwriting styles into the context of the German\nwriting system. This approach enables the generation of German text in\nEnglish handwriting styles and German handwriting styles into English,\nenriching machine-generated handwriting diversity while ensuring that\nthe generated text remains legible across both languages. To support\nthe development and evaluation of StylusAI, we present the \u2018Deutscher\nHandschriften-Datensatz\u2019 (DHSD), a comprehensive dataset encompass-\ning 37 distinct handwriting styles within the German language. This\ndataset provides a fundamental resource for training and benchmarking\nin the realm of handwritten text generation. Our results demonstrate\nthat StylusAI not only introduces a new method for style adaptation in\nhandwritten text generation but also surpasses existing models in gener-\nating handwriting samples that improve both text quality and stylistic\nfidelity, evidenced by its performance on the IAM database and our newly\nproposed DHSD. Thus, StylusAI represents a significant advancement in\nthe field of handwriting style generation, offering promising avenues for\nfuture research and applications in cross-linguistic style adaptation for\nlanguages with similar scripts.",
            "Keywords": "Handwriting Generation \u00b7 Diffusion Models\u00b7 Handwriting\nText Recognition \u00b7 Transformers"
        },
        {
            "ID": "289",
            "Authors": [
                "Sanket Biswas",
                "Carlos Boned",
                "Nil Biescas",
                "Josep Llad\u00f3s"
            ],
            "Title": "GeoContrastNet: Contrastive Key-Value Edge Learning for Language-Agnostic Document Understanding",
            "Abstract": "This paper presents GeoContrastNet, a language-agnostic\nframwork to structured document understanding (DU) by integrating a\ncontrastive learning objective with graph attention networks (GATs),\nemphasizing the significant role of geometric features. We propose a\nnovel methodology that combines geometric edge features with visual\nfeatures within an overall two-staged GAT-based framework, demon-\nstrating promising results in both link prediction and semantic entity\nrecognition performance. Our findings reveal that combining both geo-\nmetric and visual features could match the capabilities of large DU mod-\nels that rely heavily on Optical Character Recognition (OCR) features\nin terms of performance accuracy and efficiency. This approach under-\nscores the critical importance of relational layout information between\nthe named text entities in a semi-structured layout of a page. Specifically,\nour results highlight the model\u2019s proficiency in identifying key-value re-\nlationships within the FUNSD dataset for forms and also discovering the\nspatial relationships in table-structured layouts for RVLCDIP business\ninvoices. Our code is accessible on this GitHub\u2020.",
            "Keywords": "Document Understanding \u00b7 Graph Neural Networks \u00b7 Con-\ntrastive Learning \u00b7 Language-Agnostic Learning"
        },
        {
            "ID": "291",
            "Authors": [
                "George Retsinas",
                "Giorgos Sfikas",
                "Konstantina Nikolaidou"
            ],
            "Title": "Enhancing CRNN HTR Architectures with Transformer Blocks",
            "Abstract": "Handwritten Text Recognition (HTR) is a challenging prob-\nlem that plays an essential role in digitizing and interpreting diverse\nhandwritten documents. While traditional approaches primarily utilize\nCNN-RNN (CRNN) architectures, recent advancements based on Trans-\nformer architectures have demonstrated impressive results in HTR. How-\never, these Transformer-based systems often involve high-parameter con-\nfigurations and rely extensively on synthetic data. Moreover, they lack\nfocus on efficiently integrating the ability of Transformer modules to\ngrasp contextual relationships within the data. In this paper, we explore\na lightweight integration of Transformer modules into existing CRNN\nframeworks to address the complexities of HTR, aiming to enhance the\ncontext of the sequential nature of the task. We present a hybrid CNN\nimage encoder with intermediate MobileViT blocks that effectively com-\nbines the different components in a resource-efficient manner. Through\nextensive experiments and ablation studies, we refine the integration of\nthese modules and demonstrate that our proposed model enhances HTR\nperformance. Our results on the line-level IAM and RIMES datasets sug-\ngest that our proposed method achieves competitive performance with\nsignificantly fewer parameters and without integrating synthetic data\ncompared to existing systems.",
            "Keywords": "Handwriting Text Recognition \u00b7 Transformer Modules"
        },
        {
            "ID": "296",
            "Authors": [
                "Umapada Pal",
                "Sanket Biswas",
                "Ayan Banerjee",
                "Josep Llad\u00f3s"
            ],
            "Title": "GraphKD: Exploring Knowledge Distillation Towards Document Object Detection with Structured Graph Creation",
            "Abstract": "Object detection in documents is a key step to automate the\nstructural elements identification process in a digital or scanned docu-\nment through understanding the hierarchical structure and relationships\nbetween different elements. Large and complex models, while achieving\nhigh accuracy, can be computationally expensive and memory-intensive,\nmaking them impractical for deployment on resource constrained de-\nvices. Knowledge distillation allows us to create small and more effi-\ncient models that retain much of the performance of their larger coun-\nterparts. Here we present a graph-based knowledge distillation frame-\nwork to correctly identify and localize the document objects in a docu-\nment image. Here, we design a structured graph with nodes containing\nproposal-level features and edges representing the relationship between\nthe different proposal regions. Also, to reduce text bias an adaptive node\nsampling strategy is designed to prune the weight distribution and put\nmore weightage on non-text nodes. We encode the complete graph as a\nknowledge representation and transfer it from the teacher to the student\nthrough the proposed distillation loss by effectively capturing both local\nand global information concurrently. Extensive experimentation on com-\npetitive benchmarks demonstrates that the proposed framework outper-\nforms the current state-of-the-art approaches. The code will be available\nat: github.com/ayanban011/GraphKD",
            "Keywords": "Knowledge Distillation \u00b7 Document Object Detection \u00b7 Graph\nNeural Network."
        },
        {
            "ID": "316",
            "Authors": [
                "Dimosthenis Karatzas",
                "Khanh Nguyen"
            ],
            "Title": "Federated Document Visual Question Answering: A Pilot Study",
            "Abstract": "An important handicap of document analysis research is that\ndocuments tend to be copyrighted or contain private information, which\nprohibits their open publication and the creation of centralised, large-\nscale document datasets. Instead, documents are scattered in private\ndata silos, making extensive training over heterogeneous data a tedious\ntask. In this work, we explore the use of a federated learning (FL) scheme\nas a way to train a shared model on decentralised private document data.\nWe focus on the problem of Document VQA, a task particularly suited\nto this approach, as the type of reasoning capabilities required from the\nmodel can be quite different in diverse domains. Enabling training over\nheterogeneous document datasets can thus substantially enrich DocVQA\nmodels. We assemble existing DocVQA datasets from diverse domains to\nreflect the data heterogeneity in real-world applications. We explore the\nself-pretraining technique in this multi-modal setting, where the same\ndata is used for both pretraining and finetuning, making it relevant for\nprivacy preservation. We further propose combining self-pretraining with\na Federated DocVQA training method using centralized adaptive opti-\nmization that outperforms the FedAvg baseline. With extensive exper-\niments1, we also present a multi-faceted analysis on training DocVQA\nmodels with FL, which provides insights for future research on this task.\nWe show that our pretraining strategies can effectively learn and scale\nup under federated training with diverse DocVQA datasets and tuning\nhyperparameters is essential for practical document tasks under federa-\ntion.",
            "Keywords": "Document Understanding \u00b7 Document Visual Question An-\nswering \u00b7 Federated Learning."
        },
        {
            "ID": "318",
            "Authors": [
                "Anna Scius - Bertrand",
                "Daniel Ribeiro Cabral",
                "Atefeh Fakhari",
                "Lars V\u00f6gtlin",
                "Andreas Fischer"
            ],
            "Title": "Are Layout Analysis and OCR Still Useful for Document Information Extraction using Foundation Models",
            "Abstract": "With the advent of end-to-end models and the remarkable\nperformance of foundation models, the question arises regarding the rele-\nvance of preliminary steps, such as layout analysis and optical character\nrecognition (OCR), for information extraction from document images.\nWe attempt to provide some answers through experiments conducted on\na new database of food labels. The goal is to extract nutritional val-\nues from cellphone pictures taken in grocery stores. We compare the\nresults of OCR-free models that take the raw images as input (Donut\nand GPT-4-Vision) with two-stage systems that \ufb01rst perform OCR and\nthen extract information using large language models (LLMs) from the\nrecognized text (Mistral, GPT-3, and GPT-4). To assess the impact of\nlayout analysis, we applied the same systems to three di\ufb00erent views of\nthe image: the original full image, a large manual crop containing the\nentire food label, and a small crop focusing on the relevant nutrition in-\nformation. Comparative experiments are also conducted on the CORD\ndatabase of receipts. Our results demonstrate that although OCR-free\nmodels achieve a remarkable performance, they still require some guid-\nance regarding the layout, and two-stage systems achieve better results\noverall.",
            "Keywords": "Document Image Analysis \u00b7 Information Extraction \u00b7 OCR \u00b7\nLayout Analysis \u00b7 Deep Learning \u00b7 Transformers \u00b7 Large Language Models"
        },
        {
            "ID": "323",
            "Authors": [
                "D",
                "Max G \u2019 Sell",
                "Samuel V. Lemley",
                "Christopher N",
                "Kartik Goyal",
                "Warren",
                "J. Schuldt",
                "Nikolai Vogler",
                "Taylor Berg - Kirkpatrick"
            ],
            "Title": "Clustering Running Titles to Understand the Printing of Early Modern Books",
            "Abstract": "We propose a novel computational approach to automati-\ncally analyze the physical process behind printing of early modern let-\nterpress books via clustering the running titles found at the top of their\npages. Specifically, we design and compare custom neural and feature-\nbased kernels for computing pairwise visual similarity of a scanned doc-\nument\u2019s running titles and cluster the titles in order to track any devi-\nations from the expected pattern of a book\u2019s printing. Unlike body text\nwhich must be reset for every page, the running titles are one of the\nstatic type elements in a skeleton forme i.e. the frame used to print each\nside of a sheet of paper, and were often re-used during a book\u2019s print-\ning. To evaluate the effectiveness of our approach, we manually annotate\nthe running title clusters on about 1600 pages across 8 early modern\nbooks of varying size and formats. Our method can detect potential de-\nviation from the expected patterns of such skeleton formes, which helps\nbibliographers understand the phenomena associated with a text\u2019s trans-\nmission, such as censorship. We also validate our results against a manual\nbibliographic analysis of a counterfeit early edition of Thomas Hobbes\u2019\nLeviathan (1651) [27].5",
            "Keywords": "Historical document analysis \u00b7 Document provenance \u00b7 Doc-\nument image processing \u00b7 Document clustering"
        },
        {
            "ID": "C01",
            "Authors": [
                "Zhifei Zhang",
                "Xudong Xie",
                "Linger Deng",
                "Zhaowen Wang",
                "Yuliang Liu"
            ],
            "Title": "ICDAR  Competition on Artistic Text Recognition",
            "Abstract": "Artistic text is widely used in advertisements, slogans, exhi-\nbitions, decorations, magazines, and books. However, artistic text recog-\nnition is an overlooked and extremely challenging task with importance\nand practicability in various applications. Artistic text recognition of-\nten has several challenges such as the various appearances with special-\ndesigned fonts and effects, the complex connections and overlaps between\ncharacters, and the severe interference from background patterns. There-\nfore, we organized the ICDAR 2024 Competition on Artistic Text Recog-\nnition to invite participants to solve these challenges. We propose the\nWordArt-V1.5 dataset to advance the field by incorporating a broader\nrange of artistic text images sourced from diverse scenes. This enhanced\nartistic text recognition dataset contains a total of 12,000 images with\n6,000 for training and 6,000 for testing. The competition attracted 33 par-\nticipants and received 126 submissions with a best accuracy of 91.07%.\nIn this paper, we provide an overview of the competition, detailing the\nproposed dataset, task, evaluation protocol, and result summaries.",
            "Keywords": "Artistic text recognition \u00b7 Scene text recognition \u00b7 OCR"
        },
        {
            "ID": "C02",
            "Authors": [
                "Emanuela Colombi",
                "Silvia Zottin",
                "Claudio Piciarelli",
                "Gian Luca Foresti",
                "Axel De Nardin"
            ],
            "Title": "ICDAR  Competition on Few-Shot and Many-Shot Layout Segmentation of Ancient Manuscripts SAM",
            "Abstract": "Layout analysis is a critical aspect of Document Image Anal-\nysis, particularly when it comes to ancient manuscripts. It serves as a\nfoundational step in streamlining subsequent tasks such as optical char-\nacter recognition and automated transcription. However, one key chal-\nlenge in this context is represented by the lack of available ground truths\nas they are extremely time-consuming to produce. Nevertheless, numer-\nous approaches addressing this challenge heavily lean towards a fully\nsupervised learning paradigm, which represents a rare scenario in a real-\nworld setting. For this reason, with this competition, we propose the chal-\nlenge of addressing this task with a few-shot learning approach, involving\nthe use of only three images for training. The competition dataset, called\nU-DIADS-Bib, comprises four distinct ancient manuscripts, presenting\nheterogeneous layout structures, levels of degradation, and languages\nused. This diversity adds intrigue and complexity to the challenge. In\naddition, we have also allowed participating in the competition with tra-\nditional many-shot learning approaches, for which the whole training set\nof U-DIADS-Bib was made available.",
            "Keywords": "Document Layout Analysis \u00b7 Few-shot Learning \u00b7 Image\nSegmentation"
        },
        {
            "ID": "C03",
            "Authors": [
                "Be\u00e1ta Megyesi",
                "Nils Kopal",
                "Michelle Waldisp\u00fchl",
                "George Lasry",
                "Pau Torras Carles Badal",
                "Jialuo Chen",
                "Alicia Forn\u00e9s"
            ],
            "Title": "ICDAR  Competition on Handwriting Recognition of Historical Ciphers",
            "Abstract": "Handwritten Text Recognition (HTR) in low-resource sce-\nnarios (i.e. when the amount of labeled data is scarce) is a challenging\nproblem. This is particularly true for historical encrypted manuscripts,\ncommonly known as ciphers, which contain secret messages and were\ntypically used in military or diplomatic correspondence, records of secret\nsocieties, or private letters. To hide their contents, the sender and re-\nceiver created their own secret method of writing. The cipher alphabets\noften include digits, Latin or Greek letters, Zodiac and alchemical signs,\ncombined with various diacritics, as well as invented ones. The first step\nin the decryption process is the transcription of these manuscripts, which\nis difficult due to the great variation in handwriting styles and cipher al-\nphabets with a limited number of pages. Although different strategies can\nbe considered to deal with the insufficient amount of training data (e.g.,\nfew-shot learning, self-supervised learning), the performance of available\nHTR models is not yet satisfactory. Thus, the proposed competition,\nwhich includes ciphers with a large number of symbol sets and scribes,\naims to boost research in HTR in low-resource scenarios.",
            "Keywords": "Handwritten Text Recognition \u00b7 Historical Documents \u00b7 Ci-\npher Recognition \u00b7 Competition"
        },
        {
            "ID": "C04",
            "Authors": [
                "Kl\u00e9berson F. Alves",
                "V. Rocha",
                "Alejandro H. Toselli",
                "Macileide F. Oliveira",
                "S\u00e1vio S. Ara\u00fajo",
                "Hugo J",
                "F. Hazin",
                "Byron L",
                "S. Neto",
                "Arthur F",
                "Pedro H",
                "S. Souza",
                "Wiliane M",
                "Samara V",
                "S. Lins",
                "D. Bezerra",
                "A"
            ],
            "Title": "ICDAR  Competition on Handwritten Text Recognition in Brazilian Essays  BRESSAY",
            "Abstract": "This paper describes the \u201cHandwritten Text Recognition\nin Brazilian Essays \u2013 BRESSAY\u201d competition, held at the 18th Inter-\nnational Conference on Document Analysis and Recognition (ICDAR\n2024). The competition aimed to advance Handwritten Text Recogni-\ntion (HTR) by addressing challenges specific to Brazilian Portuguese\nacademic essays, such as diverse handwriting styles and document ir-\nregularities like smudges and erasures. Participants were encouraged to\ndevelop robust algorithms capable of accurately transcribing handwrit-\nten texts at line, paragraph, and page levels using the new BRESSAY\ndataset. The competition attracted 14 participants from different coun-\ntries, with 4 research groups submitting a total of 11 proposals in the\nthree challenges by the end of the competition. These proposals achieved\nimpressive recognition rates and demonstrated advancements over tra-\nditional baseline models by using key strategies such as preprocessing\ntechniques, synthetic data approaches, and advanced deep learning mod-\nels. The evaluation metrics used were Character Error Rate (CER) and\nWord Error Rate (WER), with error rates reaching up to 2.88% CER\nand 9.39% WER for line-level recognition, 3.75% CER and 10.48% WER\nfor paragraph-level recognition, and 3.77% CER and 10.08% WER for\npage-level recognition. The competition highlight the potential for con-\ntinued improvements in HTR and underscore the BRESSAY dataset as a\nresource for future researches. The dataset is available in the repository1.",
            "Keywords": "dataset \u00b7 brazilian portuguese essays \u00b7 computer vision \u00b7\ndeep learning \u00b7 handwritten text recognition."
        },
        {
            "ID": "C05",
            "Authors": [
                "Julien Perret",
                "Yijun Lin",
                "Yao - Yi Chiang",
                "Zekun Li",
                "Joseph Chazalon",
                "Nathalie Abadie",
                "Solenn Tual",
                "Jerod Weinman",
                "Bertrand Dum\u00e9nieu"
            ],
            "Title": "ICDAR  Competition on Historical Map Text Detection Recognition and Linking",
            "Abstract": "Text on digitized historical maps contains valuable informa-\ntion, e.g., providing georeferenced political and cultural context. The\ngoal of the ICDAR 2024 MapText Competition is to benchmark meth-\nods that automatically extract textual content on historical maps (e.g.,\nplace names) and connect words to form location phrases. The compe-\ntition features two primary tasks\u2014text detection and end-to-end text\nrecognition\u2014each with a secondary task of linking words into phrase\nblocks. Submissions are evaluated on two data sets: 1) David Rumsey\nHistorical Map Collection which contains 936 map images covering 80\nregions and 183 distinct publication years (from 1623 to 2012); 2) French\nLand Registers (created during the 19th century) which contains 145\nmap images of 50 French cities and towns. The competition received\n44 submissions among all tasks. This report presents the motivation for\nthe competition, the tasks, the evaluation metrics, and the submission\nanalysis.",
            "Keywords": "Text detection \u00b7 Text recognition \u00b7 Historical maps."
        },
        {
            "ID": "C06",
            "Authors": [
                "Janne van der Loop",
                "Fei Wu",
                "Vincent Christlein",
                "Florian Kordon",
                "Dalia Rodr\u00edguez - Salas",
                "Nikolaus Weichselbaumer",
                "Martin Mayr",
                "Mathias Seuret"
            ],
            "Title": "ICDAR  Competition on Multi Font Group Recognition and OCR",
            "Abstract": "This competition investigates the performance of several\nmethods for two types of analyses of early modern prints: (1) optical\ncharacter recognition, and (2) font recognition at the character level. We\nhave created and published a novel dataset that contains the ground truth\nfor both tasks. The dataset has been carefully curated and annotated by\nan expert with several years of expertise in transcribing early modern\nprints. Both tasks involved two distinct tracks, differing in ground truth\nmanagement: one that only allows the participants to use the provided\ndata for model training and a second that removes this restriction. Out\nof the five participating teams, four participated in the first track, and\nthree in the second one. The best team reached a text Character Error\nRate (CER) of 0.82 % and a font CER of 2.96 % for the first track. In the\nsecond track, these numbers could be slightly improved to 0.81 % text\nCER and 2.78 % font CER.",
            "Keywords": "Document analysis \u00b7 Optical character recognition \u00b7 Font\nrecognition \u00b7 Historical documents"
        },
        {
            "ID": "C07",
            "Authors": [
                "Jiefeng Ma",
                "Baocai Yin",
                "Hao Wu",
                "Jinshui Hu",
                "Hanbo Cheng",
                "Mingjun Chen",
                "Changpeng Pi",
                "Zhenrong Zhang",
                "Cong Liu",
                "Pengfei Hu",
                "Chenyu Liu",
                "Bing Yin",
                "Jun Du",
                "Qikai Chang"
            ],
            "Title": "ICDAR  Competition on Recognition of Chemical Structures",
            "Abstract": "The recognition of chemical molecular structures is crucial\nin \ufb01elds such as education and biochemistry. Due to the signi\ufb01cant chal-\nlenges in data acquisition and annotation, current methods mainly focus\non recognizing printed structures with clean backgrounds, with limited\nresearch devoted to handwritten chemical molecular structure recogni-\ntion. This disparity arises because researchers can readily obtain exten-\nsive, clean-printed structure datasets through rendering tools, whereas\nauthentic handwritten chemical structure data and corresponding bench-\nmarks are scarce. To prompt research in this \ufb01eld, we have organized a\nnew competition aimed at the recognition of genuine handwritten chem-\nical structures. The competition ran from January 10 to April 25, 2024,\nreceiving \ufb01ve valid submissions. In this report, we provide detailed infor-\nmation about the data, tasks, methods of the participating teams, and\na summary and discussion of the submitted results.",
            "Keywords": "Chemical structures recognition \u00b7 Handwritten recognition\n\u00b7 OCSR"
        },
        {
            "ID": "C08",
            "Authors": [
                "Ajoy Mondal",
                "Soumya Shamarao Jahagirdar",
                "Yuheng ( Carl ) Ren",
                "C",
                "V. Jawahar",
                "Omkar M Parkhi"
            ],
            "Title": "ICDAR  Competition on Reading Documents Through Aria Glasses",
            "Abstract": "This paper presents the competition report on Reading Documents through Aria\nGlasses (ICDAR 2024 RDTAG) held at the 18th International Conference on Document\nAnalysis and Recognition (ICDAR 2024). From a mixed reality perspective, understanding\nthe text in the world is of paramount importance. However, all day long, always on, machine\nperception devices like Aria Glasses pose a unique primary challenge of lower resolution due\nto their power and sensor constraints. Moreover, diverse everyday scenes like variations in\nthe lighting conditions and reading positions further complicate the reading tasks. To address\nthis, we propose a new dataset and a challenge. Specifically, we propose three novel tasks:\nIsolated Word Recognition in Low Resolution (Task A), Prediction of Reading Order (Task\nB), and Page Level Recognition (Task C). We provide new training and test sets consisting\nof document images captured by Aria Glasses while reading diverse documents in English\nunder various everyday scenarios. Our aim is to engage researchers with prior experience in\nEnglish language OCR, and to establish benchmarks contributing to the academic literature\nin this field. A total of thirty-three different teams from around the world registered for\nthis competition, and twelve teams submitted their results along with algorithm details. The\nwinning team, SRCB, achieved a 97.23% Character Recognition Rate (CRR) and a 90.45%\nWord Recognition Rate (WRR) for Task A: Isolated Word Recognition in Low Resolution.\nTeam Gang-of-N won Task B: Prediction of Reading Order with a BLEU score of 0.0939.\nTeam SRCB also won Task C: Page Level Recognition and Reading with a 77.44% average\nPage Level Character Recognition Rate (PCRR) and a 50.55% average Page Level Word\nRecognition Rate (PWRR).",
            "Keywords": "Wearable camera, Aria glasses, word recognition, reading order, page recogni-\ntion, and reading page."
        },
        {
            "ID": "C09",
            "Authors": [
                "Ajoy Mondal",
                "C",
                "V",
                "R. Manmatha",
                "Jawahar",
                "Vijay Mahadevan"
            ],
            "Title": "ICDAR  Competition on Recognition and VQA on Handwritten Documents",
            "Abstract": "This paper presents a competition report on the Recognition and Visual Question\nAnswer on Handwritten Documents towards deeper understanding of handwritten multilin-\ngual documents (ICDAR 2024-HWD) held at the 18th International Conference on Document\nAnalysis and Recognition (ICDAR 2024). Documents are in English or Indian Languages.\nEarlier editions related to recognition of Indian handwriting were held in conjunction with\nICFHR 2022 and ICDAR 2023. A related DocVQA task was held in DAS 2020. This edi-\ntion proposes three main tasks: Isolated Word Recognition (Task A), Page Level Recognition\nand Reading (Task B), and Visual Question Answers on Handwritten Documents (Task A).\nWhile Task A was already part of our previous competitions, we bring in new data as part of\nthis edition. Task B and Task C are novel additions for this year. By attracting researchers\nwith experience in printed and handwritten documents, we aim to establish benchmarks that\nsignificantly contribute to the academic literature in this field. A total of thirty-two teams\naround the world registered for this competition. Among them, only ten teams submitted\ntheir results along with algorithm details. The winning team, TSNUK, achieved an aver-\nage 98.00% Character Recognition Rate (CRR) and 94.26% Word Recognition Rate (WRR)\nacross four languages for Task A: Isolated Word Recognition. IndependentOCR excelled\nin Task B: Page Level Recognition and Reading, with 76.32% average Page Level Character\nRecognition Rate (PCRR) and 62.57% average Page Level Word Recognition Rate. The team\nPA VCG won Task C: Visual Question Answering on Handwritten Documents with a 0.643\nANLS score.",
            "Keywords": "Handwritten documents, word detection, word recognition, reading order, recog-\nnition of paragraph, and visual question answer."
        }
    ],
    "classification": [],
    "keyInformationExtraction": [],
    "opticalCharacterRecognition": [],
    "datasets": [],
    "layoutUnderstanding": [],
    "others": []
}