{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PDF Data Extraction and Processing Notebook\n",
    "\n",
    "This notebook extracts titles and authors from PDF files, cleans the text, and saves the results into both CSV and JSON formats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## 1. Imports and Setup\n",
    "# Importing required libraries for PDF extraction, natural language processing, and data handling.\n",
    "\n",
    "import fitz  # PyMuPDF\n",
    "from transformers import pipeline\n",
    "import re\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import unicodedata\n",
    "\n",
    "# Load the NER pipeline using a pre-trained model\n",
    "ner_pipeline = pipeline(\"ner\", model=\"dbmdz/bert-large-cased-finetuned-conll03-english\", grouped_entities=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ## 2. Helper Functions\n",
    "# Functions for cleaning and normalizing text, handling diacritical marks, and extracting author names.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_hash_prefix(text):\n",
    "    \"\"\"Remove or correct sequences where '##' precede diacritic characters.\"\"\"\n",
    "    return re.sub(r'##([a-zA-Z])', r'\\1', text)\n",
    "\n",
    "def clean_author_lines(text):\n",
    "    \"\"\"Clean author lines by removing unwanted characters, bracketed sequences, numeric characters, and fixing diacritical marks.\"\"\"\n",
    "    text = re.sub(r'\\[.*?\\]', '', text)  # Remove sequences within brackets\n",
    "    text = re.sub(r'\\d+', '', text)  # Remove all numeric characters\n",
    "    text = re.sub(r'[⋆*]', '', text)  # Remove stars and other unwanted special characters\n",
    "    text = clean_hash_prefix(text)  # Normalize names with the '##' prefix\n",
    "    text = fix_diacritics(text)  # Fix diacritical marks\n",
    "    text = re.sub(r',+', ',', text)  # Replace multiple commas with a single comma\n",
    "    text = re.sub(r'\\s*,\\s*', ', ', text).strip()  # Remove extra whitespace around commas\n",
    "    return text\n",
    "\n",
    "def fix_diacritics(text):\n",
    "    \"\"\"Normalize text to handle diacritical marks from various languages.\"\"\"\n",
    "    text = unicodedata.normalize('NFC', text)\n",
    "\n",
    "    diacritic_symbols = ['ˇ', '´', '`', \"'\", '^', '~', '¨', ',']\n",
    "    replacements = {\n",
    "        'ˇ': {'s': 'š', 'c': 'č', 'z': 'ž', 'n': 'ň', 'r': 'ř', 't': 'ť', 'd': 'ď'},\n",
    "        '´': {'e': 'é', 'a': 'á', 'i': 'í', 'o': 'ó', 'u': 'ú', 'ı': 'í'},\n",
    "        '`': {'e': 'è', 'a': 'à', 'i': 'ì', 'o': 'ò', 'u': 'ù'},\n",
    "        \"'\": {'e': 'é', 'a': 'á', 'i': 'í', 'o': 'ó', 'u': 'ú'},\n",
    "        '^': {'a': 'â', 'i': 'î', 'o': 'ô', 'u': 'û'},\n",
    "        '~': {'a': 'ã', 'o': 'õ', 'n': 'ñ'},\n",
    "        '¨': {'a': 'ä', 'e': 'ë', 'i': 'ï', 'o': 'ö', 'u': 'ü'},\n",
    "        ',': {'c': 'ç'}\n",
    "    }\n",
    "\n",
    "    text = re.sub(\n",
    "        rf'({\"|\".join(map(re.escape, diacritic_symbols))})([a-zA-Zı])',\n",
    "        lambda match: replacements.get(match.group(1), {}).get(match.group(2).lower(), match.group(0)),\n",
    "        text\n",
    "    )\n",
    "    return text\n",
    "\n",
    "def extract_authors(text):\n",
    "    \"\"\"Use NER to extract person names as authors, filtering out non-name entities.\"\"\"\n",
    "    ner_results = ner_pipeline(text)\n",
    "    authors = [entity['word'] for entity in ner_results if entity['entity_group'] == 'PER']\n",
    "    return list(set(authors))\n",
    "\n",
    "def clean_title(title):\n",
    "    \"\"\"Remove non-letter characters from the title, except hyphens and colons.\"\"\"\n",
    "    return re.sub(r'[^A-Za-z\\s\\-\\:]', '', title).strip()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ## 3. Extracting Data from PDFs\n",
    "# Function to extract titles and authors from PDFs, with special handling for specific files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_with_layout(pdf_path):\n",
    "    \"\"\"Extract text from the first page of a PDF using PyMuPDF, preserving block structure.\"\"\"\n",
    "    title_lines = []\n",
    "    author_lines = \"\"\n",
    "\n",
    "    with fitz.open(pdf_path) as doc:\n",
    "        first_page = doc[0]  # Access the first page of the PDF\n",
    "        blocks = first_page.get_text(\"blocks\")  # Extract text as blocks to preserve layout\n",
    "        \n",
    "        if len(blocks) > 0:\n",
    "            # Extract the title from the first block\n",
    "            if pdf_path == 'data/ICDAR2024_proceedings_pdfs\\\\0162.pdf' or pdf_path == 'data/ICDAR2024_proceedings_pdfs\\\\0123.pdf' or pdf_path == 'data/ICDAR2024_proceedings_pdfs\\\\0271.pdf':\n",
    "                title_lines = blocks[0][4] + blocks[1][4]\n",
    "                title_lines = title_lines.splitlines()\n",
    "            else:\n",
    "                title_lines = blocks[0][4].splitlines()  # Get the text content of the first block as the title\n",
    "\n",
    "            # Extract authors from the second block\n",
    "            if len(blocks) > 1:\n",
    "                if pdf_path == 'data/ICDAR2024_proceedings_pdfs\\\\0162.pdf' or pdf_path == 'data/ICDAR2024_proceedings_pdfs\\\\0123.pdf':\n",
    "                    author_lines = clean_author_lines(blocks[2][4] + blocks[3][4] + blocks[4][4])\n",
    "                elif pdf_path == 'data/ICDAR2024_proceedings_pdfs\\\\0271.pdf':\n",
    "                    author_lines = clean_author_lines(blocks[2][4])\n",
    "                else:\n",
    "                    author_lines = clean_author_lines(blocks[1][4])  # Clean the text content of the second block for authors\n",
    "    \n",
    "    title = clean_title(\" \".join(title_lines)) if title_lines else \"Unknown Title\"\n",
    "    if pdf_path == 'data/ICDAR2024_proceedings_pdfs\\\\0271.pdf':\n",
    "        authors = author_lines\n",
    "        print(authors)\n",
    "    else :\n",
    "        authors = extract_authors(author_lines) if author_lines else [\"Unknown Author\"]\n",
    "    return title, authors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function to extract abstracts and keywiords from PDFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_abstract_and_keywords(pdf_path):\n",
    "    \"\"\"Extract the abstract and keywords from the first page of a PDF.\"\"\"\n",
    "    abstract_text = None\n",
    "    keywords_text = None\n",
    "\n",
    "    with fitz.open(pdf_path) as doc:\n",
    "        first_page = doc[0]\n",
    "        text = first_page.get_text()\n",
    "\n",
    "        # Extract Abstract\n",
    "        abstract_match = re.search(r'Abstract\\.\\s*(.*?)(?=\\n\\n|\\nKeywords:|$)', text, re.DOTALL | re.IGNORECASE)\n",
    "        if abstract_match:\n",
    "            abstract_text = abstract_match.group(1).strip()\n",
    "\n",
    "        # Extract Keywords\n",
    "        keywords_match = re.search(r'Keywords:\\s*(.*?)(?=\\n1 Introduction|\\n\\d|$)', text, re.DOTALL | re.IGNORECASE)\n",
    "        if keywords_match:\n",
    "            keywords_text = keywords_match.group(1).strip()\n",
    "\n",
    "    return abstract_text, keywords_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ## 4. Processing PDF Files\n",
    "# Logic to process all PDFs in the specified folder and extract titles and authors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ting-Wei LIAO and Hsiang-An WANG\n"
     ]
    }
   ],
   "source": [
    "# Directory containing the PDF files\n",
    "pdf_folder_path = 'data/ICDAR2024_proceedings_pdfs'\n",
    "\n",
    "# Process all PDF files in the folder and store results directly into a list\n",
    "results = []\n",
    "pdf_files = sorted([f for f in os.listdir(pdf_folder_path) if f.endswith('.pdf')])\n",
    "\n",
    "for pdf_file in pdf_files:\n",
    "    pdf_path = os.path.join(pdf_folder_path, pdf_file)\n",
    "    title, authors = extract_text_with_layout(pdf_path)\n",
    "    abstract, keywords = extract_abstract_and_keywords(pdf_path)\n",
    "    pdf_id = pdf_file.split('.')[0].lstrip('0')\n",
    "\n",
    "    # Handle authors list and format it properly\n",
    "    if authors:\n",
    "        formatted_authors = authors  \n",
    "    else:\n",
    "        formatted_authors = [\"Unknown Author\"]\n",
    "\n",
    "    # Store the results in the specified format\n",
    "    results.append({\n",
    "        \"ID\": pdf_id,\n",
    "        \"Authors\": formatted_authors,\n",
    "        \"Title\": title,\n",
    "        \"Abstract\": abstract,\n",
    "        \"Keywords\": keywords\n",
    "    })\n",
    "\n",
    "# Convert the results to a DataFrame with the desired column order\n",
    "results_df = pd.DataFrame(results, columns=[\"ID\", \"Authors\", \"Title\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ## 5. Saving the Results\n",
    "# Code to save the extracted results into both CSV and JSON formats.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results have been saved to CSV file at outputs/extracted_information.csv\n",
      "Results have been saved to JSON file at outputs/extracted_information.json\n"
     ]
    }
   ],
   "source": [
    "# Save the DataFrame directly to a CSV file in the required format\n",
    "csv_output_path = 'outputs/extracted_information.csv'\n",
    "results_df.to_csv(csv_output_path, index=False)\n",
    "\n",
    "print(f\"Results have been saved to CSV file at {csv_output_path}\")\n",
    "\n",
    "json_data = {\n",
    "    \"tables\": [{\"ID\": result[\"ID\"], \"title\": result[\"Title\"], \"authors\": result[\"Authors\"]} for result in results],\n",
    "    \"classification\": [],\n",
    "    \"keyInformationExtraction\": [],\n",
    "    \"opticalCharacterRecognition\": [],\n",
    "    \"datasets\": [],\n",
    "    \"layoutUnderstanding\": [],\n",
    "    \"others\": []\n",
    "}\n",
    "\n",
    "json_output_path = 'outputs/extracted_information.json'\n",
    "with open(json_output_path, 'w') as json_file:\n",
    "    json.dump(json_data, json_file, indent=4)\n",
    "\n",
    "print(f\"Results have been saved to JSON file at {json_output_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
